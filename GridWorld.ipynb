{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Solutions for Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Markov Decision Processes (MDPs) provide a mathematical formulation for stochastic sequential decision making. Stochastic sequential decision making is prominent in fields such as economics, control systems, urban planning, and communications [Puterman]. \n",
    "\n",
    "In this project, we will be comparing a variety of classic dynamic programming and linear programming methods to solve MDPs. From the dynamic programming realm, the solution methods are:\n",
    "-  Value Iteration\n",
    "-  Policy Iteration\n",
    "\n",
    "and for linear programming methods:\n",
    "-  First order methods\n",
    "    -  Projected Gradient Method\n",
    "    -  Projected Accelerated Gradient Method\n",
    "-  Barrier Method\n",
    "-  Simplex method\n",
    "-  CVXPY toolset (SCS, ECOS, CVXOPT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Formulation\n",
    "$\\textbf{Definition 1:}$ A Markov decision process (MDP) is a tuple $\\mathcal{M}=(S,s_0,A,P)$ where $S$ is the finite set of states, i.e., $|S|=n$, $s_0\\in S$ is the initial state, $A$ is the finite set of actions, i.e. $|A|=a$, and $P: S\\times A \\rightarrow[0,1]^{n}$ is the transition function. For any given $(s,a)$, $P$ satisfies $\\sum_{t\\in S}P(t| s,a)=1$ and $P(t | s,a)\\geq 0$. Assume for simplicity that all actions are available in all states.\n",
    "\n",
    "An infinite run from the initial state $s_0$ is a sequence $\\rho$$=$$s_0a_0s_1a_1s_2...$ of states and actions such that for all $k\\geq 0$, we have $P(s_{k+1}|s_k,a_k)$$>$$0$. A policy specifies a procedure for action selection in each state depending on the history of states and actions. A deterministic policy is a function $\\pi : S \\rightarrow A$ that maps the set of states into the set of available actions for the input state. For a given MDP $\\mathcal{M}$, we denote the set of all possible deterministic policies by $\\Pi(\\mathcal{M})$.\n",
    "\n",
    "To define a reward maximization problem, we relate each state transition with a real valued reward, using the reward function $r : S\\times A\\times S \\rightarrow \\mathbb{R}$. Then, the expected total reward of an infinite horizon decision process following the policy $\\pi$ is given as follows,\n",
    "\n",
    "$\\begin{align}\n",
    "V^{\\pi}(s)=\\lim_{N\\rightarrow \\infty}\\mathbb{E}\\Big[\\sum_{k=0}^{N-1}\\alpha^kr(s_i,\\pi(s),s_{i+1}) \\ \\Big| \\ i_0=s_0\\Big]\n",
    "\\end{align}$ $\\forall s\\in S$\n",
    "\n",
    "where $V^{\\pi}(s)$ is the value of state $s$ under the policy $\\pi$. \n",
    "\n",
    "\n",
    "Suppose that our aim is to find a policy that maximizes the expected total reward. The Bellman equation and optimality conditions provide the following formulation to compute maximum values for states and corresponding actions which achieves the maximum reward values,\n",
    "\n",
    "$\\begin{align}\n",
    "V^{\\star}(s)&=\\max_{a}\\sum_{t\\in S}P(t \\ | \\ s,a)\\Big[r(s,a,t)+\\alpha V^{\\star}(t)\\Big]\\\\\n",
    "\\pi^{\\star}(s)&=\\arg\\max_{a}\\sum_{t\\in S}P(t \\ | \\ s,a)\\Big[r(s,a,t)+\\alpha V^{\\star}(t)\\Big]\\\\\n",
    "\\end{align}$\n",
    "\n",
    "Let $T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ be an operator defined as,\n",
    "\n",
    "$(TV)(s)=\\max_{a}\\sum_{t\\in S}P(t \\ | \\ s,a)\\Big[r(s,a,t)+\\alpha V(t)\\Big]$.\n",
    "\n",
    "Then, the optimality condition is given by $TV^{\\star}=V^{\\star}$. Additionally, from well known results, $T$ is a monotone operator and a contraction mapping. Hence, it satisfies that (i) for any $V$ satisfying $V\\geq TV$, $V\\geq V^{\\star}$ where inequalities are elementwise, (ii) it has a unique solution to $V^{\\star}=TV^{\\star}$ . Using this fact, and defining $r(s,a)=\\sum_{t\\in S}P(t \\ | \\ s,a)r(s,a,t)$, we can formulate finding the maximum expected reward problem as a linear program as following,\n",
    "\n",
    "$\\begin{align}\n",
    "\\min \\ \\ &c^TV\\\\\n",
    "\\text{subject to}: \\ \\ & V(s)\\geq r(s,a)+\\alpha\\sum_{t\\in S}P(t \\ | \\ s,a)V(t)\n",
    "\\end{align}$\n",
    "\n",
    "where $c\\in\\mathbb{R}_{++}^n$. Having $c>0$ for each state ensures the uniqueness of the solution. Let $P_{a_1}\\in[0,1]^{n\\times n}$ be a transition matrix for action $a_1$. We define the following matrices,\n",
    "\n",
    "$\\begin{align}\n",
    "A=\\begin{bmatrix}\n",
    "I_{n\\times n}-\\alpha P_{a_1}\\\\\n",
    "I_{n\\times n}-\\alpha P_{a_2}\\\\\n",
    "...\\\\\n",
    "I_{n\\times n}-\\alpha P_{a_a}\n",
    "\\end{bmatrix}\n",
    "\\end{align}$ ,\n",
    "$\\begin{align}\n",
    "b=\\begin{bmatrix}\n",
    "r(s_1,a_1),&\n",
    "r(s_2,a_1),&\n",
    "...,&\n",
    "r(s_n,a_1),&\n",
    "r(s_1,a_2),&\n",
    "...,&\n",
    "r(s_n,a_2),&\n",
    "...,&\n",
    "r(s_1,a_a),&\n",
    "...,&\n",
    "r(s_n,a_a)\n",
    "\\end{bmatrix}^T\n",
    "\\end{align}$.\n",
    "\n",
    "Then the problem becomes,\n",
    "\n",
    "$\\begin{align}\n",
    "\\min \\ \\ &c^TV\\\\\n",
    "\\text{subject to}: \\ \\ & AV\\geq b\n",
    "\\end{align}$\n",
    "\n",
    "#### Dual Program\n",
    "\n",
    "The Lagrangian for the primal problem is given by,\n",
    "\n",
    "$\\begin{align}\n",
    "\\mathcal{L}(V,\\lambda)=c^TV+\\lambda^T(b-AV)\n",
    "\\end{align}$\n",
    "where the dual variable $\\lambda\\in\\mathbb{R}^{na}$. Hence, the dual problem is,\n",
    "\n",
    "$\\begin{align}\n",
    "\\max & \\ \\ \\lambda^Tb\\\\\n",
    "\\text{Subject to} :&\\ \\  A^T\\lambda-c=0\\\\\n",
    "&\\ \\  \\lambda\\geq 0\n",
    "\\end{align}$,\n",
    "or writing explicitly,  \n",
    "\n",
    "\n",
    "$\\begin{align}\n",
    "\\max & \\ \\ \\sum_{a\\in A}\\sum_{s\\in S}\\lambda(s,a)b(s,a)\\\\\n",
    "\\text{Subject to} :&\\ \\  \\sum_{a\\in A}\\lambda(s,a)-\\alpha\\sum_{t\\in S}\\sum_{a\\in A}P(s \\ | \\ t,a)\\lambda(t,a)=c(s)\\ \\ \\ \\forall \\ \\ s\\in S\\\\\n",
    "&\\ \\  \\lambda(s,a)\\geq 0\\ \\ \\ \\forall s\\in S, \\ \\ a\\in A\n",
    "\\end{align}$\n",
    "\n",
    "where $\\lambda(s,a)$ variables intuitively represent the expected residence time in a state action pair. Although the deterministic policies are at least as good as randomized policies for examples we are going to use, note that the use of dual program provides an efficient way to extract the randomized policies from the output. Using the dual program, the optimal policy for each state can be obtained by using,\n",
    "\n",
    "$\\begin{align}\n",
    "P(a \\ | \\ s)= \n",
    "\\begin{cases} \\frac{\\lambda(s,a)}{\\sum_{a\\in A}\\lambda(s,a)}& \\text{if} \\sum_{a\\in A}\\lambda(s,a)\\neq 0\\\\\n",
    "\\text{arbitrary} & \\text{if} \\sum_{a\\in A}\\lambda(s,a)= 0\\end{cases}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Required Packages\n",
    "\n",
    "import numpy as np\n",
    "import math as m\n",
    "import time\n",
    "import numpy.linalg as la\n",
    "from scipy.optimize import linprog\n",
    "import numpy.random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "from cvxpy import *\n",
    "import cvxpy as cvx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the MDP\n",
    "First, we use grid world examples to compare the performance of various methods in terms of scalability and required number of iterations to satisfy the desired error bounds on the state values. The following code generates an (n,m) grid world and allows the agent to choose between five possible actions in each state, namely left, right, up, down and stay. Once the agent chooses an action, the next state is determined stochastically.\n",
    "\n",
    "Second, we use a randomly generated MDP to compare the performance of the same algorithms when used with large number of available actions. Note that having large number of actions does not change the number of variables but increases the number of constraints in the primal problem. \n",
    "\n",
    "#### Grid World Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the grid world generator with 5 possible actions for the agent i.e. (left,right,up,down,loop).\n",
    "# Inputs:\n",
    "# Row: Number of rows of the grid world\n",
    "# Col: Number of columns of the grid world\n",
    "# Prob: Probability of taking the desired action\n",
    "# Output:\n",
    "# P_0: Transition matrix with dimensions 5 x (Row x Col) x (Row x Col)  \n",
    "\n",
    "# When an action is chosen by the agent, it is performed with probability Prob and the remaining (1-Prob) probability\n",
    "# is distributed among other actions. This property is included to introduce stochasticity.\n",
    "# Output matrix is formed in a way that 0th row of the matrix represent transitions from the  bottom left corner \n",
    "# of the grid world. Similarly, the last row is for the upper right corner ((Row x Col)th grid).\n",
    "\n",
    "def Grid_world(Row,Col,Prob):\n",
    "    State=Row*Col\n",
    "    Actions=5\n",
    "    np.random.seed(0)\n",
    "    prob=Prob\n",
    "    P_0=np.zeros((Actions,State,State))\n",
    "    #action left\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[0,i,i]=prob+(1-prob)/4\n",
    "                    P_0[0,i,i+1]=(1-prob)/4\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[0,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i+1]=(1-prob)/4\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[0,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i+1]=(1-prob)/4\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[0,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i-1]=prob\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[0,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i-1]=prob\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[0,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i-1]=prob\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[0,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[0,i,i-1]=prob\n",
    "                P_0[0,i,i+1]=(1-prob)/4\n",
    "                P_0[0,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[0,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[0,i,i-1]=prob\n",
    "                P_0[0,i,i+1]=(1-prob)/4\n",
    "                P_0[0,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[0,i,i]=(1-prob)/4\n",
    "                P_0[0,i,i-1]=prob\n",
    "                P_0[0,i,i+1]=(1-prob)/4\n",
    "                P_0[0,i,i-Col]=(1-prob)/4\n",
    "                P_0[0,i,i+Col]=(1-prob)/4\n",
    "    # action right\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[1,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i+1]=prob\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[1,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i+1]=prob\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[1,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i+1]=prob\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[1,i,i]=prob+(1-prob)/4\n",
    "                    P_0[1,i,i-1]=(1-prob)/4\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[1,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i-1]=(1-prob)/4\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[1,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i-1]=(1-prob)/4\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[1,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[1,i,i-1]=(1-prob)/4\n",
    "                P_0[1,i,i+1]=prob\n",
    "                P_0[1,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[1,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[1,i,i-1]=(1-prob)/4\n",
    "                P_0[1,i,i+1]=prob\n",
    "                P_0[1,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[1,i,i]=(1-prob)/4\n",
    "                P_0[1,i,i-1]=(1-prob)/4\n",
    "                P_0[1,i,i+1]=prob\n",
    "                P_0[1,i,i-Col]=(1-prob)/4\n",
    "                P_0[1,i,i+Col]=(1-prob)/4\n",
    "    # action up\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i+1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i+1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                if i==State-Col:\n",
    "                    P_0[2,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i+1]=(1-prob)/4\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i-1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i-1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                if i==State-1:\n",
    "                    P_0[2,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i-1]=(1-prob)/4\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[2,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[2,i,i-1]=(1-prob)/4\n",
    "                P_0[2,i,i+1]=(1-prob)/4\n",
    "                P_0[2,i,i+Col]=prob\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[2,i,i]=prob+(1-prob)/4\n",
    "                P_0[2,i,i-1]=(1-prob)/4\n",
    "                P_0[2,i,i+1]=(1-prob)/4\n",
    "                P_0[2,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[2,i,i]=(1-prob)/4\n",
    "                P_0[2,i,i-1]=(1-prob)/4\n",
    "                P_0[2,i,i+1]=(1-prob)/4\n",
    "                P_0[2,i,i-Col]=(1-prob)/4\n",
    "                P_0[2,i,i+Col]=prob\n",
    "    # action down\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i+1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "                if i==0:\n",
    "                    P_0[3,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i+1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i+1]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i-1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "                if i==Col-1:\n",
    "                    P_0[3,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i-1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i-1]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[3,i,i]=prob+(1-prob)/4\n",
    "                P_0[3,i,i-1]=(1-prob)/4\n",
    "                P_0[3,i,i+1]=(1-prob)/4\n",
    "                P_0[3,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[3,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[3,i,i-1]=(1-prob)/4\n",
    "                P_0[3,i,i+1]=(1-prob)/4\n",
    "                P_0[3,i,i-Col]=prob\n",
    "            else:\n",
    "                P_0[3,i,i]=(1-prob)/4\n",
    "                P_0[3,i,i-1]=(1-prob)/4\n",
    "                P_0[3,i,i+1]=(1-prob)/4\n",
    "                P_0[3,i,i-Col]=prob\n",
    "                P_0[3,i,i+Col]=(1-prob)/4\n",
    "    # action loop\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4\n",
    "                    P_0[4,i,i+1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i+1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i+1]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4\n",
    "                    P_0[4,i,i-1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i-1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i-1]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[4,i,i]=prob+(1-prob)/4\n",
    "                P_0[4,i,i-1]=(1-prob)/4\n",
    "                P_0[4,i,i+1]=(1-prob)/4\n",
    "                P_0[4,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[4,i,i]=prob+(1-prob)/4\n",
    "                P_0[4,i,i-1]=(1-prob)/4\n",
    "                P_0[4,i,i+1]=(1-prob)/4\n",
    "                P_0[4,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[4,i,i]=prob\n",
    "                P_0[4,i,i-1]=(1-prob)/4\n",
    "                P_0[4,i,i+1]=(1-prob)/4\n",
    "                P_0[4,i,i-Col]=(1-prob)/4\n",
    "                P_0[4,i,i+Col]=(1-prob)/4\n",
    "    return P_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly Generated MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code generates a random MDP using given number of states and actions.\n",
    "# Inputs:\n",
    "# numOfStates: Number of states of MDP\n",
    "# numOfActions: Number of actions for all states\n",
    "# Output:\n",
    "# P_0: Probability matrix with dimension (a x s x s)\n",
    "\n",
    "#It is assumed that all actions are active in all states. Number of possible transitions for each state \n",
    "#action pair is determined using uniform distribution. Then, transition probabilities are determined \n",
    "#using uniform distribution and normalized afterwards.\n",
    "\n",
    "\n",
    "def randomMDP(numOfStates, numOfActions):\n",
    "    np.random.seed(1)\n",
    "    P_0=np.zeros((numOfActions,numOfStates,numOfStates))\n",
    "    for act in range(numOfActions):\n",
    "        for i in range(P_0.shape[1]):\n",
    "            outgoingNum = 1 + np.random.randint(numOfActions, size=1);\n",
    "            outgoingStateList = np.random.choice(numOfStates, outgoingNum, replace=False)\n",
    "            probs = np.random.rand(outgoingNum[0])\n",
    "            probs = probs/sum(probs)\n",
    "            for j in range(outgoingNum[0]):\n",
    "                P_0[act, i, outgoingStateList[j]] = probs[j]\n",
    "\n",
    "    return P_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Problem Formulation and Constraints for the Linear Program\n",
    "\n",
    "The below functions build the constraint matrices defined in the introduction for several cases. In addition to these constraints, we need to add an additional constraint for projected gradient, accelerated projected gradient and barrier methods. Barrier methods utilize the upper bounded constraints explicilty to determine the analytical center and gradient methods utilize analytical center as an initial point. The gradient methods are not required to use these extra constraints, otherwise. The additional constraint which apply an upper bound to state values is defined as,\n",
    "\n",
    "$ \\begin{align}\n",
    "V(s) \\leq \\frac{\\underset{ s',a}{\\max}(r(s',a,t))}{1-\\alpha} +c ,\\forall s \\in S, \\ s', t \\in S, \\ a \\in A, \\ c \\geq 0\n",
    "\\end{align}$\n",
    "\n",
    "The upper bound is derived as follows,\n",
    " \n",
    "$\\begin{align}\n",
    "V^{\\star}(s_{max})&=\\max_{a}\\sum_{t\\in S}P(t \\ | \\ s,a)\\Big[r(s,a,t)+\\alpha V^{\\star}(t)\\Big] \n",
    "\\\\\n",
    "&\\leq \\max_{a}\\sum_{t\\in S}P(t \\ | \\ s,a) \\Big[r(s,a,t)+\\alpha V^{\\star}(s_{max})\\Big]\n",
    "\\\\\n",
    "&\\leq \\max_{a}\\sum_{t\\in S}P(t \\ | \\ s,a) \\Big[ \\Big[ \\underset{s',a'}{\\max}r(s',a',t) \\Big] +\\alpha V^{\\star}(s_{max})\\Big]\n",
    "\\\\\n",
    "&\\leq \\Big[ \\Big[ \\underset{s',a'}{\\max}r(s',a',t)  \\Big] +\\alpha V^{\\star}(s_{max})\\Big]\n",
    "\\end{align}$\n",
    "\n",
    "where $V^{\\star}(s_{max})$ is the largest state value and $\\left(\\underset{s',a'}{\\max}r(s',a',t)\\right)$ is the largest reward amongst all state action pairs.\n",
    "\n",
    "Thus it can be concluded that\n",
    "\n",
    "$\\begin{align}\n",
    "V^{\\star}(s) \\leq V^{\\star}(s_{max}) \\leq  \\frac{\\underset{s',a}{\\max} r(s',a,t)}{1- \\alpha}, \\forall s \\in S.\n",
    "\\end{align}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Given a grid world, the following code generates constraints for the primal linear program\n",
    "# Inputs:\n",
    "# rows: Number of rows of the grid world\n",
    "# columns: Number of columns of the grid world\n",
    "# prob: Probability of taking the desired action\n",
    "# discount: Discount factor\n",
    "# tolerance: Desired tolerance \n",
    "\n",
    "# Output:\n",
    "# A: Rearranged probability matrix constraint\n",
    "# b: Bounds\n",
    "# c: Coefficients for decision variables\n",
    "# P_0: Transition matrix with dimensions 5 x (Row x Col) x (Row x Col)\n",
    "\n",
    "# The primal problem is \n",
    "# min   c^T x\n",
    "# s.t.  Ax>= b\n",
    "\n",
    "def Primal_parameters_1(rows,columns,prob,reward,discount,tolerance):\n",
    "    States=rows*columns\n",
    "    P_0=Grid_world(rows,columns,prob)\n",
    "    A=np.zeros((5*States,States)) # 5 is number of actions !\n",
    "    b=np.zeros((5*States,1))\n",
    "    # initial distributions USE 1 for each state to ensure the converge to the unique solution!\n",
    "    c=np.ones(States)\n",
    "    # Constraints  Ax>=b\n",
    "    for a in range(5):\n",
    "        A[a*States:(a+1)*States,:]=np.eye(States)-discount*P_0[a,:,:]\n",
    "    # assign reward=1 to the top right cell and zero reward to others\n",
    "    for a in range(P_0.shape[0]):\n",
    "        b[a*States:(a+1)*States]=reward\n",
    "    return A,b,c,P_0\n",
    "\n",
    "# The following code adds additional constraints to bound the polytope above. It is used to find an analytical center \n",
    "# for interior point methods and also as a starting point for gradient methods\n",
    "\n",
    "def Analytic_center_bounds(rows,columns,prob,reward,discount,tolerance):\n",
    "    States=rows*columns\n",
    "    P_0=Grid_world(rows,columns,prob)\n",
    "    A=np.zeros((5*States,States)) # 5 is number of actions !\n",
    "    b=np.zeros((5*States,1))\n",
    "    # initial distributions USE 1 for each state to ensure the converge to the unique solution!\n",
    "    c=np.ones(States)\n",
    "    # Constraints  Ax>=b\n",
    "    for a in range(5):\n",
    "        A[a*States:(a+1)*States,:]=np.eye(States)-discount*P_0[a,:,:]\n",
    "    # assign reward=1 to the top right cell and zero reward to others\n",
    "    for a in range(P_0.shape[0]):\n",
    "        b[a*States:(a+1)*States]=reward\n",
    "    bound=np.max(b)/(1-discount)+100\n",
    "    dum=-np.eye(States)\n",
    "    dum2=-bound*np.ones((States,1))\n",
    "    A=np.concatenate((A,dum),axis=0)\n",
    "    b=np.concatenate((b,dum2),axis=0)\n",
    "    return A,b,c,P_0\n",
    "\n",
    "# Given a primal LP, the following function outputs constraints for the dual problem\n",
    "\n",
    "# Input:\n",
    "# A: Probability matrix constraint\n",
    "# b: Bounds\n",
    "# c: Coefficients for decision variables\n",
    "\n",
    "# Output:\n",
    "# A.T: Rearranged probability matrix constraint\n",
    "# b: bounds\n",
    "# c: coefficients for decision variables\n",
    "\n",
    "# The dual problem is \n",
    "# max b^T y\n",
    "# s.t. A^T y = c\n",
    "#      y>=0\n",
    "\n",
    "def Dual_parameters_1(A,b,c):\n",
    "    return A.T,b,c\n",
    "\n",
    "# The following code generates constraints for the random MDP input\n",
    "# Inputs and outputs are the same with minor changes as explained above \n",
    "def Primal_parameters_2(P_0, discount, reward):\n",
    "    actions=P_0.shape[0]\n",
    "    States=P_0.shape[1]\n",
    "    A=np.zeros((actions*States,States)) # 5 is number of actions !\n",
    "    b=np.zeros((actions*States,1))\n",
    "    # initial distributions USE 1 for each state to ensure the converge to the unique solution!\n",
    "    c=np.ones(States)\n",
    "    # Constraints  Ax>=b\n",
    "    for a in range(actions):\n",
    "        A[a*States:(a+1)*States,:]=np.eye(States)-discount*P_0[a,:,:]\n",
    "    # assign reward=1 to the top right cell and zero reward to others\n",
    "    for a in range(actions):\n",
    "        b[a*States:(a+1)*States]=reward\n",
    "    return A,b,c\n",
    "\n",
    "def Analytic_center_bounds2(P_0, discount, reward):\n",
    "    actions=P_0.shape[0]\n",
    "    States=P_0.shape[1]\n",
    "    A=np.zeros((actions*States,States)) # 5 is number of actions !\n",
    "    b=np.zeros((actions*States,1))\n",
    "    # initial distributions USE 1 for each state to ensure the converge to the unique solution!\n",
    "    c=np.ones(States)\n",
    "    # Constraints  Ax>=b\n",
    "    for a in range(actions):\n",
    "        A[a*States:(a+1)*States,:]=np.eye(States)-discount*P_0[a,:,:]\n",
    "    # assign reward=1 to the top right cell and zero reward to others\n",
    "    for a in range(actions):\n",
    "        b[a*States:(a+1)*States]=reward\n",
    "    bound=np.max(b)/(1-discount)+100\n",
    "    dum=-np.eye(States)\n",
    "    dum2=-bound*np.ones((States,1))\n",
    "    A=np.concatenate((A,dum),axis=0)\n",
    "    b=np.concatenate((b,dum2),axis=0)\n",
    "    return A,b,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods \n",
    "The following codes are methods we use to solve 'given an MDP and a reward structure, find the optimal state values and policies achieving it' problem. First, we find the optimal states values and a policy achieving it by the policy iteration algorithm. Then using its results, we keep the log of relative error values of each iteration for other methods. The elapsed time when using different methods are all measured using 'time.time()' function. \n",
    "\n",
    "We also solve the dual problem using different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "Value iteration is an application of Dynamic Programming and it is one the most understood and used algortihms to solve MDPs. The algorithm starts with arbitrary initial values of states. Due to the discount factor any point can be used as an initial point. In an iteration, for each state it finds the best action which maximizes the state value, using the old state values. After all state values are recalculated, a new iteration starts with the updated state values. The algorithm terminates when the difference between old and updated state value are below a threshold. [PUTERMAN]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following function performs the value iteration algorithm\n",
    "# Inputs:\n",
    "# P: Probability matrix\n",
    "# R: Reward vector for states\n",
    "# discount: Discount factor\n",
    "# optimal: Optimal value vector to compute error per iteration\n",
    "# eps: tolerance \n",
    "\n",
    "# Outputs:\n",
    "# stateVals: Value vector for states\n",
    "# bestAction : Action vector for states achieving the optimal value\n",
    "# error : Error values for each iteration\n",
    "# num_iter : Required number of iterations\n",
    "# elapsed_time : Elapsed time during the computation\n",
    "\n",
    "def valitr(P, R, discount, optimal,eps):\n",
    "    start_time = time.time()\n",
    "    numOfStates = P.shape[2]\n",
    "    numOfActions = P.shape[0]\n",
    "    stateVals = np.zeros(numOfStates)\n",
    "    optimal = np.resize(optimal, stateVals.shape)\n",
    "    bestAction = np.zeros(numOfStates)\n",
    "    #df is the stopping criteria\n",
    "    df = np.zeros(1)\n",
    "    df[0] = 1\n",
    "    num_iter=0\n",
    "    error = []\n",
    "    while la.norm(df,2) > eps:\n",
    "        stateValsNew = np.zeros(P.shape[2])\n",
    "        num_iter+=1\n",
    "        #for each state maximize the value\n",
    "        for s in range(numOfStates):\n",
    "            maxVal = -np.inf;\n",
    "            bestActOfs = -1;\n",
    "            #search every action\n",
    "            for a in range(numOfActions):\n",
    "                successors = np.where(P[a,s,:] > 0)\n",
    "                valOfa = R[s] + discount*np.dot(stateVals[successors], np.squeeze(P[a,s,successors]))\n",
    "                if valOfa > maxVal:\n",
    "                    maxVal = valOfa;\n",
    "                    bestActOfs = a;\n",
    "            bestAction[s] = bestActOfs\n",
    "            stateValsNew[s] = maxVal;\n",
    "        #update the state values and continue\n",
    "        df = stateValsNew - stateVals\n",
    "        stateVals = stateValsNew\n",
    "        error.append(la.norm(stateVals - optimal, 2))\n",
    "        \n",
    "    stateVals = np.resize(stateVals, (numOfStates,1))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return stateVals, bestAction, error, num_iter, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy iteration is a well-known method to solve infinite horizon MDPs, which are known to have a stationary policy to maximize the objective function. The algorithm starts with an arbitrary deterministic policy, which induces a Markov Chain, or, in other words, each state brings only one constraint. After a policy is generated, it is evaluated to determine the new state values, which is subsequently used to determine the best actions and a new policy is generated. The new policy is generated such that it maximizes the state values of the new state values. The policy iteration algorithm terminates when the policy converges, i.e., the previous policy is identical to the current policy. [PUTERMAN] \n",
    "\n",
    "The main difference between this method and value iteration method is that value iteration method uses the old state values to update state values and relies on the convergence of state values, policy iteration method analytically calculates the state values at each step. Analytical computation of state values has complexity,$O(n^3)$, and causes problems when the number of states is relatively high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following function performs the policy iteration algorithm\n",
    "# Inputs:\n",
    "# P: Probability matrix\n",
    "# R: Reward vector for states\n",
    "# discount: Discount factor\n",
    "# eps: tolerance\n",
    "\n",
    "# Outputs:\n",
    "# stateVals: Value vector for states\n",
    "# bestActionNew : Action vector for states achieving the optimal value\n",
    "# error : Error values for each iteration\n",
    "# num_iter : Required number of iterations\n",
    "# elapsed_time : Elapsed time during the computation\n",
    "\n",
    "def policyitr(P, R, discount, eps):\n",
    "    start_time = time.time()\n",
    "    numOfStates = P.shape[2]\n",
    "    numOfActions = P.shape[0]\n",
    "    bestActionOld =  -1*np.ones(numOfStates)\n",
    "    bestActionNew = np.random.randint(numOfActions, size=numOfStates)\n",
    "    stateVals = np.zeros([numOfStates, 1]);\n",
    "    vals = []; #to keep state values;\n",
    "    #stops if the last two polies are same or\n",
    "    df = np.zeros(1)\n",
    "    df[0] = 1\n",
    "    num_iter=0\n",
    "    while not(np.array_equal(bestActionOld, bestActionNew) or la.norm(df,2) < eps):\n",
    "        num_iter+=1\n",
    "        bestActionOld =np.copy(bestActionNew);\n",
    "        #constructs linear equation to find new values of states\n",
    "        A = np.zeros([numOfStates, numOfStates])\n",
    "        #if there are rewards for actions this line must be changed\n",
    "        b = R;\n",
    "        b.resize([numOfStates,1])\n",
    "        for s in range(numOfStates):\n",
    "            ts = discount*P[bestActionNew[s],s,:];\n",
    "            ts.resize([1,numOfStates]);\n",
    "            A[s,:] = -ts;\n",
    "        #update the state values and continue\n",
    "        A = A + np.identity(numOfStates);\n",
    "        stateValsNew = np.linalg.solve(A,b);\n",
    "        df = stateValsNew-stateVals;\n",
    "        stateVals = stateValsNew\n",
    "        vals.append(stateVals)\n",
    "        #for each state maximize the value\n",
    "        for s in range(numOfStates):\n",
    "            maxVal = -np.inf;\n",
    "            bestActOfs = -1;\n",
    "            #search every action\n",
    "            for a in range(numOfActions):\n",
    "                successors = np.where(P[a,s,:] > 0)\n",
    "                valOfa = R[s] + discount*np.dot(np.squeeze(stateVals[successors]), np.squeeze(P[a,s,successors]))\n",
    "                if valOfa > maxVal:\n",
    "                    maxVal = valOfa;\n",
    "                    bestActOfs = a;\n",
    "            bestActionNew[s] = bestActOfs\n",
    "    error = []\n",
    "    for i in range(len(vals)):\n",
    "        error.append(np.linalg.norm(vals[i] - vals[-1], 2))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return stateVals, bestActionNew, error, num_iter, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) CVXPY Solvers for the primal problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following functions solve the primal problem\n",
    "\n",
    "# It take constraints and a tolerance value as input, and computes optimal value vectors \n",
    "# using different solvers in CVXPY\n",
    "def ECOS_Primal(A,b,c,tol):\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    cons=[A*Y>=b]\n",
    "    objective=cvx.Minimize(cvx.sum_entries(cvx.mul_elemwise(c,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    start_time = time.time()\n",
    "    prob.solve(solver=ECOS,reltol=tol,max_iters=1000,verbose=True)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value, Y.value, elapsed_time\n",
    "def SCS_Primal(A,b,c,tol):\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    cons=[A*Y>=b]\n",
    "    objective=cvx.Minimize(cvx.sum_entries(cvx.mul_elemwise(c,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    start_time = time.time()\n",
    "    prob.solve(solver=SCS,eps=tol,max_iters=1000,verbose=True)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value, Y.value, elapsed_time\n",
    "def CVXOPT_Primal(A,b,c,tol):\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    cons=[A*Y>=b]\n",
    "    objective=cvx.Minimize(cvx.sum_entries(cvx.mul_elemwise(c,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    start_time = time.time()\n",
    "    prob.solve(solver=CVXOPT,reltol=tol,max_iters=1000,verbose=True)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value ,Y.value, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) CVXPY Solvers for the dual problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following functions solve the dual problem\n",
    "\n",
    "# It take constraints and a tolerance value as input, and computes optimal value vectors \n",
    "# using different solvers in CVXPY\n",
    "\n",
    "def ECOS_Dual(A,b,c,tol):\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    print(A.shape)\n",
    "    cons=[A*Y==c,\n",
    "         Y>=0]\n",
    "    objective=cvx.Maximize(cvx.sum_entries(cvx.mul_elemwise(b,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    start_time = time.time()\n",
    "    prob.solve(solver=ECOS,reltol=tol,max_iters=1000,verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value, Y.value, elapsed_time\n",
    "def SCS_Dual(A,b,c,tol):\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    cons=[A*Y==c,\n",
    "         Y>=0]\n",
    "    objective=cvx.Maximize(cvx.sum_entries(cvx.mul_elemwise(b,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    start_time = time.time()\n",
    "    prob.solve(solver=SCS,eps=tol,max_iters=1000,verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value, Y.value, elapsed_time\n",
    "def CVXOPT_Dual(A,b,c,tol):\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    cons=[A*Y==c,\n",
    "         Y>=0]\n",
    "    objective=cvx.Maximize(cvx.sum_entries(cvx.mul_elemwise(b,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    start_time = time.time()\n",
    "    prob.solve(solver=CVXOPT,reltol=tol,max_iters=1000,verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value, Y.value, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplex Method\n",
    "The simplex method attempts to find the optimal solution by starting at a feasible extreme point and then following the edges until it reaches a point that is optimal. Each extreme point that the simplex method travels down will improve or not change the objective function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Order Methods\n",
    "#### Projection Function\n",
    "Both gradient descent and accelerated gradient methods require a projection such that the solution is a feasible point in the constraining set. Due to the nature of the linear program, the constraining set is a set of $ (n \\cdot m \\cdot a)$ affine inequalities $ Ax \\geq b$. A projection onto this constraining set is difficult, and so an approximate method called alternating projection method.\n",
    "\n",
    "The alternating projection method was originally applied to a set of affine constraints by von Neumann, but can be extended to this problem utilizing a relxation method. The method finds all of the affine inequalities that are violated by the current value $z$, and for those inequality constraints determines the projection onto the individual half-space defined by the constraint. For the constraints that are not violated, the projection is just $z$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proj(y, A, b):\n",
    "    x = y\n",
    "    maxIter = 1000\n",
    "    j = 0\n",
    "    while np.min(np.dot(A,x) - b) < 0 and j <= maxIter:\n",
    "        # find the constraints that are violated\n",
    "        constrainedBool = np.greater_equal(np.dot(A, x), b)\n",
    "        \n",
    "        # for each violated constrained perform the projection\n",
    "        for i in range(constrainedBool.shape[0]):\n",
    "            if not np.all(constrainedBool[i]):\n",
    "                # intermediate residual\n",
    "                qq=(b[i] - np.dot(A[i,:], x))\n",
    "                \n",
    "                # projection\n",
    "                x = x + np.reshape(qq[0]*A[i,:]/np.dot(A[i,:].T, A[i,:]), (x.shape[0],x.shape[1]))\n",
    "        # iteration counter\n",
    "        j = j + 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projected Gradient Descent\n",
    "Projected gradient descent updates the state values and then projects the state values onto the constraining set using the above projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graddes(x, A, b, t, eta0):\n",
    "    eta = eta0/np.sqrt(t + 1)\n",
    "    # gradient is always a vector of ones of shape x.shape\n",
    "    grad = np.ones((x.shape[0], 1))\n",
    "    y = x - eta*grad\n",
    "\n",
    "    x = proj(y, A, b)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projected Accelerated Gradient Method\n",
    "The accelerated gradient method performs the same general algorithm as projected gradient descent, however, accelerated gradient method uses the \"momentum\" of the gradient update to nudge the new solution in the same direction of the gradient. Accelerated gradient methods have faster convergence rates than gradient descent methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accelgrad(x, v, theta, A, b, t, eta0):\n",
    "    eta = eta0/np.sqrt(t+1)\n",
    "    grad = np.ones(x.shape)\n",
    "    \n",
    "    v = v - eta*grad\n",
    "    \n",
    "    theta_old = theta\n",
    "    theta = (1 + np.sqrt(1 + 4*theta**2))/2\n",
    "    \n",
    "    xprev = x\n",
    "    x = proj(v, A, b)\n",
    "    \n",
    "    v = x + (theta_old - 1)/(theta)*(x - xprev)\n",
    "    \n",
    "    return x, v, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def descent(update, update_name, x, A, b, eta0, tol, optVal, T=int(1e4)):\n",
    "    v = x\n",
    "    theta = 1.\n",
    "    error = []\n",
    "    \n",
    "    t = int(0)\n",
    "    xnew = x + 1;\n",
    "    while t <=  T and la.norm(x - xnew, 2) > tol:\n",
    "        x = xnew\n",
    "        if update_name == \"gradient\":\n",
    "            xnew = update(x, A, b, t, eta0)\n",
    "            \n",
    "        elif update_name == \"accelerated\":\n",
    "            xnew, v, theta = update(x, v, theta, A, b, t, eta0)\n",
    "            \n",
    "        if(t % 1 == 0) or (t == T - 1):\n",
    "            error.append(la.norm(x - optVal,2))\n",
    "        t = int(t + 1)\n",
    "    \n",
    "    return x, error, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interior Point Method (Barrier Method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barrier Method is an Interior Point Method to solve convex optimization problems with inequality constraints. In Barrier method, the constarints are added to the main problem as an extra function. This extra function goes to infinity when the optimized variable gets closer to the boundaries of the convex set i.e. it encourages the variable to stay in the feasible region. The algorithm starts from the analytical center which minimizes the barrier function. As the number of iterations increases, the original function becomes comparable with the barrier function and the optimized variable gets closer to the boundaries more. In this project the barrier function is chosen to be logarithmic. \n",
    "\n",
    "As previously mentioned Barrier Method needs additional contraints to solve MDPs. In the absence of these constraints, the intersection of hyperspaces are not bounded and the log barrier function is maximized at infinity. A reasonable analytical center can be obtained by bounding the space i.e. converting the domain into a polytope. The analytical center of the polytope is obtained by SCS solver and the central path is found by Newton method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findBarrierGradient(A,b,x):\n",
    "    #calculates the gradient of barrier function for IPM Ax>=b\n",
    "    g = np.zeros(A.shape[1])\n",
    "    for i in range(A.shape[0]):\n",
    "        g = g - A[i,:]/(np.dot(A[i,:],x) - b[i])\n",
    "    g = np.resize(g, x.shape)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findBarrierHessian(A,b,x):\n",
    "    H = np.zeros([A.shape[1], A.shape[1]])\n",
    "    for i in range(A.shape[0]):\n",
    "        H = H + np.outer(A[i,:], A[i,:])/(np.dot(A[i,:],x) - b[i])**2\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findAnalyticCenter(A,b,xFeasible,T):\n",
    "    #finds the analytical center for IPM using CVXPY\n",
    "    xFeasible=np.resize(xFeasible,(A.shape[1],1))\n",
    "    Y=cvx.Variable(A.shape[1])\n",
    "    dum=0\n",
    "    for i in range(A.shape[0]):\n",
    "        dum+=cvx.log(cvx.sum_entries(cvx.mul_elemwise(A[i,:],Y))-b[i])\n",
    "    obj=cvx.Minimize(-dum)\n",
    "    prob=cvx.Problem(obj)\n",
    "    prob.solve(solver=SCS,eps=tol,max_iters=100,verbose=False)\n",
    "    return Y.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findOptimalSolution(A,b,stateValsNew,alpha,eps, stateValsOpt,t):\n",
    "    #finds the optimal solution using IPM\n",
    "    error = []\n",
    "    #to make it enter the loop\n",
    "    stateVals = stateValsNew + 1;\n",
    "    itr = 0;\n",
    "    while (np.linalg.norm(stateVals-stateValsNew,2) > eps):\n",
    "        dec = 1\n",
    "        while(dec > eps):\n",
    "            itr+=1\n",
    "            stateVals = stateValsNew\n",
    "            #Finds the gradient and hessian\n",
    "            g = t*np.ones([stateVals.size, 1]) + findBarrierGradient(A,b,stateVals)\n",
    "            H = findBarrierHessian(A,b,stateVals)\n",
    "            #Updates the values\n",
    "            stateValsNew = stateVals - np.dot(np.linalg.inv(H),g)\n",
    "            error.append(np.linalg.norm(stateValsNew - stateValsOpt,2))\n",
    "            dec = 1/2*np.dot(np.transpose(g),np.dot(np.linalg.inv(H),g))\n",
    "        t = t*(1+alpha)\n",
    "        if np.isnan(error[-1]):\n",
    "                return stateVals , error\n",
    "    return stateVals, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interiorPoint(A, b, discount, eps, alpha, stateValsOpt,t):\n",
    "    #finds another feasible point using the optimal solution\n",
    "    xFeasible = stateValsOpt +1;\n",
    "    #finds analytical center\n",
    "    start_time = time.time()\n",
    "    xF = findAnalyticCenter(A,b,xFeasible,1000);\n",
    "    stateVals, error = findOptimalSolution(A,b,xF,alpha,eps, stateValsOpt,t)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return stateVals, error, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotGridWorld(Vals, bestAction, Rows, Columns):\n",
    "    imgData = np.resize(Vals,[Rows, Columns]);\n",
    "    imgData = np.flipud(imgData);    \n",
    "    plt.imshow(imgData, interpolation='nearest')\n",
    "    bestAction = np.resize(bestAction,[Rows, Columns])\n",
    "    for a in range(Rows):\n",
    "        for b in range(Columns): \n",
    "            if bestAction[Rows -1 - a,b]== 0:\n",
    "                plt.text(b,a ,r'$ \\leftarrow $')\n",
    "            elif bestAction[Rows -1 - a,b]== 1:\n",
    "                plt.text(b,a,r'$ \\rightarrow $')\n",
    "            elif bestAction[Rows -1 - a,b]== 2:\n",
    "                plt.text(b,a,r'$ \\uparrow $')\n",
    "            elif bestAction[Rows -1 - a,b] == 3:\n",
    "                plt.text(b,a,r'$ \\downarrow $')\n",
    "            elif bestAction[Rows -1 - a,b] == 4:\n",
    "                plt.text(b,a ,r'$ o $')\n",
    "            #plt.text(b,a,r'$ \\leftarrow $')\n",
    "            \n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "def Grid_Map(R, Rows, Columns):\n",
    "    imgData = np.resize(R,[Rows, Columns]);\n",
    "    imgData = np.flipud(imgData);    \n",
    "    plt.imshow(imgData, interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "#### 1) 4 x 4 Grid World\n",
    "In the first example we use a 4 by 4 grid world example to compare the required number of iterations for different methods to achieve a desired tolerance level. The agent starts from the bottom left grid and its aim is to reach the top left grid while avoiding some of the intermediate grids. We solve this problem using (1) value iteration, (2) policy iteration, (3) simplex method, (4) cvxpy with SCS,ECOS and CVXOPT solvers, (5) projected gradient descent, (6) accelerated projected gradient descent and (7) interior point (log barrier) method. We assume that the agent takes the chosen action with probability 0.8, and we use the discount factor of 0.9. The desired tolerance is $10^{-7}$.\n",
    "\n",
    "###### i) Primal Problem\n",
    "The primal problem has 16 variables and 80 constraints. The first figure shows the reward values assigned to grids. The aim of the agent is to reach (3,0)th grid by starting from (0,3)th grid while maximizing its reward. We provide the resulting policy obtained by policy iteration when the primal problem is used. We compare the methods in terms of the required number of iterations and the elapsed time (measured by time.time() function). Additionally, we include error vs. iteration plots for some of the methods.\n",
    "\n",
    "Note: We use the analytical center as the starting point for projected gradient methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAD8CAYAAADjcbh8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbac8be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows,columns,prob,discount,tol=4,4,0.8,0.9,1e-7 # Parameters for the example\n",
    "reward=np.zeros((rows*columns,1))\n",
    "reward[5],reward[10],reward[15]=-3,-3,5\n",
    "A,b,c,P_0=Primal_parameters_1(rows,columns,prob,reward,discount,tol) # Compute constraints\n",
    "A_c,b_c,c_c,P_0_c=Analytic_center_bounds(rows,columns,prob,reward,discount,tol) # Compute constraints\n",
    "Grid_Map(b[0:P_0.shape[1]], rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ECOS 2.0.4 - (C) embotech GmbH, Zurich Switzerland, 2012-15. Web: www.embotech.com/ECOS\n",
      "\n",
      "It     pcost       dcost      gap   pres   dres    k/t    mu     step   sigma     IR    |   BT\n",
      " 0  +3.547e+01  +3.576e+01  +6e+02  6e-01  4e-03  1e+00  8e+00    ---    ---    1  1  - |  -  - \n",
      " 1  +1.870e+02  +1.890e+02  +5e+02  5e-01  3e-03  3e+00  6e+00  0.4594  5e-01   0  0  0 |  0  0\n",
      " 2  +2.866e+02  +2.885e+02  +2e+02  3e-01  1e-03  2e+00  3e+00  0.6650  2e-01   0  0  0 |  0  0\n",
      " 3  +4.120e+02  +4.132e+02  +5e+01  8e-02  3e-04  1e+00  6e-01  0.9890  2e-01   0  0  0 |  0  0\n",
      " 4  +4.659e+02  +4.659e+02  +3e+00  4e-03  2e-05  7e-02  4e-02  0.9492  7e-03   1  0  0 |  0  0\n",
      " 5  +4.691e+02  +4.691e+02  +3e-01  4e-04  2e-06  7e-03  3e-03  0.9411  3e-02   1  0  0 |  0  0\n",
      " 6  +4.694e+02  +4.694e+02  +3e-03  4e-06  2e-08  7e-05  4e-05  0.9890  1e-04   1  0  0 |  0  0\n",
      " 7  +4.695e+02  +4.695e+02  +3e-05  5e-08  2e-10  8e-07  4e-07  0.9890  1e-04   1  0  0 |  0  0\n",
      " 8  +4.695e+02  +4.695e+02  +4e-07  5e-10  2e-12  9e-09  5e-09  0.9890  1e-04   1  0  0 |  0  0\n",
      "\n",
      "OPTIMAL (within feastol=5.1e-10, reltol=7.9e-10, abstol=3.7e-07).\n",
      "Runtime: 0.001199 seconds.\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "\tSCS v1.2.6 - Splitting Conic Solver\n",
      "\t(c) Brendan O'Donoghue, Stanford University, 2012-2016\n",
      "----------------------------------------------------------------------------\n",
      "Lin-sys: sparse-indirect, nnz in A = 320, CG tol ~ 1/iter^(2.00)\n",
      "eps = 1.00e-07, alpha = 1.50, max_iters = 1000, normalize = 1, scale = 1.00\n",
      "Variables n = 16, constraints m = 80\n",
      "Cones:\tlinear vars: 80\n",
      "Setup time: 4.22e-03s\n",
      "----------------------------------------------------------------------------\n",
      " Iter | pri res | dua res | rel gap | pri obj | dua obj | kap/tau | time (s)\n",
      "----------------------------------------------------------------------------\n",
      "     0| 1.#Je+00  1.#Je+00 -1.#Je+00 -1.#Je+00  1.#Je+00  1.#Je+00  3.57e-03 \n",
      "   100| 9.36e-03  1.38e-03  2.34e-05  4.69e+02  4.69e+02  1.20e-14  4.73e-03 \n",
      "   200| 9.27e-03  4.87e-05  2.39e-07  4.69e+02  4.69e+02  1.20e-14  5.66e-03 \n",
      "   300| 1.64e-03  6.39e-04  5.73e-06  4.69e+02  4.69e+02  1.21e-14  6.66e-03 \n",
      "   400| 1.36e-04  1.23e-05  4.45e-07  4.69e+02  4.69e+02  1.21e-14  7.62e-03 \n",
      "   500| 4.86e-06  4.15e-06  4.59e-08  4.69e+02  4.69e+02  1.21e-14  8.40e-03 \n",
      "   600| 6.24e-07  2.75e-07  2.99e-09  4.69e+02  4.69e+02  1.21e-14  9.03e-03 \n",
      "   700| 1.21e-07  8.95e-08  4.14e-09  4.69e+02  4.69e+02  1.21e-14  9.49e-03 \n",
      "   720| 7.51e-08  5.74e-08  1.29e-09  4.69e+02  4.69e+02  1.21e-14  9.81e-03 \n",
      "----------------------------------------------------------------------------\n",
      "Status: Solved\n",
      "Timing: Solve time: 9.82e-03s\n",
      "\tLin-sys: avg # CG iterations: 6.38, avg solve time: 5.88e-06s\n",
      "\tCones: avg projection time: 7.89e-08s\n",
      "----------------------------------------------------------------------------\n",
      "Error metrics:\n",
      "dist(s, K) = 1.4022e-14, dist(y, K*) = 0.0000e+00, s'y/|s||y| = -1.3853e-16\n",
      "|Ax + s - b|_2 / (1 + |b|_2) = 7.5051e-08\n",
      "|A'y + c|_2 / (1 + |c|_2) = 5.7352e-08\n",
      "|c'x + b'y| / (1 + |c'x| + |b'y|) = 1.2888e-09\n",
      "----------------------------------------------------------------------------\n",
      "c'x = 469.4522, -b'y = 469.4522\n",
      "============================================================================\n",
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0: -1.0000e+01 -1.0000e+01  8e+02  3e+00  2e-14  1e+00\n",
      " 1:  2.5848e+02  2.6108e+02  5e+02  2e+00  1e-14  3e+00\n",
      " 2:  4.0745e+02  4.0825e+02  1e+02  5e-01  3e-15  9e-01\n",
      " 3:  4.6000e+02  4.6012e+02  2e+01  6e-02  1e-15  1e-01\n",
      " 4:  4.6708e+02  4.6712e+02  4e+00  2e-02  1e-15  4e-02\n",
      " 5:  4.6942e+02  4.6942e+02  5e-02  2e-04  1e-15  5e-04\n",
      " 6:  4.6945e+02  4.6945e+02  5e-04  2e-06  2e-15  5e-06\n",
      " 7:  4.6945e+02  4.6945e+02  5e-06  2e-08  5e-15  5e-08\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATYAAAD8CAYAAAD9uIjPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wMCJgAhw7EqrwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbc27a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "polyit_val, bestAction2,error_pol, num_iter_pol, time_pol = policyitr(P_0, b[0:P_0.shape[1]], discount, tol) # Policy Iteration\n",
    "valit_val, bestAction,error_val, num_iter_val, time_val = valitr(P_0, b, discount,polyit_val, tol) # Value Iteration\n",
    "start_time = time.time()\n",
    "res = linprog(c, -A, -b, A_eq=None, b_eq=None, bounds=None, method='simplex',\\\n",
    "              callback=None, options={'disp': False, 'bland': False, 'tol': tol, 'maxiter': 1000}) # Simplex\n",
    "elapsed_time_simplex = time.time() - start_time\n",
    "obj_ecos_p, ecos_val_p, time_ecos_p=ECOS_Primal(A,b,c,tol) # ECOS Primal\n",
    "obj_scs_p, scs_val_p, time_scs_p=SCS_Primal(A,b,c,tol) # SCS Primal\n",
    "obj_cvx_p, cvx_val_p, time_cvx_p=CVXOPT_Primal(A,b,c,tol) # CVXOPT Primal\n",
    "plotGridWorld(polyit_val, bestAction2, rows, columns)\n",
    "analytical_vals=findAnalyticCenter(A_c,b_c,valit_val+10,T=1000)\n",
    "#start_time = time.time()\n",
    "#projgrad_val, projgrad_error, projgrad_iter = descent(graddes, \"gradient\", analytical_vals, A, b, 10, tol, valit_val) # Projected Gradient\n",
    "#elapsed_time_projgrad = time.time() - start_time\n",
    "#start_time = time.time()\n",
    "#accgrad_val, accgrad_error, accgrad_iter = descent(accelgrad, \"accelerated\", analytical_vals, A, b, 10, tol, valit_val) # Accelerated Gradient Descent\n",
    "#elapsed_time_accgrad = time.time() - start_time\n",
    "interior_val, interior_error, elapsed_int=interiorPoint(A_c, b_c, discount, tol, 0.6, valit_val,1e-2) # Interior Point Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Methods  | Number of Iterations | Elapsed Time (s) |\n",
    "| ------------- | ------------- |------------ |\n",
    "| Value Iteration  | 182  |0.32816 |\n",
    "| Policy Iteration  | 4  |0.01521 |\n",
    "| Simplex Method  | 39  |0.09414 |\n",
    "| ECOS Solver  | 8  |0.00381 |\n",
    "| SCS Solver  | 720  |0.03633 |\n",
    "| CVXOPT Solver  | 7  |0.01729 |\n",
    "| Barrier Method  | 101  |0.83292 |\n",
    "| Projected Gradient Descent | 37 | 19.46743 |\n",
    "| Acc Projected Gradient Descent  | 12 |5.45143 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ii) Dual Problem\n",
    "The dual problem has 80 variables and 96 constraints. We solve the dual problem using (1) simplex method and (3) cvxpy with SCS,ECOS and CVXOPT solvers. Additionally, we provide the resulting policy obtained by cvxopt solver (without state values on the graph). The methods are compared in terms of the required number of iterations and the elapsed time (measured by time.time() function). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_p,b_p,c_p=Dual_parameters_1(A,b,c)\n",
    "obj_ecos_d, ecos_val_d, time_ecos_d=ECOS_Dual(A_p,b_p,c_p,tol) # ECOS Dual\n",
    "obj_scs_d, scs_val_d, time_scs_d=SCS_Dual(A_p,b_p,c_p,tol) # SCS Dual\n",
    "obj_cvx_d, cvx_val_d, time_cvx_d=CVXOPT_Dual(A_p,b_p,c_p,tol) # CVXOPT Dual\n",
    "policy = np.zeros((A_p.shape[0],5))\n",
    "for a in range(5):\n",
    "    for i in range(A_p.shape[0]):\n",
    "         policy[i,a]=cvx_val_d[a*P_0.shape[1]+i]\n",
    "denum=np.sum(policy, axis=1)  \n",
    "for i in range(A_p.shape[0]):\n",
    "    for j in range(5):\n",
    "        policy[i,j]=policy[i,j]/denum[i]\n",
    "policy=np.argmax(policy,axis=1)\n",
    "plotGridWorld(b[0:P_0.shape[1]], policy, rows, columns)\n",
    "b_pp=np.reshape(b_p,(b_p.shape[0],))\n",
    "c_pp=np.reshape(c_p,(c_p.shape[0],1))\n",
    "eye=np.eye(A_p.shape[1])\n",
    "zeros=np.zeros((A_p.shape[1],1))\n",
    "start_time = time.time()\n",
    "res = linprog(-b_pp, -eye, zeros, A_eq=A_p, b_eq=c_pp, bounds=None, method='simplex',\\\n",
    "              callback=None, options={'disp': False, 'bland': False, 'tol': tol, 'maxiter': 1000}) # Simplex\n",
    "elapsed_time_simplex = time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Methods  | Number of Iterations | Elapsed Time (s) |\n",
    "| ------------- | ------------- |------------ |\n",
    "| Simplex Method  | 29  |0.03769 |\n",
    "| ECOS Solver  | 7  |0.00474 |\n",
    "| SCS Solver  | >1000  |0.01352 |\n",
    "| CVXOPT Solver  | 7  |0.01593 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)  5 x 5 Grid World\n",
    "Now we are going to show that even with a slight increase in the number of states, some of the methods fail. In this example, we use a 5 by 5 grid world example Again, we solve this problem using (1) value iteration, (2) policy iteration, (3) simplex method, (4) cvxpy with SCS,ECOS and CVXOPT solvers, (5) projected gradient descent, (6) accelerated projected gradient descent and (7) interior point (log barrier) method. We assume that the agent takes the chosen action with probability 0.8, and we use the discount factor of 0.9. The desired tolerance is $10^{-7}$.\n",
    "\n",
    "###### i) Primal Problem\n",
    "The primal problem has 25 variables and 125 constraints. The first figure shows the reward values assigned to grids. The aim of the agent is to reach (2,1)th grid by starting from (0,4)th grid while maximizing its reward. We provide the resulting policy obtained by policy iteration when the primal problem is used. We compare the methods in terms of the required number of iterations and the elapsed time (measured by time.time() function). Additionally, we include error vs. iteration plots for some of the methods.\n",
    "\n",
    "Note: We use the analytical center as the starting point for projected gradient methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows,columns,prob,discount,tol=5,5,0.8,0.9,1e-7 # Parameters for the example\n",
    "reward=np.zeros((rows*columns,1))\n",
    "reward[8],reward[16],reward[17]=-3,-20,5\n",
    "A,b,c,P_0=Primal_parameters_1(rows,columns,prob,reward,discount,tol) # Compute constraints\n",
    "A_c,b_c,c_c,P_0_c=Analytic_center_bounds(rows,columns,prob,reward,discount,tol) # Compute constraints\n",
    "Grid_Map(b[0:P_0.shape[1]], rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyit_val, bestAction2,error_pol, num_iter_pol, time_pol = policyitr(P_0, b[0:P_0.shape[1]], discount, tol) # Policy Iteration\n",
    "valit_val, bestAction,error_val, num_iter_val, time_val = valitr(P_0, b, discount,polyit_val, tol) # Value Iteration\n",
    "start_time = time.time()\n",
    "res = linprog(c, -A, -b, A_eq=None, b_eq=None, bounds=None, method='simplex',\\\n",
    "              callback=None, options={'disp': False, 'bland': False, 'tol': tol, 'maxiter': 1000}) # Simplex\n",
    "elapsed_time_simplex = time.time() - start_time\n",
    "obj_ecos_p, ecos_val_p, time_ecos_p=ECOS_Primal(A,b,c,tol) # ECOS Primal\n",
    "obj_scs_p, scs_val_p, time_scs_p=SCS_Primal(A,b,c,tol) # SCS Primal\n",
    "obj_cvx_p, cvx_val_p, time_cvx_p=CVXOPT_Primal(A,b,c,tol) # CVXOPT Primal\n",
    "plotGridWorld(polyit_val, bestAction2, rows, columns)\n",
    "analytical_vals=findAnalyticCenter(A_c,b_c,valit_val+10,T=1000)\n",
    "#start_time = time.time()\n",
    "#projgrad_val, projgrad_error, projgrad_iter = descent(graddes, \"gradient\", analytical_vals, A, b, 10, tol, valit_val) # Projected Gradient\n",
    "#elapsed_time_projgrad = time.time() - start_time\n",
    "#start_time = time.time()\n",
    "#accgrad_val, accgrad_error, accgrad_iter = descent(accelgrad, \"accelerated\", analytical_vals, A, b, 10, tol, valit_val) # Accelerated Gradient Descent\n",
    "#elapsed_time_accgrad = time.time() - start_time\n",
    "interior_val, interior_error, elapsed_int=interiorPoint(A_c, b_c, discount, tol, 0.4, valit_val,1e-3) # Interior Point Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interior_error[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Methods  | Number of Iterations | Elapsed Time (s) |\n",
    "| ------------- | ------------- |------------ |\n",
    "| Value Iteration  | 180  |0.32816 |\n",
    "| Policy Iteration  | 5  |0.01521 |\n",
    "| Simplex Method  | 64  |0.06693 |\n",
    "| ECOS Solver  | 10  |0.00454 |\n",
    "| SCS Solver  | >1000  |0.01903 |\n",
    "| CVXOPT Solver  | 8  |0.01064 |\n",
    "| Barrier Method  | 120  |0.83292 |\n",
    "| Projected Gradient Descent | NA | NA |\n",
    "| Acc Projected Gradient Descent  | NA |NA |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ii) Dual Problem\n",
    "The dual problem has 80 variables and 96 constraints. We solve the dual problem using (1) simplex method and (3) cvxpy with SCS,ECOS and CVXOPT solvers. Additionally, we provide the resulting policy obtained by cvxopt solver (without state values on the graph). The methods are compared in terms of the required number of iterations and the elapsed time (measured by time.time() function). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_p,b_p,c_p=Dual_parameters_1(A,b,c)\n",
    "obj_ecos_d, ecos_val_d, time_ecos_d=ECOS_Dual(A_p,b_p,c_p,tol) # ECOS Dual\n",
    "obj_scs_d, scs_val_d, time_scs_d=SCS_Dual(A_p,b_p,c_p,tol) # SCS Dual\n",
    "obj_cvx_d, cvx_val_d, time_cvx_d=CVXOPT_Dual(A_p,b_p,c_p,tol) # CVXOPT Dual\n",
    "policy = np.zeros((A_p.shape[0],5))\n",
    "for a in range(5):\n",
    "    for i in range(A_p.shape[0]):\n",
    "         policy[i,a]=cvx_val_d[a*P_0.shape[1]+i]\n",
    "denum=np.sum(policy, axis=1)  \n",
    "for i in range(A_p.shape[0]):\n",
    "    for j in range(5):\n",
    "        policy[i,j]=policy[i,j]/denum[i]\n",
    "policy=np.argmax(policy,axis=1)\n",
    "plotGridWorld(b[0:P_0.shape[1]], policy, rows, columns)\n",
    "b_pp=np.reshape(b_p,(b_p.shape[0],))\n",
    "c_pp=np.reshape(c_p,(c_p.shape[0],1))\n",
    "eye=np.eye(A_p.shape[1])\n",
    "zeros=np.zeros((A_p.shape[1],1))\n",
    "start_time = time.time()\n",
    "res = linprog(-b_pp, -eye, zeros, A_eq=A_p, b_eq=c_pp, bounds=None, method='simplex',\\\n",
    "              callback=None, options={'disp': False, 'bland': False, 'tol': tol, 'maxiter': 1000}) # Simplex\n",
    "elapsed_time_simplex = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Methods  | Number of Iterations | Elapsed Time (s) |\n",
    "| ------------- | ------------- |------------ |\n",
    "| Simplex Method  | 29  |0.058204 |\n",
    "| ECOS Solver  | 8  |0.00620 |\n",
    "| SCS Solver  | >1000  |0.02538 |\n",
    "| CVXOPT Solver  | 8  |0.02379 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nStates = 100\n",
    "nActions = 10\n",
    "P_0 = randomMDP(nStates, nActions)\n",
    "discount = 0.9\n",
    "reward=np.zeros((nStates,1))\n",
    "reward[5] = 1\n",
    "tol = 1e-7\n",
    "A, b, c = Primal_parameters_2(P_0, discount, reward)\n",
    "A_c, b_c, c_c = Analytic_center_bounds2(P_0, discount, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyit_val, bestAction2,error_pol, num_iter_pol, time_pol = policyitr(P_0, b[0:P_0.shape[1]], discount, tol) # Policy Iteration\n",
    "valit_val, bestAction,error_val, num_iter_val, time_val = valitr(P_0, b, discount,polyit_val, tol) # Value Iteration\n",
    "start_time = time.time()\n",
    "res = linprog(c, -A, -b, A_eq=None, b_eq=None, bounds=None, method='simplex',\\\n",
    "              callback=None, options={'disp': False, 'bland': False, 'tol': tol, 'maxiter': 1000}) # Simplex\n",
    "elapsed_time_simplex = time.time() - start_time\n",
    "obj_ecos_p, ecos_val_p, time_ecos_p=ECOS_Primal(A,b,c,tol) # ECOS Primal\n",
    "obj_scs_p, scs_val_p, time_scs_p=SCS_Primal(A,b,c,tol) # SCS Primal\n",
    "obj_cvx_p, cvx_val_p, time_cvx_p=CVXOPT_Primal(A,b,c,tol) # CVXOPT Primal\n",
    "analytical_vals=findAnalyticCenter(A_c,b_c,valit_val+10,T=1000)\n",
    "start_time = time.time()\n",
    "elapsed_time_accgrad = time.time() - start_time\n",
    "interior_val, interior_error, elapsed_int=interiorPoint(A_c, b_c, discount, tol, 0.3, valit_val,1e-7) # Interior Point Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interior_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
