{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Solutions for Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Markov Decision Processes (MDPs) provide a mathematical formulation for stochastic decision making.\n",
    "\n",
    "In this project, we will be comparing a variety of classic dynamic programming and linear programming methods. From the dynamic programming realm, the solution methods are:\n",
    "-  Value Iteration\n",
    "-  Policy Iteration\n",
    "\n",
    "and for linear programming methods:\n",
    "-  First order methods\n",
    "-  Interior point methods\n",
    "-  Simplex methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Formulation\n",
    "$\\textbf{Definition 1:}$ A Markov decision process (MDP) is a tuple $\\mathcal{M}=(S,s_0,A,P)$ where $S$ is the finite set of states, i.e. $|S|=n$, $s_0\\in S$ is the initial state, $A$ is the finite set of actions, i.e. $|A|=a$, and $P: S\\times A \\rightarrow[0,1]^{n}$ is the transition function. For any given $(s,a)$, $P$ satisfies $\\sum_{t\\in S}P(t| s,a)=1$ and $P(t | s,a)\\geq 0$. Assume for simplicity that all actions are available in all states.\n",
    "\n",
    "An infinite run from the initial state $s_0$ is a sequence $\\rho$$=$$s_0a_0s_1a_1s_2...$ of states and actions such that for all $k\\geq 0$, we have $P(s_{k+1}|s_k,a_k)$$>$$0$. A policy specifies a procedure for action selection in each state depending on the history of states and actions. A deterministic policy is a function $\\pi : S \\rightarrow A$ that maps the set of states into the set of available actions for the input state. For a given MDP $\\mathcal{M}$, we denote the set of all possible deterministic policies by $\\Pi(\\mathcal{M})$.\n",
    "\n",
    "To define a reward maximization problem, we relate each state transition with a real valued reward, using the reward function $r : S\\times A\\times S \\rightarrow \\mathbb{R}$. Then, the expected total reward of an infinite horizon decision process following the policy $\\pi$ is given as follows,\n",
    "\n",
    "$\\begin{align}\n",
    "V^{\\pi}(s)=\\lim_{N\\rightarrow \\infty}\\mathbb{E}\\Big[\\sum_{k=0}^{N-1}\\alpha^kr(s_i,\\pi(s),s_{i+1})\\Big| i_0=s_0\\Big]\n",
    "\\end{align}$ $\\forall s\\in S$\n",
    "\n",
    "where $V^{\\pi}(s)$ is the value of state $s$ under the policy $\\pi$. \n",
    "\n",
    "\n",
    "Suppose that our aim is to find a policy that maximizes the expected total reward. The Bellman equation and optimality conditions provide the following formulation to compute maximum values for states and corresponding actions which achieves the maximum reward values,\n",
    "\n",
    "$\\begin{align}\n",
    "V^{\\star}(s)&=\\max_{a}\\sum_{t\\in S}P(t|s,a)\\Big[r(s,a,t)+\\alpha V^{\\star}(t)\\Big]\\\\\n",
    "\\pi^{\\star}(s)&=\\arg\\max_{a}\\sum_{t\\in S}P(t|s,a)\\Big[r(s,a,t)+\\alpha V^{\\star}(t)\\Big]\\\\\n",
    "\\end{align}$\n",
    "\n",
    "Let $T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ be an operator defined as,\n",
    "\n",
    "$(TV)(s)=\\max_{a}\\sum_{t\\in S}P(t|s,a)\\Big[r(s,a,t)+\\alpha V(t)\\Big]$.\n",
    "\n",
    "Then, the optimality condition is given by $TV^{\\star}=V^{\\star}$. Additionally, from well known results, $T$ is a monotone operator and a contraction mapping. Hence, it satisfies that (i) for any $V$ satisfying $V\\geq TV$, $V\\geq V^{\\star}$ where inequalities are elementwise, (ii) it has a unique solution to $V^{\\star}=TV^{\\star}$ . Using this fact, and defining $r(s,a)=\\sum_{t\\in S}P(t|s,a)r(s,a,t)$, we can formulate finding the maximum expected reward problem as a linear program as following,\n",
    "\n",
    "$\\begin{align}\n",
    "\\min \\ \\ &c^TV\\\\\n",
    "\\text{subject to}: \\ \\ & V(s)\\geq r(s,a)+\\alpha\\sum_{t\\in S}P(t|s,a)V(t)\n",
    "\\end{align}$\n",
    "\n",
    "where $c\\in\\mathbb{R}_{++}^n$. Having $c>0$ for each state ensures the uniqueness of the solution. Let $P_{a_1}\\in[0,1]^{n\\times n}$ be a transition matrix for action $a_1$. We define the following matrices,\n",
    "\n",
    "$\\begin{align}\n",
    "A=\\begin{bmatrix}\n",
    "I_{n\\times n}-\\alpha P_{a_1}\\\\\n",
    "I_{n\\times n}-\\alpha P_{a_2}\\\\\n",
    "...\\\\\n",
    "I_{n\\times n}-\\alpha P_{a_a}\n",
    "\\end{bmatrix}\n",
    "\\end{align}$ ,\n",
    "$\\begin{align}\n",
    "b=\\begin{bmatrix}\n",
    "r(s_1,a_1),&\n",
    "r(s_2,a_1),&\n",
    "...,&\n",
    "r(s_n,a_1),&\n",
    "r(s_1,a_2),&\n",
    "...,&\n",
    "r(s_n,a_2),&\n",
    "...,&\n",
    "r(s_1,a_a),&\n",
    "...,&\n",
    "r(s_n,a_a)\n",
    "\\end{bmatrix}^T\n",
    "\\end{align}$.\n",
    "\n",
    "Then the problem becomes,\n",
    "\n",
    "$\\begin{align}\n",
    "\\min \\ \\ &c^TV\\\\\n",
    "\\text{subject to}: \\ \\ & AV\\geq b\n",
    "\\end{align}$\n",
    "\n",
    "### Dual Program\n",
    "\n",
    "The Lagrangian for the primal problem is given by,\n",
    "\n",
    "$\\begin{align}\n",
    "\\mathcal{L}(V,\\lambda)=c^TV+\\lambda^T(b-AV)\n",
    "\\end{align}$\n",
    "where the dual variable $\\lambda\\in\\mathbb{R}^{na}$. Hence, the dual problem is,\n",
    "\n",
    "$\\begin{align}\n",
    "\\max & \\ \\ \\lambda^Tb\\\\\n",
    "\\text{Subject to} :&\\ \\  A^T\\lambda-c=0\\\\\n",
    "&\\ \\  \\lambda\\geq 0\n",
    "\\end{align}$,\n",
    "or writing explicitly,  \n",
    "\n",
    "\n",
    "$\\begin{align}\n",
    "\\max & \\ \\ \\sum_{a\\in A}\\sum_{s\\in S}\\lambda(s,a)b(s,a)\\\\\n",
    "\\text{Subject to} :&\\ \\  \\sum_{a\\in A}\\lambda(s,a)-\\alpha\\sum_{t\\in S}\\sum_{a\\in A}P(s|t,a)\\lambda(t,a)=c(s)\\ \\ \\ \\forall \\ \\ s\\in S\\\\\n",
    "&\\ \\  \\lambda(s,a)\\geq 0\\ \\ \\ \\forall s\\in S, \\ \\ a\\in A\n",
    "\\end{align}$\n",
    "\n",
    "where $\\lambda(s,a)$ variables intuitively represent the expected residence time in a state action pair. Although the deterministic policies are at least as good as randomized policies for examples we are going to use, note that the use of dual program provides an efficient way to extract the randomized policies from the output. Using the dual program, the optimal policy for each state can be obtained by using,\n",
    "\n",
    "$\\begin{align}\n",
    "P(a| s)= \n",
    "\\begin{cases} \\frac{\\lambda(s,a)}{\\sum_{a\\in A}\\lambda(s,a)}& \\text{if} \\sum_{a\\in A}\\lambda(s,a)\\neq 0\\\\\n",
    "\\text{arbitrary} & \\text{if} \\sum_{a\\in A}\\lambda(s,a)= 0\\end{cases}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as m\n",
    "import numpy.linalg as la\n",
    "from scipy.optimize import linprog\n",
    "import numpy.random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "from cvxpy import *\n",
    "import cvxpy as cvx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of a candidate Grid World that will be solved using the above methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the MDP\n",
    "We will use grid world examples to compare the performance of various methods in terms of scalability and required number of iterations to satisfy the desired error bounds on the state values. The following code generates an (n,m) grid world and allows the agent to choose between five possible actions in each state, namely left, right, up, down and stay. Once the agent chooses an action, the next state is determined stochastically.\n",
    "#### Grid World Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the grid world generator with 5 possible actions for the agent i.e. (left,right,up,down,loop).\n",
    "# Inputs:\n",
    "# Row: Number of rows of the grid world\n",
    "# Col: Number of columns of the grid world\n",
    "# Prob: Probability of taking the desired action\n",
    "# Output:\n",
    "# P_0: Transition matrix with dimensions 5 x (Row x Col) x (Row x Col)  \n",
    "\n",
    "# When an action is chosen by the agent, it is performed with probability Prob and the remaining (1-Prob) probability\n",
    "# is distributed among other actions. This property is included to introduce stochasticity.\n",
    "# Output matrix is formed in a way that 0th row of the matrix represent transitions from the  bottom left corner \n",
    "# of the grid world. Similarly, the last row is for the upper right corner ((Row x Col)th grid).\n",
    "\n",
    "def Grid_world(Row,Col,Prob):\n",
    "    State=Row*Col\n",
    "    Actions=5\n",
    "    np.random.seed(0)\n",
    "    prob=Prob\n",
    "    P_0=np.zeros((Actions,State,State))\n",
    "    #action left\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[0,i,i]=prob+(1-prob)/4\n",
    "                    P_0[0,i,i+1]=(1-prob)/4\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[0,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i+1]=(1-prob)/4\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[0,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i+1]=(1-prob)/4\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[0,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i-1]=prob\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[0,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i-1]=prob\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[0,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i-1]=prob\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[0,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[0,i,i-1]=prob\n",
    "                P_0[0,i,i+1]=(1-prob)/4\n",
    "                P_0[0,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[0,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[0,i,i-1]=prob\n",
    "                P_0[0,i,i+1]=(1-prob)/4\n",
    "                P_0[0,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[0,i,i]=(1-prob)/4\n",
    "                P_0[0,i,i-1]=prob\n",
    "                P_0[0,i,i+1]=(1-prob)/4\n",
    "                P_0[0,i,i-Col]=(1-prob)/4\n",
    "                P_0[0,i,i+Col]=(1-prob)/4\n",
    "    # action right\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[1,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i+1]=prob\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[1,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i+1]=prob\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[1,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i+1]=prob\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[1,i,i]=prob+(1-prob)/4\n",
    "                    P_0[1,i,i-1]=(1-prob)/4\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[1,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i-1]=(1-prob)/4\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[1,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i-1]=(1-prob)/4\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[1,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[1,i,i-1]=(1-prob)/4\n",
    "                P_0[1,i,i+1]=prob\n",
    "                P_0[1,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[1,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[1,i,i-1]=(1-prob)/4\n",
    "                P_0[1,i,i+1]=prob\n",
    "                P_0[1,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[1,i,i]=(1-prob)/4\n",
    "                P_0[1,i,i-1]=(1-prob)/4\n",
    "                P_0[1,i,i+1]=prob\n",
    "                P_0[1,i,i-Col]=(1-prob)/4\n",
    "                P_0[1,i,i+Col]=(1-prob)/4\n",
    "    # action up\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i+1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i+1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                if i==State-Col:\n",
    "                    P_0[2,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i+1]=(1-prob)/4\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i-1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i-1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                if i==State-1:\n",
    "                    P_0[2,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i-1]=(1-prob)/4\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[2,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[2,i,i-1]=(1-prob)/4\n",
    "                P_0[2,i,i+1]=(1-prob)/4\n",
    "                P_0[2,i,i+Col]=prob\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[2,i,i]=prob+(1-prob)/4\n",
    "                P_0[2,i,i-1]=(1-prob)/4\n",
    "                P_0[2,i,i+1]=(1-prob)/4\n",
    "                P_0[2,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[2,i,i]=(1-prob)/4\n",
    "                P_0[2,i,i-1]=(1-prob)/4\n",
    "                P_0[2,i,i+1]=(1-prob)/4\n",
    "                P_0[2,i,i-Col]=(1-prob)/4\n",
    "                P_0[2,i,i+Col]=prob\n",
    "    # action down\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i+1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "                if i==0:\n",
    "                    P_0[3,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i+1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i+1]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i-1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "                if i==Col-1:\n",
    "                    P_0[3,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i-1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i-1]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[3,i,i]=prob+(1-prob)/4\n",
    "                P_0[3,i,i-1]=(1-prob)/4\n",
    "                P_0[3,i,i+1]=(1-prob)/4\n",
    "                P_0[3,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[3,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[3,i,i-1]=(1-prob)/4\n",
    "                P_0[3,i,i+1]=(1-prob)/4\n",
    "                P_0[3,i,i-Col]=prob\n",
    "            else:\n",
    "                P_0[3,i,i]=(1-prob)/4\n",
    "                P_0[3,i,i-1]=(1-prob)/4\n",
    "                P_0[3,i,i+1]=(1-prob)/4\n",
    "                P_0[3,i,i-Col]=prob\n",
    "                P_0[3,i,i+Col]=(1-prob)/4\n",
    "    # action loop\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4\n",
    "                    P_0[4,i,i+1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i+1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i+1]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4\n",
    "                    P_0[4,i,i-1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i-1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i-1]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[4,i,i]=prob+(1-prob)/4\n",
    "                P_0[4,i,i-1]=(1-prob)/4\n",
    "                P_0[4,i,i+1]=(1-prob)/4\n",
    "                P_0[4,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[4,i,i]=prob+(1-prob)/4\n",
    "                P_0[4,i,i-1]=(1-prob)/4\n",
    "                P_0[4,i,i+1]=(1-prob)/4\n",
    "                P_0[4,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[4,i,i]=prob\n",
    "                P_0[4,i,i-1]=(1-prob)/4\n",
    "                P_0[4,i,i+1]=(1-prob)/4\n",
    "                P_0[4,i,i-Col]=(1-prob)/4\n",
    "                P_0[4,i,i+Col]=(1-prob)/4\n",
    "    return P_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Problem Formulation and Constraints for the Linear Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n"
     ]
    }
   ],
   "source": [
    "Rows=50\n",
    "Columns=50\n",
    "Prob= 0.8 # Probability of performing the chosen action\n",
    "Discount= 0.9\n",
    "States=Rows*Columns\n",
    "P_0=Grid_world(Rows,Columns,Prob)\n",
    "A=np.zeros((5*States,States)) # 5 is number of actions !\n",
    "b=np.zeros((5*States,1))\n",
    "print(b.shape)\n",
    "\n",
    "# initial distributions USE 1 for each state!\n",
    "c=np.ones(States)\n",
    "\n",
    "# Constraints  Ax>=b\n",
    "for a in range(5):\n",
    "    A[a*States:(a+1)*States,:]=np.eye(States)-Discount*P_0[a,:,:]\n",
    "    \n",
    "# assign reward=1 to the top right cell and zero reward to others\n",
    "for a in range(5):\n",
    "    b[(a+1)*(States)-1]=1\n",
    "\n",
    "# Tolerance \n",
    "tol=1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.69788850e-05   1.92006628e-05   2.19357851e-05 ...,   6.68738728e+00\n",
      "   7.72253980e+00   8.92120305e+00]\n"
     ]
    }
   ],
   "source": [
    "def valitr(P, R, discount, eps):\n",
    "    numOfStates = P.shape[2]\n",
    "    numOfActions = P.shape[0]\n",
    "    stateVals = np.zeros(numOfStates)\n",
    "    bestAction = np.zeros(numOfStates)\n",
    "    #df is the stopping criteria\n",
    "    df = np.zeros(1)\n",
    "    df[0] = 1\n",
    "    while la.norm(df,2) > eps:\n",
    "        stateValsNew = np.zeros(P.shape[2])\n",
    "        #for each state maximize the value\n",
    "        for s in range(numOfStates):\n",
    "            maxVal = -np.inf;\n",
    "            bestActOfs = -1;\n",
    "            #search every action\n",
    "            for a in range(numOfActions):\n",
    "                successors = np.where(P[a,s,:] > 0)\n",
    "                valOfa = R[s] + discount*np.dot(stateVals[successors], np.squeeze(P[a,s,successors]))\n",
    "                if valOfa > maxVal:\n",
    "                    maxVal = valOfa;\n",
    "                    bestActOfs = a;\n",
    "            bestAction[s] = bestActOfs\n",
    "            stateValsNew[s] = maxVal;\n",
    "        #update the state values and continue\n",
    "        df = stateValsNew - stateVals\n",
    "        stateVals = stateValsNew\n",
    "        \n",
    "    stateVals = np.resize(stateVals, (numOfStates,1))\n",
    "    \n",
    "    return stateVals, bestAction\n",
    "\n",
    "vals, bestAction = valitr(P_0, b, Discount, tol)\n",
    "print(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.69965998e-05]\n",
      " [  1.92183776e-05]\n",
      " [  2.19534999e-05]\n",
      " ..., \n",
      " [  6.68738729e+00]\n",
      " [  7.72253982e+00]\n",
      " [  8.92120307e+00]]\n"
     ]
    }
   ],
   "source": [
    "def policyitr(P, R, discount, eps):\n",
    "    numOfStates = P.shape[2]\n",
    "    numOfActions = P.shape[0]\n",
    "    bestActionOld =  -1*np.ones(numOfStates)\n",
    "    bestActionNew = np.random.randint(numOfActions, size=numOfStates)\n",
    "    stateVals = np.zeros([numOfStates, 1]);\n",
    "    vals = []; #to keep state values;\n",
    "    #stops if the last two polies are same or\n",
    "    df = np.zeros(1)\n",
    "    df[0] = 1\n",
    "    while not(np.array_equal(bestActionOld, bestActionNew) or la.norm(df,2) < eps):\n",
    "        bestActionOld =np.copy(bestActionNew);\n",
    "        #constructs linear equation to find new values of states\n",
    "        A = np.zeros([numOfStates, numOfStates])\n",
    "        #if there are rewards for actions this line must be changed\n",
    "        b = R;\n",
    "        b.resize([numOfStates,1])\n",
    "        for s in range(numOfStates):\n",
    "            ts = discount*P[bestActionNew[s],s,:];\n",
    "            ts.resize([1,numOfStates]);\n",
    "            A[s,:] = -ts;\n",
    "        #update the state values and continue\n",
    "        A = A + np.identity(numOfStates);\n",
    "        stateValsNew = np.linalg.solve(A,b);\n",
    "        df = stateValsNew-stateVals;\n",
    "        stateVals = stateValsNew\n",
    "        vals.append(stateVals)\n",
    "        #for each state maximize the value\n",
    "        for s in range(numOfStates):\n",
    "            maxVal = -np.inf;\n",
    "            bestActOfs = -1;\n",
    "            #search every action\n",
    "            for a in range(numOfActions):\n",
    "                successors = np.where(P[a,s,:] > 0)\n",
    "                valOfa = R[s] + discount*np.dot(np.squeeze(stateVals[successors]), np.squeeze(P[a,s,successors]))\n",
    "                if valOfa > maxVal:\n",
    "                    maxVal = valOfa;\n",
    "                    bestActOfs = a;\n",
    "            bestActionNew[s] = bestActOfs\n",
    "    error = []\n",
    "    for i in range(len(vals)):\n",
    "        error.append(np.linalg.norm(vals[i] - vals[-1], 2))\n",
    "    return stateVals, bestActionNew, error\n",
    "vals2, bestAction2,error = policyitr(P_0, b[0:P_0.shape[1]], Discount, tol)\n",
    "print(vals2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplex Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ECOS 2.0.4 - (C) embotech GmbH, Zurich Switzerland, 2012-15. Web: www.embotech.com/ECOS\n",
      "\n",
      "It     pcost       dcost      gap   pres   dres    k/t    mu     step   sigma     IR    |   BT\n",
      " 0  +4.131e-11  +1.563e+01  +4e+04  1e+00  3e-01  1e+00  3e+00    ---    ---    1  1  - |  -  - \n",
      " 1  +3.863e+00  +6.116e+00  +1e+04  4e-02  1e-01  4e-01  8e-01  0.8387  1e-01   0  1  1 |  0  0\n",
      " 2  +1.971e+00  +2.358e+00  +3e+03  5e-03  7e-02  7e-02  2e-01  0.7480  6e-02   0  0  0 |  0  0\n",
      " 3  +7.077e-01  +8.074e-01  +1e+03  1e-03  3e-02  2e-02  1e-01  0.6370  1e-01   0  0  0 |  0  0\n",
      " 4  +2.857e-01  +3.178e-01  +6e+02  5e-04  7e-03  4e-03  5e-02  0.6018  2e-01   0  0  1 |  0  0\n",
      " 5  +1.279e-01  +1.400e-01  +3e+02  2e-04  2e-03  1e-03  3e-02  0.5807  2e-01   0  0  1 |  0  0\n",
      " 6  +5.835e-02  +6.312e-02  +2e+02  6e-05  6e-04  4e-04  1e-02  0.5780  2e-01   0  0  0 |  0  0\n",
      " 7  +2.720e-02  +2.916e-02  +1e+02  3e-05  2e-04  1e-04  8e-03  0.5801  2e-01   1  0  1 |  0  0\n",
      " 8  +1.281e-02  +1.364e-02  +5e+01  1e-05  6e-05  4e-05  4e-03  0.5779  2e-01   1  1  1 |  0  0\n",
      " 9  +5.935e-03  +6.285e-03  +3e+01  4e-06  2e-05  1e-05  2e-03  0.5879  2e-01   1  1  1 |  0  0\n",
      "10  +2.823e-03  +2.977e-03  +1e+01  1e-06  8e-06  5e-06  1e-03  0.5879  2e-01   1  1  1 |  0  0\n",
      "11  +1.282e-03  +1.347e-03  +7e+00  5e-07  3e-06  2e-06  6e-04  0.6068  2e-01   1  1  1 |  0  0\n",
      "12  +5.261e-04  +5.504e-04  +3e+00  2e-07  9e-07  5e-07  3e-04  0.6730  2e-01   1  1  1 |  0  0\n",
      "13  +1.026e-04  +1.063e-04  +6e-01  2e-08  1e-07  4e-08  5e-05  0.8859  8e-02   1  1  1 |  0  0\n",
      "14  +3.538e-05  +3.634e-05  +2e-01  6e-09  3e-08  1e-08  1e-05  0.7717  7e-02   1  1  1 |  0  0\n",
      "15  +2.376e-05  +2.418e-05  +8e-02  3e-09  1e-08  4e-09  7e-06  0.6546  2e-01   1  1  1 |  0  0\n",
      "16  +1.904e-05  +1.918e-05  +3e-02  8e-10  4e-09  1e-09  2e-06  0.7662  1e-01   1  1  1 |  0  0\n",
      "17  +1.831e-05  +1.840e-05  +2e-02  5e-10  2e-09  6e-10  1e-06  0.4961  3e-01   1  1  1 |  0  0\n",
      "18  +1.788e-05  +1.794e-05  +1e-02  4e-10  2e-09  4e-10  9e-07  0.4095  2e-01   1  1  1 |  0  0\n",
      "19  +1.753e-05  +1.757e-05  +8e-03  2e-10  1e-09  2e-10  6e-07  0.4483  2e-01   1  1  1 |  0  0\n",
      "20  +1.732e-05  +1.735e-05  +5e-03  2e-10  7e-10  1e-10  4e-07  0.4578  2e-01   1  1  1 |  0  0\n",
      "21  +1.720e-05  +1.721e-05  +3e-03  1e-10  4e-10  8e-11  3e-07  0.5406  4e-01   1  0  0 |  0  0\n",
      "22  +1.711e-05  +1.713e-05  +2e-03  6e-11  3e-10  5e-11  2e-07  0.5050  2e-01   1  0  0 |  0  0\n",
      "23  +1.707e-05  +1.708e-05  +1e-03  4e-11  2e-10  3e-11  1e-07  0.4851  3e-01   1  0  0 |  0  0\n",
      "24  +1.704e-05  +1.705e-05  +9e-04  3e-11  1e-10  2e-11  7e-08  0.4588  3e-01   1  0  0 |  0  0\n",
      "25  +1.703e-05  +1.703e-05  +6e-04  2e-11  7e-11  1e-11  5e-08  0.5203  3e-01   1  0  0 |  0  0\n",
      "26  +1.701e-05  +1.702e-05  +4e-04  1e-11  5e-11  7e-12  3e-08  0.5397  3e-01   1  0  0 |  0  0\n",
      "27  +1.701e-05  +1.701e-05  +2e-04  7e-12  3e-11  4e-12  2e-08  0.5993  4e-01   1  0  0 |  0  0\n",
      "28  +1.700e-05  +1.700e-05  +1e-04  5e-12  2e-11  3e-12  1e-08  0.5074  3e-01   1  0  0 |  0  0\n",
      "29  +1.700e-05  +1.700e-05  +1e-04  3e-12  1e-11  2e-12  8e-09  0.5337  4e-01   1  0  0 |  0  0\n",
      "30  +1.700e-05  +1.700e-05  +6e-05  2e-12  8e-12  1e-12  5e-09  0.5510  4e-01   1  0  0 |  0  0\n",
      "31  +1.700e-05  +1.700e-05  +4e-05  1e-12  5e-12  7e-13  3e-09  0.5055  3e-01   1  0  0 |  0  0\n",
      "32  +1.700e-05  +1.700e-05  +2e-05  7e-13  3e-12  4e-13  2e-09  0.6798  4e-01   1  0  0 |  0  0\n",
      "33  +1.700e-05  +1.700e-05  +1e-05  4e-13  2e-12  2e-13  1e-09  0.6983  4e-01   1  0  0 |  0  0\n",
      "34  +1.700e-05  +1.700e-05  +9e-06  3e-13  1e-12  2e-13  8e-10  0.5758  5e-01   1  0  0 |  0  0\n",
      "35  +1.700e-05  +1.700e-05  +6e-06  2e-13  8e-13  1e-13  5e-10  0.4571  2e-01   1  0  0 |  0  0\n",
      "36  +1.700e-05  +1.700e-05  +4e-06  1e-13  5e-13  7e-14  3e-10  0.5477  3e-01   1  0  0 |  0  0\n",
      "37  +1.700e-05  +1.700e-05  +3e-06  8e-14  4e-13  4e-14  2e-10  0.5073  3e-01   1  0  0 |  0  0\n",
      "38  +1.700e-05  +1.700e-05  +1e-06  4e-14  2e-13  2e-14  1e-10  0.7545  4e-01   1  0  0 |  0  0\n",
      "39  +1.700e-05  +1.700e-05  +1e-06  3e-14  2e-13  2e-14  8e-11  0.4098  5e-01   1  0  0 |  0  0\n",
      "40  +1.700e-05  +1.700e-05  +7e-07  2e-14  2e-13  1e-14  6e-11  0.4474  3e-01   1  0  0 |  0  0\n",
      "41  +1.700e-05  +1.700e-05  +6e-07  2e-14  1e-13  1e-14  4e-11  0.4946  6e-01   1  0  0 |  0  0\n",
      "42  +1.700e-05  +1.700e-05  +3e-07  1e-14  9e-14  6e-15  3e-11  0.4801  1e-01   1  0  0 |  0  0\n",
      "43  +1.700e-05  +1.700e-05  +2e-07  8e-15  6e-14  3e-15  2e-11  0.6927  4e-01   1  0  0 |  0  0\n",
      "44  +1.700e-05  +1.700e-05  +1e-07  5e-15  4e-14  2e-15  9e-12  0.7471  5e-01   1  0  0 |  0  0\n",
      "45  +1.700e-05  +1.700e-05  +7e-08  4e-15  3e-14  1e-15  6e-12  0.7148  5e-01   1  0  0 |  0  0\n",
      "\n",
      "OPTIMAL (within feastol=3.1e-14, reltol=4.3e-03, abstol=7.4e-08).\n",
      "Runtime: 0.558797 seconds.\n",
      "\n",
      "status: optimal\n",
      "optimal value 1.6996602705520174e-05\n",
      "[[  1.69966027e-05]\n",
      " [  1.92183809e-05]\n",
      " [  2.19535036e-05]\n",
      " ..., \n",
      " [  6.68738730e+00]\n",
      " [  7.72253982e+00]\n",
      " [  8.92120307e+00]]\n"
     ]
    }
   ],
   "source": [
    "Y=cvx.Variable(P_0.shape[1]) # number of states\n",
    "cons=[A*Y>=b]\n",
    "objective=cvx.Minimize(Y[0])\n",
    "prob=cvx.Problem(objective,cons)\n",
    "data = prob.get_problem_data(ECOS)\n",
    "prob.solve(solver=ECOS,abstol=1e-7,max_iters=1000,verbose=True)\n",
    "print (\"status: %s\" % prob.status)\n",
    "print (\"optimal value %s\" % objective.value)\n",
    "print(Y.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Order Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Derivation of Projection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj2(y, A, b):\n",
    "    x = y\n",
    "    \n",
    "    while(np.min(np.dot(A,x) - b) < 0):\n",
    "        constrainedBool = np.greater_equal(np.dot(A, x), b)\n",
    "    \n",
    "        for i in range(constrainedBool.shape[0]):\n",
    "            if not np.all(constrainedBool[i]):\n",
    "                #print(A[i,:].shape)\n",
    "                qq=(b[i] - np.dot(A[i,:], x))\n",
    "                x = x + np.reshape(qq[0]*A[i,:]/np.dot(A[i,:].T, A[i,:]), (x.shape[0],x.shape[1]))\n",
    "    #print(np.min(np.dot(A,x) - b))\n",
    "    return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(y, A, b):\n",
    "    discount = 0.9\n",
    "    minb = np.min(np.dot(A, y) - b)\n",
    "    \n",
    "    if minb <= 0:\n",
    "        x = y + np.abs(minb)/(1 - discount)\n",
    "    else:\n",
    "        x = y\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projected Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graddes(x, A, b, t, eta0):\n",
    "    eta = eta0/np.sqrt(t + 1)\n",
    "    # gradient is always a vector of ones of shape x.shape\n",
    "    grad = np.ones((x.shape[0], 1))\n",
    "    y = x - eta*grad\n",
    "\n",
    "    x = proj(y, A, b)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projected Accelerated Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accelgrad(x, v, theta, A, b, t, eta0):\n",
    "    eta = eta0/np.sqrt(t+1)\n",
    "    grad = np.ones(x.shape)\n",
    "    \n",
    "    v = v - eta*grad\n",
    "    \n",
    "    theta_old = theta\n",
    "    theta = (1 + np.sqrt(1 + 4*theta**2))/2\n",
    "    \n",
    "    xprev = x\n",
    "    x = proj(v, A, b)\n",
    "    \n",
    "    v = x + (theta_old - 1)/(theta)*(x - xprev)\n",
    "    \n",
    "    return x, v, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent(update, update_name, x, A, b, eta0, tol, T=int(1e4)):\n",
    "    v = x\n",
    "    theta = 1.\n",
    "    error = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        if update_name == \"gradient\":\n",
    "            x = update(x, A, b, t, eta0)\n",
    "            \n",
    "        elif update_name == \"accelerated\":\n",
    "            x, v, theta = update(x, v, theta, A, b, t, eta0)\n",
    "            \n",
    "        if(t % 1 == 0) or (t == T - 1):\n",
    "            pass\n",
    "    \n",
    "    return x, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pg, error_pg = descent(graddes, \"gradient\", vals+100, A, b, 1, tol)\n",
    "x_ag, error_ag = descent(accelgrad, \"accelerated\", vals+100, A, b, 1e-1, tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interior Point Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
