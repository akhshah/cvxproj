{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Solutions for Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Markov Decision Processes (MDPs) provide a mathematical formulation for stochastic sequential decision making. Stochastic sequential decision making has roots in many fields [Puterman] \n",
    "\n",
    "In this project, we will be comparing a variety of classic dynamic programming and linear programming methods. From the dynamic programming realm, the solution methods are:\n",
    "-  Value Iteration\n",
    "-  Policy Iteration\n",
    "\n",
    "and for linear programming methods:\n",
    "-  First order methods\n",
    "    -  Projected Gradient Method\n",
    "    -  Projected Accelerated Gradient Method\n",
    "-  Interior point methods\n",
    "-  Simplex methods\n",
    "-  CVXPY toolset (SCS, ECOS, CVXOPT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Formulation\n",
    "$\\textbf{Definition 1:}$ A Markov decision process (MDP) is a tuple $\\mathcal{M}=(S,s_0,A,P)$ where $S$ is the finite set of states, i.e. $|S|=n$, $s_0\\in S$ is the initial state, $A$ is the finite set of actions, i.e. $|A|=a$, and $P: S\\times A \\rightarrow[0,1]^{n}$ is the transition function. For any given $(s,a)$, $P$ satisfies $\\sum_{t\\in S}P(t| s,a)=1$ and $P(t | s,a)\\geq 0$. Assume for simplicity that all actions are available in all states.\n",
    "\n",
    "An infinite run from the initial state $s_0$ is a sequence $\\rho$$=$$s_0a_0s_1a_1s_2...$ of states and actions such that for all $k\\geq 0$, we have $P(s_{k+1}|s_k,a_k)$$>$$0$. A policy specifies a procedure for action selection in each state depending on the history of states and actions. A deterministic policy is a function $\\pi : S \\rightarrow A$ that maps the set of states into the set of available actions for the input state. For a given MDP $\\mathcal{M}$, we denote the set of all possible deterministic policies by $\\Pi(\\mathcal{M})$.\n",
    "\n",
    "To define a reward maximization problem, we relate each state transition with a real valued reward, using the reward function $r : S\\times A\\times S \\rightarrow \\mathbb{R}$. Then, the expected total reward of an infinite horizon decision process following the policy $\\pi$ is given as follows,\n",
    "\n",
    "$\\begin{align}\n",
    "V^{\\pi}(s)=\\lim_{N\\rightarrow \\infty}\\mathbb{E}\\Big[\\sum_{k=0}^{N-1}\\alpha^kr(s_i,\\pi(s),s_{i+1})\\Big| i_0=s_0\\Big]\n",
    "\\end{align}$ $\\forall s\\in S$\n",
    "\n",
    "where $V^{\\pi}(s)$ is the value of state $s$ under the policy $\\pi$. \n",
    "\n",
    "\n",
    "Suppose that our aim is to find a policy that maximizes the expected total reward. The Bellman equation and optimality conditions provide the following formulation to compute maximum values for states and corresponding actions which achieves the maximum reward values,\n",
    "\n",
    "$\\begin{align}\n",
    "V^{\\star}(s)&=\\max_{a}\\sum_{t\\in S}P(t|s,a)\\Big[r(s,a,t)+\\alpha V^{\\star}(t)\\Big]\\\\\n",
    "\\pi^{\\star}(s)&=\\arg\\max_{a}\\sum_{t\\in S}P(t|s,a)\\Big[r(s,a,t)+\\alpha V^{\\star}(t)\\Big]\\\\\n",
    "\\end{align}$\n",
    "\n",
    "Let $T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ be an operator defined as,\n",
    "\n",
    "$(TV)(s)=\\max_{a}\\sum_{t\\in S}P(t|s,a)\\Big[r(s,a,t)+\\alpha V(t)\\Big]$.\n",
    "\n",
    "Then, the optimality condition is given by $TV^{\\star}=V^{\\star}$. Additionally, from well known results, $T$ is a monotone operator and a contraction mapping. Hence, it satisfies that (i) for any $V$ satisfying $V\\geq TV$, $V\\geq V^{\\star}$ where inequalities are elementwise, (ii) it has a unique solution to $V^{\\star}=TV^{\\star}$ . Using this fact, and defining $r(s,a)=\\sum_{t\\in S}P(t|s,a)r(s,a,t)$, we can formulate finding the maximum expected reward problem as a linear program as following,\n",
    "\n",
    "$\\begin{align}\n",
    "\\min \\ \\ &c^TV\\\\\n",
    "\\text{subject to}: \\ \\ & V(s)\\geq r(s,a)+\\alpha\\sum_{t\\in S}P(t|s,a)V(t)\n",
    "\\end{align}$\n",
    "\n",
    "where $c\\in\\mathbb{R}_{++}^n$. Having $c>0$ for each state ensures the uniqueness of the solution. Let $P_{a_1}\\in[0,1]^{n\\times n}$ be a transition matrix for action $a_1$. We define the following matrices,\n",
    "\n",
    "$\\begin{align}\n",
    "A=\\begin{bmatrix}\n",
    "I_{n\\times n}-\\alpha P_{a_1}\\\\\n",
    "I_{n\\times n}-\\alpha P_{a_2}\\\\\n",
    "...\\\\\n",
    "I_{n\\times n}-\\alpha P_{a_a}\n",
    "\\end{bmatrix}\n",
    "\\end{align}$ ,\n",
    "$\\begin{align}\n",
    "b=\\begin{bmatrix}\n",
    "r(s_1,a_1),&\n",
    "r(s_2,a_1),&\n",
    "...,&\n",
    "r(s_n,a_1),&\n",
    "r(s_1,a_2),&\n",
    "...,&\n",
    "r(s_n,a_2),&\n",
    "...,&\n",
    "r(s_1,a_a),&\n",
    "...,&\n",
    "r(s_n,a_a)\n",
    "\\end{bmatrix}^T\n",
    "\\end{align}$.\n",
    "\n",
    "Then the problem becomes,\n",
    "\n",
    "$\\begin{align}\n",
    "\\min \\ \\ &c^TV\\\\\n",
    "\\text{subject to}: \\ \\ & AV\\geq b\n",
    "\\end{align}$\n",
    "\n",
    "#### Dual Program\n",
    "\n",
    "The Lagrangian for the primal problem is given by,\n",
    "\n",
    "$\\begin{align}\n",
    "\\mathcal{L}(V,\\lambda)=c^TV+\\lambda^T(b-AV)\n",
    "\\end{align}$\n",
    "where the dual variable $\\lambda\\in\\mathbb{R}^{na}$. Hence, the dual problem is,\n",
    "\n",
    "$\\begin{align}\n",
    "\\max & \\ \\ \\lambda^Tb\\\\\n",
    "\\text{Subject to} :&\\ \\  A^T\\lambda-c=0\\\\\n",
    "&\\ \\  \\lambda\\geq 0\n",
    "\\end{align}$,\n",
    "or writing explicitly,  \n",
    "\n",
    "\n",
    "$\\begin{align}\n",
    "\\max & \\ \\ \\sum_{a\\in A}\\sum_{s\\in S}\\lambda(s,a)b(s,a)\\\\\n",
    "\\text{Subject to} :&\\ \\  \\sum_{a\\in A}\\lambda(s,a)-\\alpha\\sum_{t\\in S}\\sum_{a\\in A}P(s|t,a)\\lambda(t,a)=c(s)\\ \\ \\ \\forall \\ \\ s\\in S\\\\\n",
    "&\\ \\  \\lambda(s,a)\\geq 0\\ \\ \\ \\forall s\\in S, \\ \\ a\\in A\n",
    "\\end{align}$\n",
    "\n",
    "where $\\lambda(s,a)$ variables intuitively represent the expected residence time in a state action pair. Although the deterministic policies are at least as good as randomized policies for examples we are going to use, note that the use of dual program provides an efficient way to extract the randomized policies from the output. Using the dual program, the optimal policy for each state can be obtained by using,\n",
    "\n",
    "$\\begin{align}\n",
    "P(a| s)= \n",
    "\\begin{cases} \\frac{\\lambda(s,a)}{\\sum_{a\\in A}\\lambda(s,a)}& \\text{if} \\sum_{a\\in A}\\lambda(s,a)\\neq 0\\\\\n",
    "\\text{arbitrary} & \\text{if} \\sum_{a\\in A}\\lambda(s,a)= 0\\end{cases}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Required Packages\n",
    "\n",
    "import numpy as np\n",
    "import math as m\n",
    "import time\n",
    "import numpy.linalg as la\n",
    "from scipy.optimize import linprog\n",
    "import numpy.random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "from cvxpy import *\n",
    "import cvxpy as cvx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of a candidate Grid World that will be solved using the above methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the MDP\n",
    "We will use grid world examples to compare the performance of various methods in terms of scalability and required number of iterations to satisfy the desired error bounds on the state values. The following code generates an (n,m) grid world and allows the agent to choose between five possible actions in each state, namely left, right, up, down and stay. Once the agent chooses an action, the next state is determined stochastically.\n",
    "#### Grid World Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the grid world generator with 5 possible actions for the agent i.e. (left,right,up,down,loop).\n",
    "# Inputs:\n",
    "# Row: Number of rows of the grid world\n",
    "# Col: Number of columns of the grid world\n",
    "# Prob: Probability of taking the desired action\n",
    "# Output:\n",
    "# P_0: Transition matrix with dimensions 5 x (Row x Col) x (Row x Col)  \n",
    "\n",
    "# When an action is chosen by the agent, it is performed with probability Prob and the remaining (1-Prob) probability\n",
    "# is distributed among other actions. This property is included to introduce stochasticity.\n",
    "# Output matrix is formed in a way that 0th row of the matrix represent transitions from the  bottom left corner \n",
    "# of the grid world. Similarly, the last row is for the upper right corner ((Row x Col)th grid).\n",
    "\n",
    "def Grid_world(Row,Col,Prob):\n",
    "    State=Row*Col\n",
    "    Actions=5\n",
    "    np.random.seed(0)\n",
    "    prob=Prob\n",
    "    P_0=np.zeros((Actions,State,State))\n",
    "    #action left\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[0,i,i]=prob+(1-prob)/4\n",
    "                    P_0[0,i,i+1]=(1-prob)/4\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[0,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i+1]=(1-prob)/4\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[0,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i+1]=(1-prob)/4\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[0,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i-1]=prob\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[0,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i-1]=prob\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[0,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i-1]=prob\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[0,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[0,i,i-1]=prob\n",
    "                P_0[0,i,i+1]=(1-prob)/4\n",
    "                P_0[0,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[0,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[0,i,i-1]=prob\n",
    "                P_0[0,i,i+1]=(1-prob)/4\n",
    "                P_0[0,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[0,i,i]=(1-prob)/4\n",
    "                P_0[0,i,i-1]=prob\n",
    "                P_0[0,i,i+1]=(1-prob)/4\n",
    "                P_0[0,i,i-Col]=(1-prob)/4\n",
    "                P_0[0,i,i+Col]=(1-prob)/4\n",
    "    # action right\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[1,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i+1]=prob\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[1,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i+1]=prob\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[1,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i+1]=prob\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[1,i,i]=prob+(1-prob)/4\n",
    "                    P_0[1,i,i-1]=(1-prob)/4\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[1,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i-1]=(1-prob)/4\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[1,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i-1]=(1-prob)/4\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[1,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[1,i,i-1]=(1-prob)/4\n",
    "                P_0[1,i,i+1]=prob\n",
    "                P_0[1,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[1,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[1,i,i-1]=(1-prob)/4\n",
    "                P_0[1,i,i+1]=prob\n",
    "                P_0[1,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[1,i,i]=(1-prob)/4\n",
    "                P_0[1,i,i-1]=(1-prob)/4\n",
    "                P_0[1,i,i+1]=prob\n",
    "                P_0[1,i,i-Col]=(1-prob)/4\n",
    "                P_0[1,i,i+Col]=(1-prob)/4\n",
    "    # action up\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i+1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i+1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                if i==State-Col:\n",
    "                    P_0[2,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i+1]=(1-prob)/4\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i-1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i-1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                if i==State-1:\n",
    "                    P_0[2,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i-1]=(1-prob)/4\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[2,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[2,i,i-1]=(1-prob)/4\n",
    "                P_0[2,i,i+1]=(1-prob)/4\n",
    "                P_0[2,i,i+Col]=prob\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[2,i,i]=prob+(1-prob)/4\n",
    "                P_0[2,i,i-1]=(1-prob)/4\n",
    "                P_0[2,i,i+1]=(1-prob)/4\n",
    "                P_0[2,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[2,i,i]=(1-prob)/4\n",
    "                P_0[2,i,i-1]=(1-prob)/4\n",
    "                P_0[2,i,i+1]=(1-prob)/4\n",
    "                P_0[2,i,i-Col]=(1-prob)/4\n",
    "                P_0[2,i,i+Col]=prob\n",
    "    # action down\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i+1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "                if i==0:\n",
    "                    P_0[3,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i+1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i+1]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i-1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "                if i==Col-1:\n",
    "                    P_0[3,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i-1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i-1]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[3,i,i]=prob+(1-prob)/4\n",
    "                P_0[3,i,i-1]=(1-prob)/4\n",
    "                P_0[3,i,i+1]=(1-prob)/4\n",
    "                P_0[3,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[3,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[3,i,i-1]=(1-prob)/4\n",
    "                P_0[3,i,i+1]=(1-prob)/4\n",
    "                P_0[3,i,i-Col]=prob\n",
    "            else:\n",
    "                P_0[3,i,i]=(1-prob)/4\n",
    "                P_0[3,i,i-1]=(1-prob)/4\n",
    "                P_0[3,i,i+1]=(1-prob)/4\n",
    "                P_0[3,i,i-Col]=prob\n",
    "                P_0[3,i,i+Col]=(1-prob)/4\n",
    "    # action loop\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4\n",
    "                    P_0[4,i,i+1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i+1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i+1]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4\n",
    "                    P_0[4,i,i-1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i-1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i-1]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[4,i,i]=prob+(1-prob)/4\n",
    "                P_0[4,i,i-1]=(1-prob)/4\n",
    "                P_0[4,i,i+1]=(1-prob)/4\n",
    "                P_0[4,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[4,i,i]=prob+(1-prob)/4\n",
    "                P_0[4,i,i-1]=(1-prob)/4\n",
    "                P_0[4,i,i+1]=(1-prob)/4\n",
    "                P_0[4,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[4,i,i]=prob\n",
    "                P_0[4,i,i-1]=(1-prob)/4\n",
    "                P_0[4,i,i+1]=(1-prob)/4\n",
    "                P_0[4,i,i-Col]=(1-prob)/4\n",
    "                P_0[4,i,i+Col]=(1-prob)/4\n",
    "    return P_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Problem Formulation and Constraints for the Linear Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "  def Primal_parameters_1(rows,columns,prob,discount,tolerance):\n",
    "    States=rows*columns\n",
    "    P_0=Grid_world(rows,columns,prob)\n",
    "    A=np.zeros((5*States,States)) # 5 is number of actions !\n",
    "    b=np.zeros((5*States,1))\n",
    "    # initial distributions USE 1 for each state to ensure the converge to the unique solution!\n",
    "    c=np.ones(States)\n",
    "    # Constraints  Ax>=b\n",
    "    for a in range(5):\n",
    "        A[a*States:(a+1)*States,:]=np.eye(States)-discount*P_0[a,:,:]\n",
    "    # assign reward=1 to the top right cell and zero reward to others\n",
    "    for a in range(5):\n",
    "        b[(a+1)*(States)-1]=3\n",
    "        b[(a+1)*(States)-1-17]=-10\n",
    "        b[(a+1)*(States)-1-13]=-10\n",
    "    return A,b,c,P_0\n",
    "  def Dual_parameters_1(A,b,c):\n",
    "    return A.T,b,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods \n",
    "The following codes are methods we use to solve 'given an MDP and a reward structure, find the optimal state values and policies achieving it' problem. First, we find the optimal states values and a policy achieving it by the policy iteration algorithm. Then using its results, we keep the log of relative error values of each iteration for other methods. The elapsed time when using different methods are all measured using 'time.time()' function. \n",
    "\n",
    "We also solve the dual problem using different methods\n",
    "\n",
    "#### 1) Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policyitr(P, R, discount, eps):\n",
    "    start_time = time.time()\n",
    "    numOfStates = P.shape[2]\n",
    "    numOfActions = P.shape[0]\n",
    "    bestActionOld =  -1*np.ones(numOfStates)\n",
    "    bestActionNew = np.random.randint(numOfActions, size=numOfStates)\n",
    "    stateVals = np.zeros([numOfStates, 1]);\n",
    "    vals = []; #to keep state values;\n",
    "    #stops if the last two polies are same or\n",
    "    df = np.zeros(1)\n",
    "    df[0] = 1\n",
    "    num_iter=0\n",
    "    while not(np.array_equal(bestActionOld, bestActionNew) or la.norm(df,2) < eps):\n",
    "        num_iter+=1\n",
    "        bestActionOld =np.copy(bestActionNew);\n",
    "        #constructs linear equation to find new values of states\n",
    "        A = np.zeros([numOfStates, numOfStates])\n",
    "        #if there are rewards for actions this line must be changed\n",
    "        b = R;\n",
    "        b.resize([numOfStates,1])\n",
    "        for s in range(numOfStates):\n",
    "            ts = discount*P[bestActionNew[s],s,:];\n",
    "            ts.resize([1,numOfStates]);\n",
    "            A[s,:] = -ts;\n",
    "        #update the state values and continue\n",
    "        A = A + np.identity(numOfStates);\n",
    "        stateValsNew = np.linalg.solve(A,b);\n",
    "        df = stateValsNew-stateVals;\n",
    "        stateVals = stateValsNew\n",
    "        vals.append(stateVals)\n",
    "        #for each state maximize the value\n",
    "        for s in range(numOfStates):\n",
    "            maxVal = -np.inf;\n",
    "            bestActOfs = -1;\n",
    "            #search every action\n",
    "            for a in range(numOfActions):\n",
    "                successors = np.where(P[a,s,:] > 0)\n",
    "                valOfa = R[s] + discount*np.dot(np.squeeze(stateVals[successors]), np.squeeze(P[a,s,successors]))\n",
    "                if valOfa > maxVal:\n",
    "                    maxVal = valOfa;\n",
    "                    bestActOfs = a;\n",
    "            bestActionNew[s] = bestActOfs\n",
    "    error = []\n",
    "    for i in range(len(vals)):\n",
    "        error.append(np.linalg.norm(vals[i] - vals[-1], 2))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return stateVals, bestActionNew, error, num_iter, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valitr(P, R, discount, eps):\n",
    "    start_time = time.time()\n",
    "    numOfStates = P.shape[2]\n",
    "    numOfActions = P.shape[0]\n",
    "    stateVals = np.zeros(numOfStates)\n",
    "    bestAction = np.zeros(numOfStates)\n",
    "    #df is the stopping criteria\n",
    "    df = np.zeros(1)\n",
    "    df[0] = 1\n",
    "    num_iter=0\n",
    "    while la.norm(df,2) > eps:\n",
    "        stateValsNew = np.zeros(P.shape[2])\n",
    "        num_iter+=1\n",
    "        #for each state maximize the value\n",
    "        for s in range(numOfStates):\n",
    "            maxVal = -np.inf;\n",
    "            bestActOfs = -1;\n",
    "            #search every action\n",
    "            for a in range(numOfActions):\n",
    "                successors = np.where(P[a,s,:] > 0)\n",
    "                valOfa = R[s] + discount*np.dot(stateVals[successors], np.squeeze(P[a,s,successors]))\n",
    "                if valOfa > maxVal:\n",
    "                    maxVal = valOfa;\n",
    "                    bestActOfs = a;\n",
    "            bestAction[s] = bestActOfs\n",
    "            stateValsNew[s] = maxVal;\n",
    "        #update the state values and continue\n",
    "        df = stateValsNew - stateVals\n",
    "        stateVals = stateValsNew\n",
    "        \n",
    "    stateVals = np.resize(stateVals, (numOfStates,1))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return stateVals, bestAction, num_iter, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) CVXPY Solvers for the primal problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ECOS_Primal(A,b,c,tol):\n",
    "    start_time = time.time()\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    cons=[A*Y>=b]\n",
    "    objective=cvx.Minimize(cvx.sum_entries(cvx.mul_elemwise(c,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    prob.solve(solver=ECOS,reltol=tol,max_iters=1000,verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value, Y.value, elapsed_time\n",
    "def SCS_Primal(A,b,c,tol):\n",
    "    start_time = time.time()\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    cons=[A*Y>=b]\n",
    "    objective=cvx.Minimize(cvx.sum_entries(cvx.mul_elemwise(c,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    prob.solve(solver=SCS,eps=tol,max_iters=1000,verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value, Y.value, elapsed_time\n",
    "def CVXOPT_Primal(A,b,c,tol):\n",
    "    start_time = time.time()\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    cons=[A*Y>=b]\n",
    "    objective=cvx.Minimize(cvx.sum_entries(cvx.mul_elemwise(c,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    prob.solve(solver=CVXOPT,reltol=tol,max_iters=1000,verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value ,Y.value, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) CVXPY Solvers for the dual problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ECOS_Dual(A,b,c,tol):\n",
    "    start_time = time.time()\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    print(A.shape)\n",
    "    cons=[A*Y==c,\n",
    "         Y>=0]\n",
    "    objective=cvx.Maximize(cvx.sum_entries(cvx.mul_elemwise(b,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    prob.solve(solver=ECOS,reltol=tol,max_iters=1000,verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value, Y.value, elapsed_time\n",
    "def SCS_Dual(A,b,c,tol):\n",
    "    start_time = time.time()\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    cons=[A*Y==c,\n",
    "         Y>=0]\n",
    "    objective=cvx.Maximize(cvx.sum_entries(cvx.mul_elemwise(b,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    prob.solve(solver=SCS,eps=tol,max_iters=1000,verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value, Y.value, elapsed_time\n",
    "def CVXOPT_Dual(A,b,c,tol):\n",
    "    start_time = time.time()\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    cons=[A*Y==c,\n",
    "         Y>=0]\n",
    "    objective=cvx.Maximize(cvx.sum_entries(cvx.mul_elemwise(b,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    prob.solve(solver=CVXOPT,reltol=tol,max_iters=1000,verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value, Y.value, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) First Order Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Derivation of Projection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proj2(y, A, b):\n",
    "    x = y\n",
    "    \n",
    "    while(np.min(np.dot(A,x) - b) < 0):\n",
    "        constrainedBool = np.greater_equal(np.dot(A, x), b)\n",
    "    \n",
    "        for i in range(constrainedBool.shape[0]):\n",
    "            if not np.all(constrainedBool[i]):\n",
    "                #print(A[i,:].shape)\n",
    "                qq=(b[i] - np.dot(A[i,:], x))\n",
    "                x = x + np.reshape(qq[0]*A[i,:]/np.dot(A[i,:].T, A[i,:]), (x.shape[0],x.shape[1]))\n",
    "    #print(np.min(np.dot(A,x) - b))\n",
    "    return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proj(y, A, b):\n",
    "    discount = 0.9\n",
    "    minb = np.min(np.dot(A, y) - b)\n",
    "    \n",
    "    if minb <= 0:\n",
    "        x = y + np.abs(minb)/(1 - discount)\n",
    "    else:\n",
    "        x = y\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1) Projected Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graddes(x, A, b, t, eta0):\n",
    "    eta = eta0/np.sqrt(t + 1)\n",
    "    # gradient is always a vector of ones of shape x.shape\n",
    "    grad = np.ones((x.shape[0], 1))\n",
    "    y = x - eta*grad\n",
    "\n",
    "    x = proj(y, A, b)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2) Projected Accelerated Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accelgrad(x, v, theta, A, b, t, eta0):\n",
    "    eta = eta0/np.sqrt(t+1)\n",
    "    grad = np.ones(x.shape)\n",
    "    \n",
    "    v = v - eta*grad\n",
    "    \n",
    "    theta_old = theta\n",
    "    theta = (1 + np.sqrt(1 + 4*theta**2))/2\n",
    "    \n",
    "    xprev = x\n",
    "    x = proj(v, A, b)\n",
    "    \n",
    "    v = x + (theta_old - 1)/(theta)*(x - xprev)\n",
    "    \n",
    "    return x, v, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def descent(update, update_name, x, A, b, eta0, tol, optVal, T=int(1e4)):\n",
    "    v = x\n",
    "    theta = 1.\n",
    "    error = []\n",
    "    \n",
    "    t = int(0)\n",
    "    xnew = x + 1;\n",
    "    while t <=  T and la.norm(x - xnew, 2) > tol:\n",
    "        x = xnew\n",
    "        if update_name == \"gradient\":\n",
    "            xnew = update(x, A, b, t, eta0)\n",
    "            \n",
    "        elif update_name == \"accelerated\":\n",
    "            xnew, v, theta = update(x, v, theta, A, b, t, eta0)\n",
    "            \n",
    "        if(t % 1 == 0) or (t == T - 1):\n",
    "            error.append(la.norm(x - optVal,2))\n",
    "        t = int(t + 1)\n",
    "    \n",
    "    return x, error, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Interior Point Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findBarrierGradient(A,b,x):\n",
    "    #calculates the gradient of barrier function for IPM Ax>=b\n",
    "    d = np.zeros([A.shape[0],1])\n",
    "    for i in range(A.shape[0]):\n",
    "        d[i,0] = -1.0/(np.dot(A[i,:],x) - b[i])\n",
    "    g = np.dot(np.transpose(A),d)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findBarrierHessian(A,b,x):\n",
    "    #calculates the hessian of barrier function for IPM Ax>=b\n",
    "    d = np.zeros([A.shape[0],1])\n",
    "    dMat = np.zeros([A.shape[0],A.shape[0]])\n",
    "    for i in range(A.shape[0]):\n",
    "        dMat[i,i] = 1.0/((np.dot(A[i,:],x) - b[i])**2)  \n",
    "    dummy = np.dot(np.transpose(A),dMat)\n",
    "    H = np.dot(dummy,A)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findAnalyticCenter(A,b,xFeasible,T):\n",
    "    #finds the analytical center for IPM using gradient descent\n",
    "    for i in range (T):\n",
    "        g = findBarrierGradient(A,b,xFeasible)\n",
    "        xFeasible = xFeasible - 0.01/(i+1)*g;\n",
    "    return xFeasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findOptimalSolution(A,b,stateValsNew,alpha,eps, stateValsOpt):\n",
    "    #finds the optimal solution using IPM\n",
    "    error = []\n",
    "    t = 1;\n",
    "    #to make it enter the loop\n",
    "    stateVals = stateValsNew + 1;\n",
    "    itr = 1;\n",
    "    while (np.linalg.norm(stateVals-stateValsNew,2) > eps):\n",
    "        stateVals = stateValsNew\n",
    "        #Finds the gradient and hessian\n",
    "        g = t*np.ones([stateVals.size, 1]) + findBarrierGradient(A,b,stateVals)\n",
    "        H = findBarrierHessian(A,b,stateVals)\n",
    "        #Updates the calues\n",
    "        stateValsNew = stateVals - np.dot(np.linalg.inv(H),g)\n",
    "        #stateValsNew = stateVals - 1/np.sqrt(itr)*g;\n",
    "        error.append(np.linalg.norm(stateValsNew - stateValsOpt,2))\n",
    "        t = t*(1+alpha)\n",
    "        if np.isnan(error[-1]):\n",
    "                return stateVals , error\n",
    "    return stateVals, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interiorPoint(A, b, discount, eps, alpha, stateValsOpt):\n",
    "    #finds another feasible point using the optimal solution\n",
    "    xFeasible = stateValsOpt +1;\n",
    "    #finds analytical center\n",
    "    xF = findAnalyticCenter(A,b,xFeasible,1000);\n",
    "    stateVals, error = findOptimalSolution(A,b,xF,alpha,eps, stateValsOpt)\n",
    "    return stateVals, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotGridWorld(Vals, bestAction, Rows, Columns):\n",
    "    imgData = np.resize(Vals,[Rows, Columns]);\n",
    "    imgData = np.flipud(imgData);    \n",
    "    plt.imshow(imgData, interpolation='nearest')\n",
    "    bestAction = np.resize(bestAction,[Rows, Columns])\n",
    "    for a in range(Rows):\n",
    "        for b in range(Columns): \n",
    "            if bestAction[Rows -1 - a,b]== 0:\n",
    "                plt.text(b,a ,r'$ \\leftarrow $')\n",
    "            elif bestAction[Rows -1 - a,b]== 1:\n",
    "                plt.text(b,a,r'$ \\rightarrow $')\n",
    "            elif bestAction[Rows -1 - a,b]== 2:\n",
    "                plt.text(b,a,r'$ \\uparrow $')\n",
    "            elif bestAction[Rows -1 - a,b] == 3:\n",
    "                plt.text(b,a,r'$ \\downarrow $')\n",
    "            elif bestAction[Rows -1 - a,b] == 4:\n",
    "                plt.text(b,a ,r'$ o $')\n",
    "            #plt.text(b,a,r'$ \\leftarrow $')\n",
    "            \n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "def Grid_Map(R, Rows, Columns):\n",
    "    imgData = np.resize(R,[Rows, Columns]);\n",
    "    imgData = np.flipud(imgData);    \n",
    "    plt.imshow(imgData, interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "##### 1) 5 x 5 Grid World\n",
    "In the first example we use a 5 by 5 grid world example to compare the required number of iterations for different methods to achieve a desired tolerance level. The agent starts from the bottom left grid and its aim is to reach the top left grid while avoiding some of the intermediate grids. We solve this problem using (1) value iteration, (2) policy iteration, (3) simplex method, (4) cvxpy with SCS,ECOS and CVXOPT solvers, (5) projected gradient descent, (6) accelerated projected gradient descent and (7) interior point method. We assume that the agent takes the chosen action with probability 0.8, and we use the discount factor of 0.9. The desired tolerance is $10^{-7}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAD8CAYAAADNNJnuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUAwCC0AxCCwAxfg/sS6EqfJk1fQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a400160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows,columns,prob,discount,tol=5,5,0.8,0.9,1e-7 # Parameters for the example\n",
    "A,b,c,P_0=Primal_parameters_1(rows,columns,prob,discount,tol) # Compute constraints\n",
    "Grid_Map(b[0:P_0.shape[1]], rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASQAAAD8CAYAAADe49kaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f104f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-6e5fe4892d92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprojgrad_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojgrad_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojgrad_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraddes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gradient\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalit_val\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalit_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Projected Gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0maccgrad_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccgrad_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccgrad_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccelgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accelerated\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalit_val\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalit_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Accelerated Gradient Descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0minterior_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterior_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minteriorPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalit_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Interior Point Method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-3fad68b89021>\u001b[0m in \u001b[0;36minteriorPoint\u001b[0;34m(A, b, discount, eps, alpha, stateValsOpt)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#finds analytical center\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mxF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindAnalyticCenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxFeasible\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mstateVals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindOptimalSolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstateValsOpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstateVals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-059cd52ed140>\u001b[0m in \u001b[0;36mfindOptimalSolution\u001b[0;34m(A, b, stateValsNew, alpha, eps, stateValsOpt)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindBarrierHessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstateVals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#Updates the calues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mstateValsNew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstateVals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m#stateValsNew = stateVals - 1/np.sqrt(itr)*g;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstateValsNew\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstateValsOpt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yagizsavas/anaconda/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yagizsavas/anaconda/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "polyit_val, bestAction2,error_pol, num_iter_pol, time_pol = policyitr(P_0, b[0:P_0.shape[1]], discount, tol) # Policy Iteration\n",
    "valit_val, bestAction,num_iter_val, time_val = valitr(P_0, b, discount, tol) # Value Iteration\n",
    "start_time = time.time()\n",
    "res = linprog(c, -A, -b, A_eq=None, b_eq=None, bounds=None, method='simplex',\\\n",
    "              callback=None, options={'disp': False, 'bland': False, 'tol': tol, 'maxiter': 1000}) # Simplex\n",
    "elapsed_time = time.time() - start_time\n",
    "obj_ecos_p, ecos_val_p, time_ecos_p=ECOS_Primal(A,b,c,tol) # ECOS Primal\n",
    "obj_scs_p, scs_val_p, time_scs_p=SCS_Primal(A,b,c,tol) # SCS Primal\n",
    "obj_cvx_p, cvx_val_p, time_cvx_p=CVXOPT_Primal(A,b,c,tol) # CVXOPT Primal\n",
    "plotGridWorld(polyit_val, bestAction2, rows, columns)\n",
    "projgrad_val, projgrad_error, projgrad_iter = descent(graddes, \"gradient\", valit_val+100, A, b, 1, tol, valit_val) # Projected Gradient\n",
    "accgrad_val, accgrad_error, accgrad_iter = descent(accelgrad, \"accelerated\", valit_val+100, A, b, 1e-1, tol, valit_val) # Accelerated Gradient Descent\n",
    "interior_val, interior_error=interiorPoint(A, b, discount, tol, 0.01, valit_val) # Interior Point Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Methods  | Number of Iterations | Elapsed Time (s) |\n",
    "| ------------- | ------------- |------------ |\n",
    "| Value Iteration  | 168  |0.01667 |\n",
    "| Policy Iteration  | 5  |0.35952 |\n",
    "| Simplex Method  | 75  |0.08410 |\n",
    "| ECOS Solver  | 10  |0.00434 |\n",
    "| SCS Solver  | >1000  |0.02763 |\n",
    "| CVXOPT Solver  | 9  |0.02035 |\n",
    "| Barrier Method  | Content Cell  |Elapsed Time (s) |\n",
    "| Projected Gradient Descent | Content Cell  |Elapsed Time (s) |\n",
    "| Acc Projected Gradient Descent  | Content Cell  |Elapsed Time (s) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 125)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAD8CAYAAADNNJnuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkuHCMrNkuLDMLBn/D5L+3wvB7qtGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e224ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_p,b_p,c_p=Dual_parameters_1(A,b,c)\n",
    "obj_ecos_d, ecos_val_d, time_ecos_d=ECOS_Dual(A_p,b_p,c_p,tol) # ECOS Dual\n",
    "obj_scs_d, scs_val_d, time_scs_d=SCS_Dual(A_p,b_p,c_p,tol) # SCS Dual\n",
    "obj_cvx_d, cvx_val_d, time_cvx_d=CVXOPT_Dual(A_p,b_p,c_p,tol) # CVXOPT Dual\n",
    "policy = np.zeros((A_p.shape[0],5))\n",
    "for a in range(5):\n",
    "    for i in range(A_p.shape[0]):\n",
    "         policy[i,a]=cvx_val_d[a*25+i]\n",
    "denum=np.sum(policy, axis=1)  \n",
    "for i in range(A_p.shape[0]):\n",
    "    for j in range(5):\n",
    "        policy[i,j]=policy[i,j]/denum[i]\n",
    "policy=np.argmax(policy,axis=1)\n",
    "plotGridWorld(b[0:P_0.shape[1]], policy, rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
