{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Solutions for Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Markov Decision Processes (MDPs) provide a mathematical formulation for stochastic sequential decision making. Stochastic sequential decision making is prominent in fields such as economics, control systems, urban planning, and communications [Puterman]. \n",
    "\n",
    "In this project, we will be comparing a variety of classic dynamic programming and linear programming methods to solve MDPs. From the dynamic programming realm, the solution methods are:\n",
    "-  Value Iteration\n",
    "-  Policy Iteration\n",
    "\n",
    "and for linear programming methods:\n",
    "-  First order methods\n",
    "    -  Projected Gradient Method\n",
    "    -  Projected Accelerated Gradient Method\n",
    "-  Interior point method\n",
    "-  Simplex method\n",
    "-  CVXPY toolset (SCS, ECOS, CVXOPT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Formulation\n",
    "$\\textbf{Definition 1:}$ A Markov decision process (MDP) is a tuple $\\mathcal{M}=(S,s_0,A,P)$ where $S$ is the finite set of states, i.e., $|S|=n$, $s_0\\in S$ is the initial state, $A$ is the finite set of actions, i.e. $|A|=a$, and $P: S\\times A \\rightarrow[0,1]^{n}$ is the transition function. For any given $(s,a)$, $P$ satisfies $\\sum_{t\\in S}P(t| s,a)=1$ and $P(t | s,a)\\geq 0$. Assume for simplicity that all actions are available in all states.\n",
    "\n",
    "An infinite run from the initial state $s_0$ is a sequence $\\rho$$=$$s_0a_0s_1a_1s_2...$ of states and actions such that for all $k\\geq 0$, we have $P(s_{k+1}|s_k,a_k)$$>$$0$. A policy specifies a procedure for action selection in each state depending on the history of states and actions. A deterministic policy is a function $\\pi : S \\rightarrow A$ that maps the set of states into the set of available actions for the input state. For a given MDP $\\mathcal{M}$, we denote the set of all possible deterministic policies by $\\Pi(\\mathcal{M})$.\n",
    "\n",
    "To define a reward maximization problem, we relate each state transition with a real valued reward, using the reward function $r : S\\times A\\times S \\rightarrow \\mathbb{R}$. Then, the expected total reward of an infinite horizon decision process following the policy $\\pi$ is given as follows,\n",
    "\n",
    "$\\begin{align}\n",
    "V^{\\pi}(s)=\\lim_{N\\rightarrow \\infty}\\mathbb{E}\\Big[\\sum_{k=0}^{N-1}\\alpha^kr(s_i,\\pi(s),s_{i+1}) \\ \\Big| \\ i_0=s_0\\Big]\n",
    "\\end{align}$ $\\forall s\\in S$\n",
    "\n",
    "where $V^{\\pi}(s)$ is the value of state $s$ under the policy $\\pi$. \n",
    "\n",
    "\n",
    "Suppose that our aim is to find a policy that maximizes the expected total reward. The Bellman equation and optimality conditions provide the following formulation to compute maximum values for states and corresponding actions which achieves the maximum reward values,\n",
    "\n",
    "$\\begin{align}\n",
    "V^{\\star}(s)&=\\max_{a}\\sum_{t\\in S}P(t \\ | \\ s,a)\\Big[r(s,a,t)+\\alpha V^{\\star}(t)\\Big]\\\\\n",
    "\\pi^{\\star}(s)&=\\arg\\max_{a}\\sum_{t\\in S}P(t \\ | \\ s,a)\\Big[r(s,a,t)+\\alpha V^{\\star}(t)\\Big]\\\\\n",
    "\\end{align}$\n",
    "\n",
    "Let $T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ be an operator defined as,\n",
    "\n",
    "$(TV)(s)=\\max_{a}\\sum_{t\\in S}P(t \\ | \\ s,a)\\Big[r(s,a,t)+\\alpha V(t)\\Big]$.\n",
    "\n",
    "Then, the optimality condition is given by $TV^{\\star}=V^{\\star}$. Additionally, from well known results, $T$ is a monotone operator and a contraction mapping. Hence, it satisfies that (i) for any $V$ satisfying $V\\geq TV$, $V\\geq V^{\\star}$ where inequalities are elementwise, (ii) it has a unique solution to $V^{\\star}=TV^{\\star}$ . Using this fact, and defining $r(s,a)=\\sum_{t\\in S}P(t \\ | \\ s,a)r(s,a,t)$, we can formulate finding the maximum expected reward problem as a linear program as following,\n",
    "\n",
    "$\\begin{align}\n",
    "\\min \\ \\ &c^TV\\\\\n",
    "\\text{subject to}: \\ \\ & V(s)\\geq r(s,a)+\\alpha\\sum_{t\\in S}P(t \\ | \\ s,a)V(t)\n",
    "\\end{align}$\n",
    "\n",
    "where $c\\in\\mathbb{R}_{++}^n$. Having $c>0$ for each state ensures the uniqueness of the solution. Let $P_{a_1}\\in[0,1]^{n\\times n}$ be a transition matrix for action $a_1$. We define the following matrices,\n",
    "\n",
    "$\\begin{align}\n",
    "A=\\begin{bmatrix}\n",
    "I_{n\\times n}-\\alpha P_{a_1}\\\\\n",
    "I_{n\\times n}-\\alpha P_{a_2}\\\\\n",
    "...\\\\\n",
    "I_{n\\times n}-\\alpha P_{a_a}\n",
    "\\end{bmatrix}\n",
    "\\end{align}$ ,\n",
    "$\\begin{align}\n",
    "b=\\begin{bmatrix}\n",
    "r(s_1,a_1),&\n",
    "r(s_2,a_1),&\n",
    "...,&\n",
    "r(s_n,a_1),&\n",
    "r(s_1,a_2),&\n",
    "...,&\n",
    "r(s_n,a_2),&\n",
    "...,&\n",
    "r(s_1,a_a),&\n",
    "...,&\n",
    "r(s_n,a_a)\n",
    "\\end{bmatrix}^T\n",
    "\\end{align}$.\n",
    "\n",
    "Then the problem becomes,\n",
    "\n",
    "$\\begin{align}\n",
    "\\min \\ \\ &c^TV\\\\\n",
    "\\text{subject to}: \\ \\ & AV\\geq b\n",
    "\\end{align}$\n",
    "\n",
    "#### Dual Program\n",
    "\n",
    "The Lagrangian for the primal problem is given by,\n",
    "\n",
    "$\\begin{align}\n",
    "\\mathcal{L}(V,\\lambda)=c^TV+\\lambda^T(b-AV)\n",
    "\\end{align}$\n",
    "where the dual variable $\\lambda\\in\\mathbb{R}^{na}$. Hence, the dual problem is,\n",
    "\n",
    "$\\begin{align}\n",
    "\\max & \\ \\ \\lambda^Tb\\\\\n",
    "\\text{Subject to} :&\\ \\  A^T\\lambda-c=0\\\\\n",
    "&\\ \\  \\lambda\\geq 0\n",
    "\\end{align}$,\n",
    "or writing explicitly,  \n",
    "\n",
    "\n",
    "$\\begin{align}\n",
    "\\max & \\ \\ \\sum_{a\\in A}\\sum_{s\\in S}\\lambda(s,a)b(s,a)\\\\\n",
    "\\text{Subject to} :&\\ \\  \\sum_{a\\in A}\\lambda(s,a)-\\alpha\\sum_{t\\in S}\\sum_{a\\in A}P(s \\ | \\ t,a)\\lambda(t,a)=c(s)\\ \\ \\ \\forall \\ \\ s\\in S\\\\\n",
    "&\\ \\  \\lambda(s,a)\\geq 0\\ \\ \\ \\forall s\\in S, \\ \\ a\\in A\n",
    "\\end{align}$\n",
    "\n",
    "where $\\lambda(s,a)$ variables intuitively represent the expected residence time in a state action pair. Although the deterministic policies are at least as good as randomized policies for examples we are going to use, note that the use of dual program provides an efficient way to extract the randomized policies from the output. Using the dual program, the optimal policy for each state can be obtained by using,\n",
    "\n",
    "$\\begin{align}\n",
    "P(a \\ | \\ s)= \n",
    "\\begin{cases} \\frac{\\lambda(s,a)}{\\sum_{a\\in A}\\lambda(s,a)}& \\text{if} \\sum_{a\\in A}\\lambda(s,a)\\neq 0\\\\\n",
    "\\text{arbitrary} & \\text{if} \\sum_{a\\in A}\\lambda(s,a)= 0\\end{cases}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as m\n",
    "import numpy.linalg as la\n",
    "from scipy.optimize import linprog\n",
    "import numpy.random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "from cvxpy import *\n",
    "import cvxpy as cvx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the MDP\n",
    "Grid world examples are straight forward MDPs to scale and to compare the convergence performance for each proposed MDP solution method. The Grid world example has rewards, which the agent wishes to acquire, and negative rewards, which the agent wishes to avoid.\n",
    "\n",
    "The following code generates an $n \\ x \\ m$ Grid world and allows the agent to choose between five possible actions in each state, namely, left, right, up, down and stay. Once the agent chooses an action, the next state is determined stochastically with some probability. The chosen state is attained at a probability $p$, and the other four potential states have a probablity of $\\frac{1-p}{4}$.\n",
    "#### Grid World Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the grid world generator with 5 possible actions for the agent i.e. (left,right,up,down,loop).\n",
    "# Inputs:\n",
    "# Row: Number of rows of the grid world\n",
    "# Col: Number of columns of the grid world\n",
    "# Prob: Probability of taking the desired action\n",
    "# Output:\n",
    "# P_0: Transition matrix with dimensions 5 x (Row x Col) x (Row x Col)  \n",
    "\n",
    "# When an action is chosen by the agent, it is performed with probability Prob and the remaining (1-Prob) probability\n",
    "# is distributed among other actions. This property is included to introduce stochasticity.\n",
    "# Output matrix is formed in a way that 0th row of the matrix represent transitions from the  bottom left corner \n",
    "# of the grid world. Similarly, the last row is for the upper right corner ((Row x Col)th grid).\n",
    "\n",
    "def Grid_world(Row,Col,Prob):\n",
    "    State=Row*Col\n",
    "    Actions=5\n",
    "    np.random.seed(0)\n",
    "    prob=Prob\n",
    "    P_0=np.zeros((Actions,State,State))\n",
    "    #action left\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[0,i,i]=prob+(1-prob)/4\n",
    "                    P_0[0,i,i+1]=(1-prob)/4\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[0,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i+1]=(1-prob)/4\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[0,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i+1]=(1-prob)/4\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[0,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i-1]=prob\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[0,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i-1]=prob\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[0,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i-1]=prob\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[0,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[0,i,i-1]=prob\n",
    "                P_0[0,i,i+1]=(1-prob)/4\n",
    "                P_0[0,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[0,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[0,i,i-1]=prob\n",
    "                P_0[0,i,i+1]=(1-prob)/4\n",
    "                P_0[0,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[0,i,i]=(1-prob)/4\n",
    "                P_0[0,i,i-1]=prob\n",
    "                P_0[0,i,i+1]=(1-prob)/4\n",
    "                P_0[0,i,i-Col]=(1-prob)/4\n",
    "                P_0[0,i,i+Col]=(1-prob)/4\n",
    "    # action right\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[1,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i+1]=prob\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[1,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i+1]=prob\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[1,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i+1]=prob\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[1,i,i]=prob+(1-prob)/4\n",
    "                    P_0[1,i,i-1]=(1-prob)/4\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[1,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i-1]=(1-prob)/4\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[1,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i-1]=(1-prob)/4\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[1,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[1,i,i-1]=(1-prob)/4\n",
    "                P_0[1,i,i+1]=prob\n",
    "                P_0[1,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[1,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[1,i,i-1]=(1-prob)/4\n",
    "                P_0[1,i,i+1]=prob\n",
    "                P_0[1,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[1,i,i]=(1-prob)/4\n",
    "                P_0[1,i,i-1]=(1-prob)/4\n",
    "                P_0[1,i,i+1]=prob\n",
    "                P_0[1,i,i-Col]=(1-prob)/4\n",
    "                P_0[1,i,i+Col]=(1-prob)/4\n",
    "    # action up\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i+1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i+1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                if i==State-Col:\n",
    "                    P_0[2,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i+1]=(1-prob)/4\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i-1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i-1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                if i==State-1:\n",
    "                    P_0[2,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i-1]=(1-prob)/4\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[2,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[2,i,i-1]=(1-prob)/4\n",
    "                P_0[2,i,i+1]=(1-prob)/4\n",
    "                P_0[2,i,i+Col]=prob\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[2,i,i]=prob+(1-prob)/4\n",
    "                P_0[2,i,i-1]=(1-prob)/4\n",
    "                P_0[2,i,i+1]=(1-prob)/4\n",
    "                P_0[2,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[2,i,i]=(1-prob)/4\n",
    "                P_0[2,i,i-1]=(1-prob)/4\n",
    "                P_0[2,i,i+1]=(1-prob)/4\n",
    "                P_0[2,i,i-Col]=(1-prob)/4\n",
    "                P_0[2,i,i+Col]=prob\n",
    "    # action down\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i+1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "                if i==0:\n",
    "                    P_0[3,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i+1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i+1]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i-1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "                if i==Col-1:\n",
    "                    P_0[3,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i-1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i-1]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[3,i,i]=prob+(1-prob)/4\n",
    "                P_0[3,i,i-1]=(1-prob)/4\n",
    "                P_0[3,i,i+1]=(1-prob)/4\n",
    "                P_0[3,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[3,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[3,i,i-1]=(1-prob)/4\n",
    "                P_0[3,i,i+1]=(1-prob)/4\n",
    "                P_0[3,i,i-Col]=prob\n",
    "            else:\n",
    "                P_0[3,i,i]=(1-prob)/4\n",
    "                P_0[3,i,i-1]=(1-prob)/4\n",
    "                P_0[3,i,i+1]=(1-prob)/4\n",
    "                P_0[3,i,i-Col]=prob\n",
    "                P_0[3,i,i+Col]=(1-prob)/4\n",
    "    # action loop\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4\n",
    "                    P_0[4,i,i+1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i+1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i+1]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4\n",
    "                    P_0[4,i,i-1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i-1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i-1]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[4,i,i]=prob+(1-prob)/4\n",
    "                P_0[4,i,i-1]=(1-prob)/4\n",
    "                P_0[4,i,i+1]=(1-prob)/4\n",
    "                P_0[4,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[4,i,i]=prob+(1-prob)/4\n",
    "                P_0[4,i,i-1]=(1-prob)/4\n",
    "                P_0[4,i,i+1]=(1-prob)/4\n",
    "                P_0[4,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[4,i,i]=prob\n",
    "                P_0[4,i,i-1]=(1-prob)/4\n",
    "                P_0[4,i,i+1]=(1-prob)/4\n",
    "                P_0[4,i,i-Col]=(1-prob)/4\n",
    "                P_0[4,i,i+Col]=(1-prob)/4\n",
    "    return P_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Problem Formulation and Constraints for the Linear Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "  def example(rows,columns,prob,discount,tolerance):\n",
    "    States=rows*columns\n",
    "    P_0=Grid_world(rows,columns,prob)\n",
    "    A=np.zeros((5*States,States)) # 5 is number of actions !\n",
    "    b=np.zeros((5*States,1))\n",
    "    # initial distributions USE 1 for each state!\n",
    "    c=np.ones(States)\n",
    "    # Constraints  Ax>=b\n",
    "    for a in range(5):\n",
    "        A[a*States:(a+1)*States,:]=np.eye(States)-discount*P_0[a,:,:]\n",
    "    # assign reward=1 to the top right cell and zero reward to others\n",
    "    for a in range(5):\n",
    "        b[(a+1)*(States)-1]=1\n",
    "    return A,b,c,P_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration is an application of Dynamic Programming and it is one the most understood and used algortihms to solve MDPs. The algorithm starts with arbitrary initial values of states. Due to the discount factor any point can be used as an initial point. In an iteration, for each state it finds the best action which maximizes the state value, using the old state values. After all state values are recalculated, a new iteration starts with the updated state values. The algorithm terminates when the difference between old and updated state value are below a threshold. [PUTERMAN]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valitr(P, R, discount, eps):\n",
    "    numOfStates = P.shape[2]\n",
    "    numOfActions = P.shape[0]\n",
    "    stateVals = np.zeros(numOfStates)\n",
    "    bestAction = np.zeros(numOfStates)\n",
    "    #df is the stopping criteria\n",
    "    df = np.zeros(1)\n",
    "    df[0] = 1\n",
    "    while la.norm(df,2) > eps:\n",
    "        stateValsNew = np.zeros(P.shape[2])\n",
    "        #for each state maximize the value\n",
    "        for s in range(numOfStates):\n",
    "            maxVal = -np.inf;\n",
    "            bestActOfs = -1;\n",
    "            #search every action\n",
    "            for a in range(numOfActions):\n",
    "                successors = np.where(P[a,s,:] > 0)\n",
    "                valOfa = R[s] + discount*np.dot(stateVals[successors], np.squeeze(P[a,s,successors]))\n",
    "                if valOfa > maxVal:\n",
    "                    maxVal = valOfa;\n",
    "                    bestActOfs = a;\n",
    "            bestAction[s] = bestActOfs\n",
    "            stateValsNew[s] = maxVal;\n",
    "        #update the state values and continue\n",
    "        df = stateValsNew - stateVals\n",
    "        stateVals = stateValsNew\n",
    "        \n",
    "    stateVals = np.resize(stateVals, (numOfStates,1))\n",
    "    \n",
    "    return stateVals, bestAction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy iteration is a well-known method to solve infinite horizon MDPs, which are known to have a stationary policy to maximize the objective function. The algorithm starts with an arbitrary deterministic policy, which induces a Markov Chain, or, in other words, each state brings only one constraint. After a policy is generated, it is evaluated to determine the new state values, which is subsequently used to determine the best actions and a new policy is generated. The new policy is generated such that it maximizes the state values of the new state values. The policy iteration algorithm terminates when the policy converges, i.e., the previous policy is identical to the current policy. [PUTERMAN] The main difference between this method and value iteration method is that value iteration method uses the old state values to update state values and relies on the convergence of state values, policy iteration method analytically calculates the state values at each step. Analytical computation of state values has complexity,$O(n^3)$, and causes problems when the number of states is relatively high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policyitr(P, R, discount, eps):\n",
    "    numOfStates = P.shape[2]\n",
    "    numOfActions = P.shape[0]\n",
    "    bestActionOld =  -1*np.ones(numOfStates)\n",
    "    bestActionNew = np.random.randint(numOfActions, size=numOfStates)\n",
    "    stateVals = np.zeros([numOfStates, 1]);\n",
    "    vals = []; #to keep state values;\n",
    "    #stops if the last two polies are same or\n",
    "    df = np.zeros(1)\n",
    "    df[0] = 1\n",
    "    while not(np.array_equal(bestActionOld, bestActionNew) or la.norm(df,2) < eps):\n",
    "        bestActionOld =np.copy(bestActionNew);\n",
    "        #constructs linear equation to find new values of states\n",
    "        A = np.zeros([numOfStates, numOfStates])\n",
    "        #if there are rewards for actions this line must be changed\n",
    "        b = R;\n",
    "        b.resize([numOfStates,1])\n",
    "        for s in range(numOfStates):\n",
    "            ts = discount*P[bestActionNew[s],s,:];\n",
    "            ts.resize([1,numOfStates]);\n",
    "            A[s,:] = -ts;\n",
    "        #update the state values and continue\n",
    "        A = A + np.identity(numOfStates);\n",
    "        stateValsNew = np.linalg.solve(A,b);\n",
    "        df = stateValsNew-stateVals;\n",
    "        stateVals = stateValsNew\n",
    "        vals.append(stateVals)\n",
    "        #for each state maximize the value\n",
    "        for s in range(numOfStates):\n",
    "            maxVal = -np.inf;\n",
    "            bestActOfs = -1;\n",
    "            #search every action\n",
    "            for a in range(numOfActions):\n",
    "                successors = np.where(P[a,s,:] > 0)\n",
    "                valOfa = R[s] + discount*np.dot(np.squeeze(stateVals[successors]), np.squeeze(P[a,s,successors]))\n",
    "                if valOfa > maxVal:\n",
    "                    maxVal = valOfa;\n",
    "                    bestActOfs = a;\n",
    "            bestActionNew[s] = bestActOfs\n",
    "    error = []\n",
    "    for i in range(len(vals)):\n",
    "        error.append(np.linalg.norm(vals[i] - vals[-1], 2))\n",
    "    return stateVals, bestActionNew, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CVXPY Solvers for the primal problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ECOS_Primal(A,b,c,P_0,tol):\n",
    "    Y=cvx.Variable(P_0.shape[1]) # number of states\n",
    "    cons=[A*Y>=b]\n",
    "    objective=cvx.Minimize(cvx.sum_entries(cvx.mul_elemwise(c,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    prob.solve(solver=ECOS,reltol=tol,max_iters=1000,verbose=False)\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return Y.value\n",
    "def SCS_Primal(A,b,c,P_0,tol):\n",
    "    Y=cvx.Variable(P_0.shape[1]) # number of states\n",
    "    cons=[A*Y>=b]\n",
    "    objective=cvx.Minimize(cvx.sum_entries(cvx.mul_elemwise(c,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    prob.solve(solver=SCS,eps=tol,max_iters=1000,verbose=False)\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return Y.value\n",
    "def CVXOPT_Primal(A,b,c,P_0,tol):\n",
    "    Y=cvx.Variable(P_0.shape[1]) # number of states\n",
    "    cons=[A*Y>=b]\n",
    "    objective=cvx.Minimize(cvx.sum_entries(cvx.mul_elemwise(c,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    prob.solve(solver=CVXOPT,reltol=tol,max_iters=1000,verbose=False)\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return Y.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Order Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection Function\n",
    "Both gradient descent and accelerated gradient method requires a projection such that the solution is a feasible point in the constraining set. Due to the nature of the linear program, the constraining set is a set of $ (n \\cdot m \\cdot a)$ affine inequalities $ Ax \\geq b$. A projection onto this constraining set is difficult, and so an approximate method called alternating projection method. \n",
    "\n",
    "The alternating projection method requires a set of closed, convex sets, however, this can be extended to a set of constraining affine inequalities [Boyd]. The method finds all of the affine inequalities that are violated by the current value $z$, and for those inequality constraints determines the projection onto the individual half-space defined by the constraint. For the constraints that are not violated, the projection is just $z$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(y, A, b):\n",
    "    x = y\n",
    "    maxIter = 1000\n",
    "    j = 0\n",
    "    while np.min(np.dot(A,x) - b) < 0 and j <= maxIter:\n",
    "        # find the constraints that are violated\n",
    "        constrainedBool = np.greater_equal(np.dot(A, x), b)\n",
    "        \n",
    "        # for each violated constrained perform the projection\n",
    "        for i in range(constrainedBool.shape[0]):\n",
    "            if not np.all(constrainedBool[i]):\n",
    "                # intermediate residual\n",
    "                qq=(b[i] - np.dot(A[i,:], x))\n",
    "                \n",
    "                # projection\n",
    "                x = x + np.reshape(qq[0]*A[i,:]/np.dot(A[i,:].T, A[i,:]), (x.shape[0],x.shape[1]))\n",
    "        # iteration counter\n",
    "        j = j + 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projected Gradient Descent\n",
    "Projected gradient descent updates the state values and then projects the state values onto the constraining set using the above projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graddes(x, A, b, t, eta0):\n",
    "    eta = eta0/np.sqrt(t + 1)\n",
    "    # gradient is always a vector of ones of shape x.shape\n",
    "    grad = np.ones((x.shape[0], 1))\n",
    "    y = x - eta*grad\n",
    "\n",
    "    x = proj(y, A, b)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projected Accelerated Gradient Method\n",
    "The accelerated gradient method performs the same general algorithm as projected gradient descent, however, accelerated gradient method uses the \"momentum\" of the gradient update to nudge the new solution in the same direction of the gradient. Accelerated gradient methods have faster convergence rates than gradient descent methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accelgrad(x, v, theta, A, b, t, eta0):\n",
    "    eta = eta0/np.sqrt(t+1)\n",
    "    grad = np.ones(x.shape)\n",
    "    \n",
    "    v = v - eta*grad\n",
    "    \n",
    "    theta_old = theta\n",
    "    theta = (1 + np.sqrt(1 + 4*theta**2))/2\n",
    "    \n",
    "    xprev = x\n",
    "    x = proj(v, A, b)\n",
    "    \n",
    "    v = x + (theta_old - 1)/(theta)*(x - xprev)\n",
    "    \n",
    "    return x, v, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent(update, update_name, x, A, b, eta0, tol, optVal, T=int(1e4)):\n",
    "    v = x\n",
    "    theta = 1.\n",
    "    error = []\n",
    "    \n",
    "    t = int(0)\n",
    "    xnew = x + 1;\n",
    "    while t <=  T and la.norm(x - xnew, 2) > tol:\n",
    "        x = xnew\n",
    "        if update_name == \"gradient\":\n",
    "            xnew = update(x, A, b, t, eta0)\n",
    "            \n",
    "        elif update_name == \"accelerated\":\n",
    "            xnew, v, theta = update(x, v, theta, A, b, t, eta0)\n",
    "            \n",
    "        if(t % 1 == 0) or (t == T - 1):\n",
    "            error.append(la.norm(x - optVal,2))\n",
    "        t = int(t + 1)\n",
    "    \n",
    "    return x, error, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interior Point Method (Barrier Method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barrier Method is an Interior Point Method to solve convex optimization problems with inequality constraints. In Barrier method, the constarints are added to the main problem as an extra function. This extra function goes to infinity when the optimized variable gets closer to the boundaries of the convex set i.e. it encourages the variable to stay in the feasible region. The algorithm starts from the analytical center which minimizes the barrier function. As the number of iterations increases, the original function becomes comparable with the barrier function and the optimized variable gets closer to the boundaries more. In this project the barrier function is chosen to be logarithmic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBarrierGradient(A,b,x):\n",
    "    #calculates the gradient of barrier function for IPM Ax>=b\n",
    "    d = np.zeros([A.shape[0],1])\n",
    "    for i in range(A.shape[0]):\n",
    "        d[i,0] = -1.0/(np.dot(A[i,:],x) - b[i])\n",
    "    g = np.dot(np.transpose(A),d)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBarrierHessian(A,b,x):\n",
    "    #calculates the hessian of barrier function for IPM Ax>=b\n",
    "    d = np.zeros([A.shape[0],1])\n",
    "    dMat = np.zeros([A.shape[0],A.shape[0]])\n",
    "    for i in range(A.shape[0]):\n",
    "        dMat[i,i] = 1.0/((np.dot(A[i,:],x) - b[i])**2)  \n",
    "    dummy = np.dot(np.transpose(A),dMat)\n",
    "    H = np.dot(dummy,A)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAnalyticCenter(A,b,xFeasible,T):\n",
    "    #finds the analytical center for IPM using gradient descent\n",
    "    for i in range (T):\n",
    "        g = findBarrierGradient(A,b,xFeasible)\n",
    "        xFeasible = xFeasible - 0.01/(i+1)*g;\n",
    "    return xFeasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findOptimalSolution(A,b,stateValsNew,alpha,eps, stateValsOpt):\n",
    "    #finds the optimal solution using IPM\n",
    "    error = []\n",
    "    t = 1;\n",
    "    #to make it enter the loop\n",
    "    stateVals = stateValsNew + 1;\n",
    "    itr = 1;\n",
    "    while (np.linalg.norm(stateVals-stateValsNew,2) > eps):\n",
    "        stateVals = stateValsNew\n",
    "        #Finds the gradient and hessian\n",
    "        g = t*np.ones([stateVals.size, 1]) + findBarrierGradient(A,b,stateVals)\n",
    "        H = findBarrierHessian(A,b,stateVals)\n",
    "        #Updates the calues\n",
    "        stateValsNew = stateVals - np.dot(np.linalg.inv(H),g)\n",
    "        #stateValsNew = stateVals - 1/np.sqrt(itr)*g;\n",
    "        error.append(np.linalg.norm(stateValsNew - stateValsOpt,2))\n",
    "        t = t*(1+alpha)\n",
    "        if np.isnan(error[-1]):\n",
    "                return stateVals , error\n",
    "    return stateVals, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interiorPoint(A, b, discount, eps, alpha, stateValsOpt):\n",
    "    #finds another feasible point using the optimal solution\n",
    "    xFeasible = stateValsOpt +1;\n",
    "    #finds analytical center\n",
    "    xF = findAnalyticCenter(A,b,xFeasible,1000);\n",
    "    stateVals, error = findOptimalSolution(A,b,xF,alpha,eps, stateValsOpt)\n",
    "    return stateVals, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotGridWorld(R, bestAction, Rows, Columns):\n",
    "    imgData = np.resize(R,[Rows, Columns]);\n",
    "    imgData = np.flipud(imgData);    \n",
    "    plt.imshow(imgData, interpolation='nearest')\n",
    "    bestAction = np.resize(bestAction,[Rows, Columns])\n",
    "    for a in range(Rows):\n",
    "        for b in range(Columns): \n",
    "            if bestAction[Rows -1 - a,b]== 0:\n",
    "                plt.text(b,a ,r'$ \\leftarrow $')\n",
    "            elif bestAction[Rows -1 - a,b]== 1:\n",
    "                plt.text(b,a,r'$ \\rightarrow $')\n",
    "            elif bestAction[Rows -1 - a,b]== 2:\n",
    "                plt.text(b,a,r'$ \\uparrow $')\n",
    "            elif bestAction[Rows -1 - a,b] == 3:\n",
    "                plt.text(b,a,r'$ \\downarrow $')\n",
    "            elif bestAction[Rows -1 - a,b] == 4:\n",
    "                plt.text(b,a ,r'$ o $')\n",
    "            #plt.text(b,a,r'$ \\leftarrow $')\n",
    "            \n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "In the first example we use a 5 by 5 grid world example to compare the required number of iterations for different methods to achieve a desired tolerance level. The agent starts from the bottom left grid and its aim is to reach the top left grid while avoiding some of the intermediate grids. We solve this problem using (1) value iteration, (2) policy iteration, (3) simplex method, (4) cvxpy with SCS,ECOS and CVXOPT solvers, (5) projected gradient descent, (6) accelerated projected gradient descent and (7) interior point method. We assume that the agent takes the chosen action with probability 0.8, and we use the discount factor of 0.9. The desired tolerance is $10^{-7}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-8cff06ed5cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mscs_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSCS_Primal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP_0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# SCS Primal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcvxopt_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCVXOPT_Primal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP_0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# CVXOPT Primal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprojgrad_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojgrad_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojgrad_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraddes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gradient\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalit_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Projected Gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0maccgrad_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccgrad_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccgrad_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccelgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accelerated\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalit_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Accelerated Gradient Descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0minterior_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterior_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minteriorPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalit_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Interior Point Method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vals' is not defined"
     ]
    }
   ],
   "source": [
    "rows,columns,prob,discount,tol=5,5,0.8,0.9,1e-7\n",
    "A,b,c,P_0=example(rows,columns,prob,discount,tol)\n",
    "valit_val, bestAction = valitr(P_0, b, discount, tol) # Value Iteration\n",
    "polyit_val, bestAction2,error = policyitr(P_0, b[0:P_0.shape[1]], discount, tol) # Policy Iteration\n",
    "res = linprog(c, -A, -b, A_eq=None, b_eq=None, bounds=None, method='simplex',\\\n",
    "              callback=None, options={'disp': False, 'bland': False, 'tol': tol, 'maxiter': 1000}) # Simplex\n",
    "ecos_val=ECOS_Primal(A,b,c,P_0,tol) # ECOS Primal\n",
    "scs_val=SCS_Primal(A,b,c,P_0,tol) # SCS Primal\n",
    "cvxopt_val=CVXOPT_Primal(A,b,c,P_0,tol) # CVXOPT Primal\n",
    "projgrad_val, projgrad_error, projgrad_iter = descent(graddes, \"gradient\", vals+100, A, b, 1, tol, valit_val) # Projected Gradient\n",
    "accgrad_val, accgrad_error, accgrad_iter = descent(accelgrad, \"accelerated\", vals+100, A, b, 1e-1, tol, valit_val) # Accelerated Gradient Descent\n",
    "interior_val, interior_error=interiorPoint(A, b, discount, tol, 0.01, valit_val) # Interior Point Method\n",
    "plotGridWorld(b[0:P_0.shape[1]], bestAction2, rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds another feasible point using the optimal solution\n",
    "valsFeasible = polyit_val +1;\n",
    "#finds analytical center\n",
    "vals_analytical = findAnalyticCenter(A,b,valsFeasible,1000);\n",
    "print(np.min(np.dot(A,vals_analytical)-b))\n",
    "projgrad_val, projgrad_error, projgrad_iter = descent(graddes, \"gradient\", vals_analytical, A, b, 1, tol, valit_val) # Projected Gradient\n",
    "accgrad_val, accgrad_error, accgrad_iter = descent(accelgrad, \"accelerated\", vals_analytical, A, b, 1, tol, valit_val, T = 10) # Accelerated Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
