{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Solutions for Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Markov Decision Processes (MDPs) provide a mathematical formulation for stochastic sequential decision making. Stochastic sequential decision making has roots in many fields [Puterman] \n",
    "\n",
    "In this project, we will be comparing a variety of classic dynamic programming and linear programming methods. From the dynamic programming realm, the solution methods are:\n",
    "-  Value Iteration\n",
    "-  Policy Iteration\n",
    "\n",
    "and for linear programming methods:\n",
    "-  First order methods\n",
    "    -  Projected Gradient Method\n",
    "    -  Projected Accelerated Gradient Method\n",
    "-  Interior point methods\n",
    "-  Simplex methods\n",
    "-  CVXPY toolset (SCS, ECOS, CVXOPT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Formulation\n",
    "$\\textbf{Definition 1:}$ A Markov decision process (MDP) is a tuple $\\mathcal{M}=(S,s_0,A,P)$ where $S$ is the finite set of states, i.e. $|S|=n$, $s_0\\in S$ is the initial state, $A$ is the finite set of actions, i.e. $|A|=a$, and $P: S\\times A \\rightarrow[0,1]^{n}$ is the transition function. For any given $(s,a)$, $P$ satisfies $\\sum_{t\\in S}P(t| s,a)=1$ and $P(t | s,a)\\geq 0$. Assume for simplicity that all actions are available in all states.\n",
    "\n",
    "An infinite run from the initial state $s_0$ is a sequence $\\rho$$=$$s_0a_0s_1a_1s_2...$ of states and actions such that for all $k\\geq 0$, we have $P(s_{k+1}|s_k,a_k)$$>$$0$. A policy specifies a procedure for action selection in each state depending on the history of states and actions. A deterministic policy is a function $\\pi : S \\rightarrow A$ that maps the set of states into the set of available actions for the input state. For a given MDP $\\mathcal{M}$, we denote the set of all possible deterministic policies by $\\Pi(\\mathcal{M})$.\n",
    "\n",
    "To define a reward maximization problem, we relate each state transition with a real valued reward, using the reward function $r : S\\times A\\times S \\rightarrow \\mathbb{R}$. Then, the expected total reward of an infinite horizon decision process following the policy $\\pi$ is given as follows,\n",
    "\n",
    "$\\begin{align}\n",
    "V^{\\pi}(s)=\\lim_{N\\rightarrow \\infty}\\mathbb{E}\\Big[\\sum_{k=0}^{N-1}\\alpha^kr(s_i,\\pi(s),s_{i+1})\\Big| i_0=s_0\\Big]\n",
    "\\end{align}$ $\\forall s\\in S$\n",
    "\n",
    "where $V^{\\pi}(s)$ is the value of state $s$ under the policy $\\pi$. \n",
    "\n",
    "\n",
    "Suppose that our aim is to find a policy that maximizes the expected total reward. The Bellman equation and optimality conditions provide the following formulation to compute maximum values for states and corresponding actions which achieves the maximum reward values,\n",
    "\n",
    "$\\begin{align}\n",
    "V^{\\star}(s)&=\\max_{a}\\sum_{t\\in S}P(t|s,a)\\Big[r(s,a,t)+\\alpha V^{\\star}(t)\\Big]\\\\\n",
    "\\pi^{\\star}(s)&=\\arg\\max_{a}\\sum_{t\\in S}P(t|s,a)\\Big[r(s,a,t)+\\alpha V^{\\star}(t)\\Big]\\\\\n",
    "\\end{align}$\n",
    "\n",
    "Let $T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ be an operator defined as,\n",
    "\n",
    "$(TV)(s)=\\max_{a}\\sum_{t\\in S}P(t|s,a)\\Big[r(s,a,t)+\\alpha V(t)\\Big]$.\n",
    "\n",
    "Then, the optimality condition is given by $TV^{\\star}=V^{\\star}$. Additionally, from well known results, $T$ is a monotone operator and a contraction mapping. Hence, it satisfies that (i) for any $V$ satisfying $V\\geq TV$, $V\\geq V^{\\star}$ where inequalities are elementwise, (ii) it has a unique solution to $V^{\\star}=TV^{\\star}$ . Using this fact, and defining $r(s,a)=\\sum_{t\\in S}P(t|s,a)r(s,a,t)$, we can formulate finding the maximum expected reward problem as a linear program as following,\n",
    "\n",
    "$\\begin{align}\n",
    "\\min \\ \\ &c^TV\\\\\n",
    "\\text{subject to}: \\ \\ & V(s)\\geq r(s,a)+\\alpha\\sum_{t\\in S}P(t|s,a)V(t)\n",
    "\\end{align}$\n",
    "\n",
    "where $c\\in\\mathbb{R}_{++}^n$. Having $c>0$ for each state ensures the uniqueness of the solution. Let $P_{a_1}\\in[0,1]^{n\\times n}$ be a transition matrix for action $a_1$. We define the following matrices,\n",
    "\n",
    "$\\begin{align}\n",
    "A=\\begin{bmatrix}\n",
    "I_{n\\times n}-\\alpha P_{a_1}\\\\\n",
    "I_{n\\times n}-\\alpha P_{a_2}\\\\\n",
    "...\\\\\n",
    "I_{n\\times n}-\\alpha P_{a_a}\n",
    "\\end{bmatrix}\n",
    "\\end{align}$ ,\n",
    "$\\begin{align}\n",
    "b=\\begin{bmatrix}\n",
    "r(s_1,a_1),&\n",
    "r(s_2,a_1),&\n",
    "...,&\n",
    "r(s_n,a_1),&\n",
    "r(s_1,a_2),&\n",
    "...,&\n",
    "r(s_n,a_2),&\n",
    "...,&\n",
    "r(s_1,a_a),&\n",
    "...,&\n",
    "r(s_n,a_a)\n",
    "\\end{bmatrix}^T\n",
    "\\end{align}$.\n",
    "\n",
    "Then the problem becomes,\n",
    "\n",
    "$\\begin{align}\n",
    "\\min \\ \\ &c^TV\\\\\n",
    "\\text{subject to}: \\ \\ & AV\\geq b\n",
    "\\end{align}$\n",
    "\n",
    "#### Dual Program\n",
    "\n",
    "The Lagrangian for the primal problem is given by,\n",
    "\n",
    "$\\begin{align}\n",
    "\\mathcal{L}(V,\\lambda)=c^TV+\\lambda^T(b-AV)\n",
    "\\end{align}$\n",
    "where the dual variable $\\lambda\\in\\mathbb{R}^{na}$. Hence, the dual problem is,\n",
    "\n",
    "$\\begin{align}\n",
    "\\max & \\ \\ \\lambda^Tb\\\\\n",
    "\\text{Subject to} :&\\ \\  A^T\\lambda-c=0\\\\\n",
    "&\\ \\  \\lambda\\geq 0\n",
    "\\end{align}$,\n",
    "or writing explicitly,  \n",
    "\n",
    "\n",
    "$\\begin{align}\n",
    "\\max & \\ \\ \\sum_{a\\in A}\\sum_{s\\in S}\\lambda(s,a)b(s,a)\\\\\n",
    "\\text{Subject to} :&\\ \\  \\sum_{a\\in A}\\lambda(s,a)-\\alpha\\sum_{t\\in S}\\sum_{a\\in A}P(s|t,a)\\lambda(t,a)=c(s)\\ \\ \\ \\forall \\ \\ s\\in S\\\\\n",
    "&\\ \\  \\lambda(s,a)\\geq 0\\ \\ \\ \\forall s\\in S, \\ \\ a\\in A\n",
    "\\end{align}$\n",
    "\n",
    "where $\\lambda(s,a)$ variables intuitively represent the expected residence time in a state action pair. Although the deterministic policies are at least as good as randomized policies for examples we are going to use, note that the use of dual program provides an efficient way to extract the randomized policies from the output. Using the dual program, the optimal policy for each state can be obtained by using,\n",
    "\n",
    "$\\begin{align}\n",
    "P(a| s)= \n",
    "\\begin{cases} \\frac{\\lambda(s,a)}{\\sum_{a\\in A}\\lambda(s,a)}& \\text{if} \\sum_{a\\in A}\\lambda(s,a)\\neq 0\\\\\n",
    "\\text{arbitrary} & \\text{if} \\sum_{a\\in A}\\lambda(s,a)= 0\\end{cases}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Required Packages\n",
    "\n",
    "import numpy as np\n",
    "import math as m\n",
    "import time\n",
    "import numpy.linalg as la\n",
    "from scipy.optimize import linprog\n",
    "import numpy.random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "from cvxpy import *\n",
    "import cvxpy as cvx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of a candidate Grid World that will be solved using the above methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the MDP\n",
    "First, we use grid world examples to compare the performance of various methods in terms of scalability and required number of iterations to satisfy the desired error bounds on the state values. The following code generates an (n,m) grid world and allows the agent to choose between five possible actions in each state, namely left, right, up, down and stay. Once the agent chooses an action, the next state is determined stochastically.\n",
    "\n",
    "Second, we use a randomly generated MDP to compare the performance of the same algorithms when used with large number of available actions. Note that having large number of actions does not change the number of variables but increases the number of constraints in the primal problem. \n",
    "\n",
    "#### Grid World Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the grid world generator with 5 possible actions for the agent i.e. (left,right,up,down,loop).\n",
    "# Inputs:\n",
    "# Row: Number of rows of the grid world\n",
    "# Col: Number of columns of the grid world\n",
    "# Prob: Probability of taking the desired action\n",
    "# Output:\n",
    "# P_0: Transition matrix with dimensions 5 x (Row x Col) x (Row x Col)  \n",
    "\n",
    "# When an action is chosen by the agent, it is performed with probability Prob and the remaining (1-Prob) probability\n",
    "# is distributed among other actions. This property is included to introduce stochasticity.\n",
    "# Output matrix is formed in a way that 0th row of the matrix represent transitions from the  bottom left corner \n",
    "# of the grid world. Similarly, the last row is for the upper right corner ((Row x Col)th grid).\n",
    "\n",
    "def Grid_world(Row,Col,Prob):\n",
    "    State=Row*Col\n",
    "    Actions=5\n",
    "    np.random.seed(0)\n",
    "    prob=Prob\n",
    "    P_0=np.zeros((Actions,State,State))\n",
    "    #action left\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[0,i,i]=prob+(1-prob)/4\n",
    "                    P_0[0,i,i+1]=(1-prob)/4\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[0,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i+1]=(1-prob)/4\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[0,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i+1]=(1-prob)/4\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[0,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i-1]=prob\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[0,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i-1]=prob\n",
    "                    P_0[0,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[0,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[0,i,i-1]=prob\n",
    "                    P_0[0,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[0,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[0,i,i-1]=prob\n",
    "                P_0[0,i,i+1]=(1-prob)/4\n",
    "                P_0[0,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[0,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[0,i,i-1]=prob\n",
    "                P_0[0,i,i+1]=(1-prob)/4\n",
    "                P_0[0,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[0,i,i]=(1-prob)/4\n",
    "                P_0[0,i,i-1]=prob\n",
    "                P_0[0,i,i+1]=(1-prob)/4\n",
    "                P_0[0,i,i-Col]=(1-prob)/4\n",
    "                P_0[0,i,i+Col]=(1-prob)/4\n",
    "    # action right\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[1,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i+1]=prob\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[1,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i+1]=prob\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[1,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i+1]=prob\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[1,i,i]=prob+(1-prob)/4\n",
    "                    P_0[1,i,i-1]=(1-prob)/4\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[1,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i-1]=(1-prob)/4\n",
    "                    P_0[1,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[1,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[1,i,i-1]=(1-prob)/4\n",
    "                    P_0[1,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[1,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[1,i,i-1]=(1-prob)/4\n",
    "                P_0[1,i,i+1]=prob\n",
    "                P_0[1,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[1,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[1,i,i-1]=(1-prob)/4\n",
    "                P_0[1,i,i+1]=prob\n",
    "                P_0[1,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[1,i,i]=(1-prob)/4\n",
    "                P_0[1,i,i-1]=(1-prob)/4\n",
    "                P_0[1,i,i+1]=prob\n",
    "                P_0[1,i,i-Col]=(1-prob)/4\n",
    "                P_0[1,i,i+Col]=(1-prob)/4\n",
    "    # action up\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i+1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i+1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                if i==State-Col:\n",
    "                    P_0[2,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i+1]=(1-prob)/4\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i-1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[2,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i-1]=(1-prob)/4\n",
    "                    P_0[2,i,i+Col]=prob\n",
    "                if i==State-1:\n",
    "                    P_0[2,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[2,i,i-1]=(1-prob)/4\n",
    "                    P_0[2,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[2,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[2,i,i-1]=(1-prob)/4\n",
    "                P_0[2,i,i+1]=(1-prob)/4\n",
    "                P_0[2,i,i+Col]=prob\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[2,i,i]=prob+(1-prob)/4\n",
    "                P_0[2,i,i-1]=(1-prob)/4\n",
    "                P_0[2,i,i+1]=(1-prob)/4\n",
    "                P_0[2,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[2,i,i]=(1-prob)/4\n",
    "                P_0[2,i,i-1]=(1-prob)/4\n",
    "                P_0[2,i,i+1]=(1-prob)/4\n",
    "                P_0[2,i,i-Col]=(1-prob)/4\n",
    "                P_0[2,i,i+Col]=prob\n",
    "    # action down\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i+1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "                if i==0:\n",
    "                    P_0[3,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i+1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i+1]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i-1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "                if i==Col-1:\n",
    "                    P_0[3,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i-1]=(1-prob)/4\n",
    "                    P_0[3,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[3,i,i]=(1-prob)/4+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[3,i,i-1]=(1-prob)/4\n",
    "                    P_0[3,i,i-Col]=prob\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[3,i,i]=prob+(1-prob)/4\n",
    "                P_0[3,i,i-1]=(1-prob)/4\n",
    "                P_0[3,i,i+1]=(1-prob)/4\n",
    "                P_0[3,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[3,i,i]=(1-prob)/4+(1-prob)/4\n",
    "                P_0[3,i,i-1]=(1-prob)/4\n",
    "                P_0[3,i,i+1]=(1-prob)/4\n",
    "                P_0[3,i,i-Col]=prob\n",
    "            else:\n",
    "                P_0[3,i,i]=(1-prob)/4\n",
    "                P_0[3,i,i-1]=(1-prob)/4\n",
    "                P_0[3,i,i+1]=(1-prob)/4\n",
    "                P_0[3,i,i-Col]=prob\n",
    "                P_0[3,i,i+Col]=(1-prob)/4\n",
    "    # action loop\n",
    "    for i in range(P_0.shape[1]):\n",
    "            if i%Col==0:\n",
    "                if i!=0 and i!=State-Col:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4\n",
    "                    P_0[4,i,i+1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "                if i==0:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i+1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                if i==State-Col:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i+1]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "            elif i%Col==Col-1:\n",
    "                if i!=Col-1 and i!=State-1:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4\n",
    "                    P_0[4,i,i-1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "                if i==Col-1:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i-1]=(1-prob)/4\n",
    "                    P_0[4,i,i+Col]=(1-prob)/4\n",
    "                if i==State-1:\n",
    "                    P_0[4,i,i]=prob+(1-prob)/4+(1-prob)/4\n",
    "                    P_0[4,i,i-1]=(1-prob)/4\n",
    "                    P_0[4,i,i-Col]=(1-prob)/4\n",
    "            elif i>0 and i<Col-1:\n",
    "                P_0[4,i,i]=prob+(1-prob)/4\n",
    "                P_0[4,i,i-1]=(1-prob)/4\n",
    "                P_0[4,i,i+1]=(1-prob)/4\n",
    "                P_0[4,i,i+Col]=(1-prob)/4\n",
    "            elif i>(Row-1)*Col and i<State-1:\n",
    "                P_0[4,i,i]=prob+(1-prob)/4\n",
    "                P_0[4,i,i-1]=(1-prob)/4\n",
    "                P_0[4,i,i+1]=(1-prob)/4\n",
    "                P_0[4,i,i-Col]=(1-prob)/4\n",
    "            else:\n",
    "                P_0[4,i,i]=prob\n",
    "                P_0[4,i,i-1]=(1-prob)/4\n",
    "                P_0[4,i,i+1]=(1-prob)/4\n",
    "                P_0[4,i,i-Col]=(1-prob)/4\n",
    "                P_0[4,i,i+Col]=(1-prob)/4\n",
    "    return P_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly Generated MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code generates a random MDP using given number of states and actions.\n",
    "# Inputs:\n",
    "# numOfStates: Number of states of MDP\n",
    "# numOfActions: Number of actions for all states\n",
    "# Output:\n",
    "# P_0: Probability matrix with dimension (a x s x s)\n",
    "\n",
    "#It is assumed that all actions are active in all states. Number of possible transitions for each state \n",
    "#action pair is determined using uniform distribution. Then, transition probabilities are determined \n",
    "#using uniform distribution and normalized afterwards.\n",
    "\n",
    "\n",
    "def randomMDP(numOfStates, numOfActions):\n",
    "    np.random.seed(1)\n",
    "    P_0=np.zeros((numOfActions,numOfStates,numOfStates))\n",
    "    for act in range(numOfActions):\n",
    "        for i in range(P_0.shape[1]):\n",
    "            outgoingNum = 1 + np.random.randint(numOfActions, size=1);\n",
    "            outgoingStateList = np.random.choice(numOfStates, outgoingNum, replace=False)\n",
    "            probs = np.random.rand(outgoingNum[0])\n",
    "            probs = probs/sum(probs)\n",
    "            for j in range(outgoingNum):\n",
    "                P_0[act, i, outgoingStateList[j]] = probs[j]\n",
    "\n",
    "    return P_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Problem Formulation and Constraints for the Linear Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Given a grid world, the following code generates constraints for the primal linear program\n",
    "# Inputs:\n",
    "# rows: Number of rows of the grid world\n",
    "# columns: Number of columns of the grid world\n",
    "# prob: Probability of taking the desired action\n",
    "# discount: Discount factor\n",
    "# tolerance: Desired tolerance \n",
    "\n",
    "# Output:\n",
    "# A: Rearranged probability matrix constraint\n",
    "# b: Bounds\n",
    "# c: Coefficients for decision variables\n",
    "# P_0: Transition matrix with dimensions 5 x (Row x Col) x (Row x Col)\n",
    "\n",
    "# The primal problem is \n",
    "# min   c^T x\n",
    "# s.t.  Ax>= b\n",
    "\n",
    "def Primal_parameters_1(rows,columns,prob,discount,tolerance):\n",
    "    States=rows*columns\n",
    "    P_0=Grid_world(rows,columns,prob)\n",
    "    A=np.zeros((5*States,States)) # 5 is number of actions !\n",
    "    b=np.zeros((5*States,1))\n",
    "    # initial distributions USE 1 for each state to ensure the converge to the unique solution!\n",
    "    c=np.ones(States)\n",
    "    # Constraints  Ax>=b\n",
    "    for a in range(5):\n",
    "        A[a*States:(a+1)*States,:]=np.eye(States)-discount*P_0[a,:,:]\n",
    "    # assign reward=1 to the top right cell and zero reward to others\n",
    "    for a in range(5):\n",
    "        b[(a+1)*(States)-1]=5\n",
    "        b[(a+1)*(States)-1-17]=-3\n",
    "        b[(a+1)*(States)-1-13]=-3\n",
    "    bound=np.max(b)/(1-discount)+100\n",
    "    dum=-np.eye(States)\n",
    "    dum2=-bound*np.ones((States,1))\n",
    "    A=np.concatenate((A,dum), axis=0)\n",
    "    b=np.concatenate((b,dum2), axis=0)\n",
    "    return A,b,c,P_0\n",
    "\n",
    "# Given a primal LP, the following function outputs constraints for the dual problem\n",
    "\n",
    "# Input:\n",
    "# A: Probability matrix constraint\n",
    "# b: Bounds\n",
    "# c: Coefficients for decision variables\n",
    "\n",
    "# Output:\n",
    "# A.T: Rearranged probability matrix constraint\n",
    "# b: bounds\n",
    "# c: coefficients for decision variables\n",
    "\n",
    "# The dual problem is \n",
    "# max b^T y\n",
    "# s.t. A^T y = c\n",
    "#      y>=0\n",
    "\n",
    "def Dual_parameters_1(A,b,c):\n",
    "    return A.T,b,c\n",
    "\n",
    "# The following code generates constraints for the random MDP input\n",
    "# Inputs and outputs are the same with minor changes as explained above \n",
    "def Primal_parameters_2(P_0,discount,tolerance):\n",
    "    actions=P_0.shape[0]\n",
    "    States=P_0.shape[1]\n",
    "    A=np.zeros((actions*States,States)) # 5 is number of actions !\n",
    "    b=np.zeros((actions*States,1))\n",
    "    # initial distributions USE 1 for each state to ensure the converge to the unique solution!\n",
    "    c=np.ones(States)\n",
    "    # Constraints  Ax>=b\n",
    "    for a in range(actions):\n",
    "        A[a*States:(a+1)*States,:]=np.eye(States)-discount*P_0[a,:,:]\n",
    "    # assign reward=1 to the top right cell and zero reward to others\n",
    "    for a in range(actions):\n",
    "        b[(a+1)*(States)-1]=3\n",
    "        b[(a+1)*(States)-1-17]=-10\n",
    "        b[(a+1)*(States)-1-13]=-10\n",
    "    return A,b,c,P_0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods \n",
    "The following codes are methods we use to solve 'given an MDP and a reward structure, find the optimal state values and policies achieving it' problem. First, we find the optimal states values and a policy achieving it by the policy iteration algorithm. Then using its results, we keep the log of relative error values of each iteration for other methods. The elapsed time when using different methods are all measured using 'time.time()' function. \n",
    "\n",
    "We also solve the dual problem using different methods\n",
    "\n",
    "#### 1) Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following function performs the policy iteration algorithm\n",
    "# Inputs:\n",
    "# P: Probability matrix\n",
    "# R: Reward vector for states\n",
    "# discount: Discount factor\n",
    "# eps: tolerance \n",
    "\n",
    "# Outputs:\n",
    "# stateVals: Value vector for states\n",
    "# bestActionNew : Action vector for states achieving the optimal value\n",
    "# error : Error values for each iteration\n",
    "# num_iter : Required number of iterations\n",
    "# elapsed_time : Elapsed time during the computation\n",
    "\n",
    "def policyitr(P, R, discount, eps):\n",
    "    start_time = time.time()\n",
    "    numOfStates = P.shape[2]\n",
    "    numOfActions = P.shape[0]\n",
    "    bestActionOld =  -1*np.ones(numOfStates)\n",
    "    bestActionNew = np.random.randint(numOfActions, size=numOfStates)\n",
    "    stateVals = np.zeros([numOfStates, 1]);\n",
    "    vals = []; #to keep state values;\n",
    "    #stops if the last two polies are same or\n",
    "    df = np.zeros(1)\n",
    "    df[0] = 1\n",
    "    num_iter=0\n",
    "    while not(np.array_equal(bestActionOld, bestActionNew) or la.norm(df,2) < eps):\n",
    "        num_iter+=1\n",
    "        bestActionOld =np.copy(bestActionNew);\n",
    "        #constructs linear equation to find new values of states\n",
    "        A = np.zeros([numOfStates, numOfStates])\n",
    "        #if there are rewards for actions this line must be changed\n",
    "        b = R;\n",
    "        b.resize([numOfStates,1])\n",
    "        for s in range(numOfStates):\n",
    "            ts = discount*P[bestActionNew[s],s,:];\n",
    "            ts.resize([1,numOfStates]);\n",
    "            A[s,:] = -ts;\n",
    "        #update the state values and continue\n",
    "        A = A + np.identity(numOfStates);\n",
    "        stateValsNew = np.linalg.solve(A,b);\n",
    "        df = stateValsNew-stateVals;\n",
    "        stateVals = stateValsNew\n",
    "        vals.append(stateVals)\n",
    "        #for each state maximize the value\n",
    "        for s in range(numOfStates):\n",
    "            maxVal = -np.inf;\n",
    "            bestActOfs = -1;\n",
    "            #search every action\n",
    "            for a in range(numOfActions):\n",
    "                successors = np.where(P[a,s,:] > 0)\n",
    "                valOfa = R[s] + discount*np.dot(np.squeeze(stateVals[successors]), np.squeeze(P[a,s,successors]))\n",
    "                if valOfa > maxVal:\n",
    "                    maxVal = valOfa;\n",
    "                    bestActOfs = a;\n",
    "            bestActionNew[s] = bestActOfs\n",
    "    error = []\n",
    "    for i in range(len(vals)):\n",
    "        error.append(np.linalg.norm(vals[i] - vals[-1], 2))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return stateVals, bestActionNew, error, num_iter, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following function performs the value iteration algorithm\n",
    "# Inputs:\n",
    "# P: Probability matrix\n",
    "# R: Reward vector for states\n",
    "# discount: Discount factor\n",
    "# optimal: Optimal value vector to compute error per iteration\n",
    "# eps: tolerance \n",
    "\n",
    "# Outputs:\n",
    "# stateVals: Value vector for states\n",
    "# bestAction : Action vector for states achieving the optimal value\n",
    "# error : Error values for each iteration\n",
    "# num_iter : Required number of iterations\n",
    "# elapsed_time : Elapsed time during the computation\n",
    "\n",
    "def valitr(P, R, discount, optimal,eps):\n",
    "    start_time = time.time()\n",
    "    numOfStates = P.shape[2]\n",
    "    numOfActions = P.shape[0]\n",
    "    stateVals = np.zeros(numOfStates)\n",
    "    optimal = np.resize(optimal, stateVals.shape)\n",
    "    bestAction = np.zeros(numOfStates)\n",
    "    #df is the stopping criteria\n",
    "    df = np.zeros(1)\n",
    "    df[0] = 1\n",
    "    num_iter=0\n",
    "    error = []\n",
    "    while la.norm(df,2) > eps:\n",
    "        stateValsNew = np.zeros(P.shape[2])\n",
    "        num_iter+=1\n",
    "        #for each state maximize the value\n",
    "        for s in range(numOfStates):\n",
    "            maxVal = -np.inf;\n",
    "            bestActOfs = -1;\n",
    "            #search every action\n",
    "            for a in range(numOfActions):\n",
    "                successors = np.where(P[a,s,:] > 0)\n",
    "                valOfa = R[s] + discount*np.dot(stateVals[successors], np.squeeze(P[a,s,successors]))\n",
    "                if valOfa > maxVal:\n",
    "                    maxVal = valOfa;\n",
    "                    bestActOfs = a;\n",
    "            bestAction[s] = bestActOfs\n",
    "            stateValsNew[s] = maxVal;\n",
    "        #update the state values and continue\n",
    "        df = stateValsNew - stateVals\n",
    "        stateVals = stateValsNew\n",
    "        error.append(la.norm(stateVals - optimal, 2))\n",
    "        \n",
    "    stateVals = np.resize(stateVals, (numOfStates,1))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return stateVals, bestAction, error, num_iter, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) CVXPY Solvers for the primal problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following functions solve the primal problem\n",
    "\n",
    "# It take constraints and a tolerance value as input, and computes optimal value vectors \n",
    "# using different solvers in CVXPY\n",
    "def ECOS_Primal(A,b,c,tol):\n",
    "    start_time = time.time()\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    cons=[A*Y>=b]\n",
    "    objective=cvx.Minimize(cvx.sum_entries(cvx.mul_elemwise(c,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    prob.solve(solver=ECOS,reltol=tol,max_iters=1000,verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value, Y.value, elapsed_time\n",
    "def SCS_Primal(A,b,c,tol):\n",
    "    start_time = time.time()\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    cons=[A*Y>=b]\n",
    "    objective=cvx.Minimize(cvx.sum_entries(cvx.mul_elemwise(c,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    prob.solve(solver=SCS,eps=tol,max_iters=1000,verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value, Y.value, elapsed_time\n",
    "def CVXOPT_Primal(A,b,c,tol):\n",
    "    start_time = time.time()\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    cons=[A*Y>=b]\n",
    "    objective=cvx.Minimize(cvx.sum_entries(cvx.mul_elemwise(c,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    prob.solve(solver=CVXOPT,reltol=tol,max_iters=1000,verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value ,Y.value, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) CVXPY Solvers for the dual problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following functions solve the dual problem\n",
    "\n",
    "# It take constraints and a tolerance value as input, and computes optimal value vectors \n",
    "# using different solvers in CVXPY\n",
    "\n",
    "def ECOS_Dual(A,b,c,tol):\n",
    "    start_time = time.time()\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    print(A.shape)\n",
    "    cons=[A*Y==c,\n",
    "         Y>=0]\n",
    "    objective=cvx.Maximize(cvx.sum_entries(cvx.mul_elemwise(b,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    prob.solve(solver=ECOS,reltol=tol,max_iters=1000,verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value, Y.value, elapsed_time\n",
    "def SCS_Dual(A,b,c,tol):\n",
    "    start_time = time.time()\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    cons=[A*Y==c,\n",
    "         Y>=0]\n",
    "    objective=cvx.Maximize(cvx.sum_entries(cvx.mul_elemwise(b,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    prob.solve(solver=SCS,eps=tol,max_iters=1000,verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value, Y.value, elapsed_time\n",
    "def CVXOPT_Dual(A,b,c,tol):\n",
    "    start_time = time.time()\n",
    "    Y=cvx.Variable(A.shape[1]) # number of states\n",
    "    cons=[A*Y==c,\n",
    "         Y>=0]\n",
    "    objective=cvx.Maximize(cvx.sum_entries(cvx.mul_elemwise(b,Y)))\n",
    "    prob=cvx.Problem(objective,cons)\n",
    "    prob.solve(solver=CVXOPT,reltol=tol,max_iters=1000,verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print (\"status: %s\" % prob.status)\n",
    "    #print (\"optimal value %s\" % objective.value)\n",
    "    return objective.value, Y.value, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) First Order Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Derivation of Projection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proj2(y, A, b):\n",
    "    x = y\n",
    "    \n",
    "    while(np.min(np.dot(A,x) - b) < 0):\n",
    "        constrainedBool = np.greater_equal(np.dot(A, x), b)\n",
    "    \n",
    "        for i in range(constrainedBool.shape[0]):\n",
    "            if not np.all(constrainedBool[i]):\n",
    "                #print(A[i,:].shape)\n",
    "                qq=(b[i] - np.dot(A[i,:], x))\n",
    "                x = x + np.reshape(qq[0]*A[i,:]/np.dot(A[i,:].T, A[i,:]), (x.shape[0],x.shape[1]))\n",
    "    #print(np.min(np.dot(A,x) - b))\n",
    "    return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proj(y, A, b):\n",
    "    discount = 0.9\n",
    "    minb = np.min(np.dot(A, y) - b)\n",
    "    \n",
    "    if minb <= 0:\n",
    "        x = y + np.abs(minb)/(1 - discount)\n",
    "    else:\n",
    "        x = y\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1) Projected Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graddes(x, A, b, t, eta0):\n",
    "    eta = eta0/np.sqrt(t + 1)\n",
    "    # gradient is always a vector of ones of shape x.shape\n",
    "    grad = np.ones((x.shape[0], 1))\n",
    "    y = x - eta*grad\n",
    "\n",
    "    x = proj(y, A, b)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2) Projected Accelerated Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accelgrad(x, v, theta, A, b, t, eta0):\n",
    "    eta = eta0/np.sqrt(t+1)\n",
    "    grad = np.ones(x.shape)\n",
    "    \n",
    "    v = v - eta*grad\n",
    "    \n",
    "    theta_old = theta\n",
    "    theta = (1 + np.sqrt(1 + 4*theta**2))/2\n",
    "    \n",
    "    xprev = x\n",
    "    x = proj(v, A, b)\n",
    "    \n",
    "    v = x + (theta_old - 1)/(theta)*(x - xprev)\n",
    "    \n",
    "    return x, v, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def descent(update, update_name, x, A, b, eta0, tol, optVal, T=int(1e4)):\n",
    "    v = x\n",
    "    theta = 1.\n",
    "    error = []\n",
    "    \n",
    "    t = int(0)\n",
    "    xnew = x + 1;\n",
    "    while t <=  T and la.norm(x - xnew, 2) > tol:\n",
    "        x = xnew\n",
    "        if update_name == \"gradient\":\n",
    "            xnew = update(x, A, b, t, eta0)\n",
    "            \n",
    "        elif update_name == \"accelerated\":\n",
    "            xnew, v, theta = update(x, v, theta, A, b, t, eta0)\n",
    "            \n",
    "        if(t % 1 == 0) or (t == T - 1):\n",
    "            error.append(la.norm(x - optVal,2))\n",
    "        t = int(t + 1)\n",
    "    \n",
    "    return x, error, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Interior Point Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findBarrierGradient(A,b,x):\n",
    "    #calculates the gradient of barrier function for IPM Ax>=b\n",
    "    d = np.zeros([A.shape[0],1])\n",
    "    for i in range(A.shape[0]):\n",
    "        d[i,0] = 1.0/(np.dot(A[i,:],x) - b[i])\n",
    "    g = np.dot(np.transpose(A),d)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findBarrierHessian(A,b,x):\n",
    "    #calculates the hessian of barrier function for IPM Ax>=b\n",
    "    d = np.zeros([A.shape[0],1])\n",
    "    dMat = np.zeros([A.shape[0],A.shape[0]])\n",
    "    for i in range(A.shape[0]):\n",
    "        dMat[i,i] = -1.0/((np.dot(A[i,:],x) - b[i])**2)  \n",
    "    dummy = np.dot(np.transpose(A),dMat)\n",
    "    H = np.dot(dummy,A)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findAnalyticCenter(A,b,xFeasible,T):\n",
    "    #finds the analytical center for IPM using gradient descent\n",
    "    xFeasible=np.resize(xFeasible,(A.shape[1],1))\n",
    "   # for i in range (T):\n",
    "       # H = findBarrierHessian(A,b,xFeasible)\n",
    "        #g = findBarrierGradient(A,b,xFeasible)\n",
    "        #xFeasible = xFeasible -np.dot(H,g);\n",
    "    Y=cvx.Variable(A.shape[1])\n",
    "    dum=0\n",
    "    for i in range(A.shape[0]):\n",
    "        dum+=cvx.log(cvx.sum_entries(cvx.mul_elemwise(A[i,:],Y))-b[i])\n",
    "    obj=cvx.Minimize(-dum)\n",
    "    prob=cvx.Problem(obj)\n",
    "    prob.solve(solver=SCS,eps=tol,max_iters=100,verbose=False)\n",
    "    return Y.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findOptimalSolution(A,b,stateValsNew,alpha,eps, stateValsOpt):\n",
    "    #finds the optimal solution using IPM\n",
    "    error = []\n",
    "    t = 1;\n",
    "    #to make it enter the loop\n",
    "    stateVals = stateValsNew + 1;\n",
    "    itr = 1;\n",
    "    print(stateValsNew.shape)\n",
    "    while (np.linalg.norm(stateVals-stateValsNew,2) > eps):\n",
    "        stateVals = stateValsNew\n",
    "        #Finds the gradient and hessian\n",
    "        g = t*np.ones([stateVals.size, 1]) + findBarrierGradient(A,b,stateVals)\n",
    "        H = findBarrierHessian(A,b,stateVals)\n",
    "        #Updates the calues\n",
    "        stateValsNew = stateVals - np.dot(np.linalg.inv(H),g)\n",
    "        #stateValsNew = stateVals - 1/np.sqrt(itr)*g;\n",
    "        error.append(np.linalg.norm(stateValsNew - stateValsOpt,2))\n",
    "        t = t*(1+alpha)\n",
    "        if np.isnan(error[-1]):\n",
    "                return stateVals , error\n",
    "    return stateVals, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interiorPoint(A, b, discount, eps, alpha, stateValsOpt):\n",
    "    #finds another feasible point using the optimal solution\n",
    "    xFeasible = stateValsOpt +1;\n",
    "    #finds analytical center\n",
    "    xF = findAnalyticCenter(A,b,xFeasible,1000);\n",
    "    stateVals, error = findOptimalSolution(A,b,xF,alpha,eps, stateValsOpt)\n",
    "    return stateVals, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotGridWorld(Vals, bestAction, Rows, Columns):\n",
    "    imgData = np.resize(Vals,[Rows, Columns]);\n",
    "    imgData = np.flipud(imgData);    \n",
    "    plt.imshow(imgData, interpolation='nearest')\n",
    "    bestAction = np.resize(bestAction,[Rows, Columns])\n",
    "    for a in range(Rows):\n",
    "        for b in range(Columns): \n",
    "            if bestAction[Rows -1 - a,b]== 0:\n",
    "                plt.text(b,a ,r'$ \\leftarrow $')\n",
    "            elif bestAction[Rows -1 - a,b]== 1:\n",
    "                plt.text(b,a,r'$ \\rightarrow $')\n",
    "            elif bestAction[Rows -1 - a,b]== 2:\n",
    "                plt.text(b,a,r'$ \\uparrow $')\n",
    "            elif bestAction[Rows -1 - a,b] == 3:\n",
    "                plt.text(b,a,r'$ \\downarrow $')\n",
    "            elif bestAction[Rows -1 - a,b] == 4:\n",
    "                plt.text(b,a ,r'$ o $')\n",
    "            #plt.text(b,a,r'$ \\leftarrow $')\n",
    "            \n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "def Grid_Map(R, Rows, Columns):\n",
    "    imgData = np.resize(R,[Rows, Columns]);\n",
    "    imgData = np.flipud(imgData);    \n",
    "    plt.imshow(imgData, interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "##### 1) 5 x 5 Grid World\n",
    "In the first example we use a 5 by 5 grid world example to compare the required number of iterations for different methods to achieve a desired tolerance level. The agent starts from the bottom left grid and its aim is to reach the top left grid while avoiding some of the intermediate grids. We solve this problem using (1) value iteration, (2) policy iteration, (3) simplex method, (4) cvxpy with SCS,ECOS and CVXOPT solvers, (5) projected gradient descent, (6) accelerated projected gradient descent and (7) interior point method. We assume that the agent takes the chosen action with probability 0.8, and we use the discount factor of 0.9. The desired tolerance is $10^{-7}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASYAAAD8CAYAAADaFgknAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEBFJREFUeJzt3X+IZeV9x/HPx8nE3WzWaFibFXdbhYiwlaiwbK0WijY2\nGxWlhYK2poQKS2gEY0JFESqlBAqC8Y9a6JCIAa1WkiwRY2I3uLJI/LUaK66r7WIN7la7GjHRBn/s\nzKd/3DvNRHbnnjv33Hueu8/7JQfnzNzzzFdxPz7nOc95HicRAJTkmK4LAIAPI5gAFIdgAlAcgglA\ncQgmAMUhmAAU5yNdFwBg+tl+WdLbkuYlHUqyeZT2CCYAbTk/yRttNMStHIDieBwzvz+yek1mP/HJ\n1tsFxul3P/V61yU09vIrH+iNN+c9ShufO39Nfv7mfKPPPvXse3skvbvkW3NJ5hZPbP+XpF+odyv3\nz0t/thJjuZWb/cQn9em/+Oo4mgbG5om/+aeuS2hsy+deGbmNn785ryce/O1Gn5056T/fHTBu9AdJ\nDtj+LUk7bL+QZNdKa+NWDqhUJC00/GtgW8mB/t8PStouacsotRFMQKWi6IPMNzqWY3uN7bWLX0v6\nY0nPjVIbT+WAijXpDTXwKUnbbUu9TPmXJD8apUGCCahUFM238PAryUuSzhy9ol8jmICKLajM9dgI\nJqBSkTRPMAEoDT0mAEWJpA8KXVqbYAIqFYVbOQCFiTRfZi4RTECtejO/y0QwAdWy5jXSe8BjQzAB\nleoNfhNMAArSm8dEMAEozAI9JgAloccEoDiRNV/oykeNqrK91faLtvfZvn7cRQGYjIW40TFpA3tM\ntmck3SbpQkn7JT1p+74kz4+7OADjE1nvZ6brMg6rSY9pi6R9SV5K8r6keyRdNt6yAIxbb4LlMY2O\nSWsyxnSypKUrn++X9Hsf/pDtbZK2SdLs2hNaKQ7AeB31g9/97VrmJGn1+o2FvoEDYFFizWd6B78P\nSNq45HxD/3sAptyC3OhowvaM7Z/avn/Uupr0mJ6UdJrtU9ULpMsl/fmovxhAt3qD363OGLpG0l5J\nx43a0MAeU5JDkq6W9GD/l96bZM+ovxhAt9oc/La9QdLFkr7ZRm2N4jLJA5IeaOMXAijHfHtzlG6V\ndJ2ktW00VubIF4CxW5z53eSQtM727iXHtsV2bF8i6WCSp9qqjVdSgIotNH8q90aSzUf42XmSLrV9\nkaRVko6zfWeSK1daFz0moFK9l3gb95iO3E5yQ5INSU5R7+HYQ6OEkkSPCahWZH1Q6CspBBNQqUSt\nT7BM8rCkh0dth2ACqtV88uSkEUxApaL2e0xtIZiAipW6UBzBBFQq6mYRuCYIJqBSve2byoyAMqsC\nMAFseAmgMNFQM78nimACKkaPCUBREtNjAlCW3uA3r6QAKEq5a34TTEDfmTf/ddclNLbvf24ZuY3e\n4DdjTAAKw8xvAEVh5jeAInWxy24TBBNQqUT6YIFgAlCQ3q0cwQSgMMz8BlAUpgsAKFA7t3K2V0na\nJelY9TLlO0luGqVNggmoWEtrfr8n6YIk79ielfSI7R8meWylDRJMQKV6T+VGf1cuSSS90z+d7R8Z\npc0yh+QBjN3iBMsmh5bZIlySbM/YfkbSQUk7kjw+Sm30mICKDXErt9wW4UoyL+ks28dL2m77jCTP\nrbQuekxApRafyjXsMTVrM3lL0k5JW0epjWACKraQYxody7F9Yr+nJNurJV0o6YVR6uJWDqhUYh1q\nZ+b3SZK+bXtGvc7OvUnuH6VBggmoWBsTLJM8K+ns0av5NYIJqFTJM78H9uNs3277oO0Vj7ADKFPb\ng99taXKDeYdGHGEHUJ4h5zFN1MBbuSS7bJ8y/lIATFpLr6S0jjEmoFKJdOhoXyiuP0V9myTNrj2h\nrWYBjFGpg9+tBVOSOUlzkrR6/caRXuADMH5sRgCgSCk0mJpMF7hb0qOSTre93/ZV4y8LwCQsyI2O\nSWvyVO6KSRQCYLKSCsaYAEwba/5ofyoHYPqUOsZEMAGVKvldOYIJqFV640wlIpiAivFKCoCihMFv\nACXiVg5AcUp9KldmPw7A2CW9YGpyLMf2Rts7bT9ve4/ta0atjR4TULGWpgsckvS1JE/bXivpKds7\nkjy/0gYJJqBibYwxJXlV0qv9r9+2vVfSyZIIJgDDiayF5k/l1tneveR8rr/U0W/or3Z7tiS2CAew\nMkN0mJbdIlySbH9c0nclfSXJL0epi2ACapX2nsrZnlUvlO5K8r1R2yOYgJq1MMZk25K+JWlvkltG\nb5HpAkDV2pguIOk8SV+QdIHtZ/rHRaPUVX2Paf03ftJ1CUN57dpzuy4BR4lIWlhoZYvwR6R2X7qr\nPpiAakVSoTO/CSagYrwrB6A8BBOAsjQa2O4EwQTUjB4TgKJESgtP5caBYAKqRjABKA23cgCKQzAB\nKAoTLAGUiAmWAMrDUzkApTE9JgBFiRj8BlAaM/gNoED0mAAUZ6HrAg6PYAJqVfA8poFrfo9j+18A\nZXCaHQPbsW+3fdD2c23U1WQzgsXtfzdJOkfSl21vauOXA+hYGh6D3SFpa1tlDQymJK8mebr/9duS\nFrf/BQBJUpJdkt5sq72hxpiW2/7X9jZJ2yRpdu0JLZQGYNyGmGDZaIvwtjQOpkHb//aLnJOk1es3\nFvoQEsD/i4Z5JWXgFuFtahRMbW//C6AQhXYhBgbTOLb/BVCGUt+Va/JUrvXtfwEUoqWncrbvlvSo\npNNt77d91ShlDewxjWP7XwCFaKnHlOSKdlrqYeY3UKmmkye7QDABNWOhOACloccEoDwEE4CiMMYE\noEgEE4DSuNCF4ppMsASAiaLHBNSMWzkARWHwG0CRCCYAxSGYAJTEKvepHMEE1IoxJgBFIpgAFIdg\nKtNr157bdQlDWf+Nn3RdwlCm7d9vbbiVA1CeQoOJV1KAWqX3VK7JMYjtrbZftL3P9vWjlkYwATVr\nYTMC2zOSbpP0eUmbJF1he9MoZRFMQMUW1/0edAywRdK+JC8leV/SPZIuG6UuggmoWfMe0zrbu5cc\n25a0crKkV5ac7+9/b8UY/AZq1XDPuL7ytggHcPSxWpsucEDSxiXnG/rfWzFu5YCKtTTG9KSk02yf\navujki6XdN8oddFjAmrWQo8pySHbV0t6UNKMpNuT7BmlTYIJqFl7W4Q/IOmBdlojmIB6sboAgCIR\nTABKw0JxAIrDrRyAsgw3wXKiCCagZgQTgJK0OPO7dQODyfYqSbskHdv//HeS3DTuwgCMnxfKTKYm\nPab3JF2Q5B3bs5Iesf3DJI+NuTYA4zTNY0xJIumd/uls/yj0HwfAMEq9lWv0Eq/tGdvPSDooaUeS\nx8dbFoCJaGEFy3FoFExJ5pOcpd5yBltsn/Hhz9jetriI1Pyv/rftOgGMQUurC7RuqGVPkrwlaaek\nrYf52VySzUk2z3xsTVv1ARinae0x2T7R9vH9r1dLulDSC+MuDMCYtbhLStuaPJU7SdK3+zshHCPp\n3iT3j7csAOM21fOYkjwr6ewJ1AJg0lJmMjHzG6jY1PaYABylCp5gyWYEQMUmMfht+89s77G9YLvR\nFlAEE1CxCT2Ve07Sn6r3zm0j3MoBtYomMvidZK8k2W58DcEEVGyIwe91tncvOZ9LMtd+RT0EE1Cz\nlrYIt/1jSesP86Mbk3x/2LIIJqBSbU6wTPLZdlrqIZiAWiXFLhTHUzmgZhN4idf2n9jeL+n3Jf3A\n9oODrqHHBFRsEjO/k2yXtH2YawgmoFaRVOitHMEE1KzMXCKYgJrxEi+A4pT6VI5gAmpV8OoCBNOU\nee3ac7suAUeJ3gTLMpOJYAJq1sF63k0QTEDF6DEBKAtjTADKU+67cgQTUDNu5QAUJd1sZtkEwQTU\njB4TgOKUmUsEE1AzL5R5L0cwAbWKmGAJoCxWmGAJoECFBhNrfgM1S5odI7B9s+0XbD9re7vt4wdd\nQzABtVocY2pyjGaHpDOSfEbSf0i6YdAFBBNQMS8sNDpGkeTfkhzqnz4macOgaxhjAqo1+m3aCvyV\npH8d9CGCCahVNEwwrbO9e8n5XJK5xZMmW4TbvlHSIUl3DfplBBNQs+Z3aW8k2XykHw7aItz2FyVd\nIumPksFp2DiYbM9I2i3pQJJLml4HoFyTmMdke6uk6yT9YZJfNblmmB7TNZL2SjpuBbUBKNFkxpj+\nUdKxknbYlqTHknxpuQsaBZPtDZIulvR1SV8dsUgAJUik+fG/k5Lk08Ne07THdKt6XbG1R/qA7W2S\ntknS7NoThq0DQBemdea37UskHUzy1HKfSzKXZHOSzTMfW9NagQDGaAIzv1eiSY/pPEmX2r5I0ipJ\nx9m+M8mV4y0NwFhFUqFrfg/sMSW5IcmGJKdIulzSQ4QScDSIlIVmx4QxjwmoVTSRwe+VGCqYkjws\n6eGxVAJg8god/KbHBNSMYAJQlm6euDVBMAG1iiQ2IwBQHHpMAMoymVdSVoJgAmoVKR3MUWqCYAJq\nVujMb4IJqBljTACKkvBUDkCB6DEBKEuU+fmuizgsggmoVcHLnhBMQM2YLgCgJJGUCfSYbP+9pMvU\n2yzqoKQvJvnv5a5hi3CgVpnYQnE3J/lMkrMk3S/pbwddQI8JqNgkBr+T/HLJ6Rr1OmvLcoNNMYdm\n+3VJP2u52XWS3mi5zXGapnqnqVZpuuodV62/k+TEURqw/SP16mtilaR3l5z/xhbhDX7X1yX9paRf\nSDo/yevLfn4cwTQOtncvt0Vxaaap3mmqVZqueqep1lHY/rGk9Yf50Y1Jvr/kczdIWpXkpuXa41YO\nwMiSfLbhR++S9ICkZYOJwW8AY2X7tCWnl0l6YdA109Rjanw/W4hpqneaapWmq95pqnVc/sH26epN\nF/iZpC8NumBqxpgA1INbOQDFIZgAFGcqgsn2Vtsv2t5n+/qu61mO7dttH7T9XNe1DGJ7o+2dtp+3\nvcf2NV3XdCS2V9l+wva/92v9u65rasL2jO2f2r6/61qmSfHBZHtG0m2SPi9pk6QrbG/qtqpl3SFp\na9dFNHRI0teSbJJ0jqQvF/zv9j1JFyQ5U9JZkrbaPqfjmpq4RtLerouYNsUHk6QtkvYleSnJ+5Lu\nUe+RY5GS7JL0Ztd1NJHk1SRP979+W70/QCd3W9Xhpeed/uls/yj6yY3tDZIulvTNrmuZNtMQTCdL\nemXJ+X4V+odnmtk+RdLZkh7vtpIj698WPaPeG+o7khRba9+tkq5T7zE5hjANwYQxs/1xSd+V9JUP\nvXBZlCTz/TfUN0jaYvuMrms6EtuXSDqY5Kmua5lG0xBMByRtXHK+of89tMD2rHqhdFeS73VdTxNJ\n3pK0U2WP5Z0n6VLbL6s3/HCB7Tu7LWl6TEMwPSnpNNun2v6opMsl3ddxTUcF25b0LUl7k9zSdT3L\nsX2i7eP7X6+WdKEavNrQlSQ3JNmQ5BT1/pt9KMmVHZc1NYoPpiSHJF0t6UH1BmfvTbKn26qOzPbd\nkh6VdLrt/bav6rqmZZwn6Qvq/d/8mf5xUddFHcFJknbafla9/1ntSMIj+KMUr6QAKE7xPSYA9SGY\nABSHYAJQHIIJQHEIJgDFIZgAFIdgAlCc/wMhQwkub4PT2AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bea7048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows,columns,prob,discount,tol=5,5,0.8,0.9,1e-7 # Parameters for the example\n",
    "A,b,c,P_0=Primal_parameters_1(rows,columns,prob,discount,tol) # Compute constraints\n",
    "Grid_Map(b[0:P_0.shape[1]], rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASQAAAD8CAYAAADe49kaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSDRUSCISjf8LNi9iXxKqVmsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1107cedd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 46.43474913]]\n"
     ]
    }
   ],
   "source": [
    "polyit_val, bestAction2,error_pol, num_iter_pol, time_pol = policyitr(P_0, b[0:P_0.shape[1]], discount, tol) # Policy Iteration\n",
    "valit_val, bestAction,error_val, num_iter_val, time_val = valitr(P_0, b, discount,polyit_val, tol) # Value Iteration\n",
    "start_time = time.time()\n",
    "res = linprog(c, -A, -b, A_eq=None, b_eq=None, bounds=None, method='simplex',\\\n",
    "              callback=None, options={'disp': False, 'bland': False, 'tol': tol, 'maxiter': 1000}) # Simplex\n",
    "elapsed_time = time.time() - start_time\n",
    "obj_ecos_p, ecos_val_p, time_ecos_p=ECOS_Primal(A,b,c,tol) # ECOS Primal\n",
    "obj_scs_p, scs_val_p, time_scs_p=SCS_Primal(A,b,c,tol) # SCS Primal\n",
    "obj_cvx_p, cvx_val_p, time_cvx_p=CVXOPT_Primal(A,b,c,tol) # CVXOPT Primal\n",
    "plotGridWorld(polyit_val, bestAction2, rows, columns)\n",
    "xF = findAnalyticCenter(A,b,valit_val+80,1000);\n",
    "#print(xF)\n",
    "#print(valit_val)\n",
    "projgrad_val, projgrad_error, projgrad_iter = descent(graddes, \"gradient\", xF, A, b, 100, tol, valit_val) # Projected Gradient\n",
    "accgrad_val, accgrad_error, accgrad_iter = descent(accelgrad, \"accelerated\", xF, A, b, 1e-1, tol, valit_val) # Accelerated Gradient Descent\n",
    "#interior_val, interior_error=interiorPoint(A, b, discount, tol, 1, valit_val) # Interior Point Method\n",
    "print(projgrad_val[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Methods  | Number of Iterations | Elapsed Time (s) |\n",
    "| ------------- | ------------- |------------ |\n",
    "| Value Iteration  | 168  |0.01667 |\n",
    "| Policy Iteration  | 5  |0.35952 |\n",
    "| Simplex Method  | 75  |0.08410 |\n",
    "| ECOS Solver  | 10  |0.00434 |\n",
    "| SCS Solver  | >1000  |0.02763 |\n",
    "| CVXOPT Solver  | 9  |0.02035 |\n",
    "| Barrier Method  | Content Cell  |Elapsed Time (s) |\n",
    "| Projected Gradient Descent | Content Cell  |Elapsed Time (s) |\n",
    "| Acc Projected Gradient Descent  | Content Cell  |Elapsed Time (s) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 125)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAD8CAYAAADNNJnuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkuHCMrNkuLDMLBn/D5L+3wvB7qtGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e224ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_p,b_p,c_p=Dual_parameters_1(A,b,c)\n",
    "obj_ecos_d, ecos_val_d, time_ecos_d=ECOS_Dual(A_p,b_p,c_p,tol) # ECOS Dual\n",
    "obj_scs_d, scs_val_d, time_scs_d=SCS_Dual(A_p,b_p,c_p,tol) # SCS Dual\n",
    "obj_cvx_d, cvx_val_d, time_cvx_d=CVXOPT_Dual(A_p,b_p,c_p,tol) # CVXOPT Dual\n",
    "policy = np.zeros((A_p.shape[0],5))\n",
    "for a in range(5):\n",
    "    for i in range(A_p.shape[0]):\n",
    "         policy[i,a]=cvx_val_d[a*25+i]\n",
    "denum=np.sum(policy, axis=1)  \n",
    "for i in range(A_p.shape[0]):\n",
    "    for j in range(5):\n",
    "        policy[i,j]=policy[i,j]/denum[i]\n",
    "policy=np.argmax(policy,axis=1)\n",
    "plotGridWorld(b[0:P_0.shape[1]], policy, rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
